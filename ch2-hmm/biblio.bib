
@article{gassiat2020identifiability,
	title = {Identifiability and Consistent Estimation of Nonparametric Translation Hidden Markov Models with General State Space.},
	author = {Gassiat, Elisabeth and Le Corff, Sylvain and Leh{\'e}ricy, Luc},
	year = 2020,
	journal = {J. Mach. Learn. Res.},
	volume = 21,
	pages = {115--1}
}
@article{fischer2020sobolev,
	title = {Sobolev norm learning rates for regularized least-squares algorithms},
	author = {Fischer, Simon and Steinwart, Ingo},
	year = 2020,
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLRORG},
	volume = 21,
	number = 1,
	pages = {8464--8501}
}
@article{decastro2017,
	title = {Consistent Estimation of the Filtering and Marginal Smoothing Distributions in Nonparametric Hidden Markov Models},
	author = {De Castro, Yohann and Gassiat, Élisabeth and Le Corff, Sylvain},
	year = 2017,
	journal = {IEEE Transactions on Information Theory},
	volume = 63,
	number = 8,
	pages = {4758--4777},
	doi = {10.1109/TIT.2017.2696959}
}
@article{alquier,
	title = {Noisy Monte Carlo: convergence of Markov chains with approximate transition kernels},
	author = {Alquier, P. and Friel, N. and Everitt, R. and Boland, A.},
	year = 2016,
	journal = {Statistics and Computing},
	volume = 26,
	number = 1,
	pages = {29--47}
}
@article{mitrophanov-hmm-stability-2005,
	title = {Sensitivity of hidden Markov models},
	author = {Mitrophanov, Alexander Yu. and Lomsadze, Alexandre and Borodovsky, Mark},
	year = 2005,
	journal = {Journal of Applied Probability},
	publisher = {Cambridge University Press},
	volume = 42,
	number = 3,
	pages = {632–642},
	doi = {10.1239/jap/1127322017}
}
@book{cappehmm,
	title = {Inference in Hidden Markov Models (Springer Series in Statistics)},
	author = {Capp\'{e}, Olivier and Moulines, Eric and Ryden, Tobias},
	year = 2005,
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	isbn = {0387402640}
}
@misc{wills2017,
	title = {A Bayesian Filtering Algorithm for Gaussian Mixture Models},
	author = {Wills, Adrian G. and Hendriks, Johannes and Renton, Christopher and Ninness, Brett},
	year = 2017,
	publisher = {arXiv},
	doi = {10.48550/ARXIV.1705.05495},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Machine Learning (stat.ML), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering}
}
@article{mcdonald2020,
	title = {{Exponential filter stability via Dobrushin’s coefficient}},
	author = {Curtis McDonald and Serdar Y{\"u}ksel},
	year = 2020,
	journal = {Electronic Communications in Probability},
	publisher = {Institute of Mathematical Statistics and Bernoulli Society},
	volume = 25,
	number = {none},
	pages = {1 -- 13},
	doi = {10.1214/20-ECP333},
	keywords = {Dobrushin coefficient, filter stability, geometric convergence, non-linear filtering}
}
@inproceedings{legland99,
	title = {Stability and approximation of nonlinear filters: an information theoretic approach},
	author = {Fran{\c{c}}ois {Le Gland}},
	year = 1999,
	booktitle = {Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304)},
	volume = 2,
	number = {},
	pages = {1889--1894 vol.2},
	doi = {10.1109/CDC.1999.830910}
}
@inproceedings{ciliberto2021,
	title = {PSD Representations for Effective Probability Models},
	author = {Rudi, Alessandro and Ciliberto, Carlo},
	year = 2021,
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 34,
	pages = {19411--19422},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@inproceedings{sampling-ulysse,
	title = {Sampling from Arbitrary Functions via PSD Models},
	author = {Marteau-Ferey, Ulysse and Bach, Francis and Rudi, Alessandro},
	year = 2022,
	booktitle = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	volume = 151,
	pages = {2823--2861},
	editor = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
	pdf = {https://proceedings.mlr.press/v151/marteau-ferey22a/marteau-ferey22a.pdf},
	abstract = {In many areas of applied statistics and machine learning, generating an arbitrary number of inde- pendent and identically distributed (i.i.d.) samples from a given distribution is a key task. When the distribution is known only through evaluations of the density, current methods either scale badly with the dimension or require very involved implemen- tations. Instead, we take a two-step approach by first modeling the probability distribution and then sampling from that model. We use the recently introduced class of positive semi-definite (PSD) models which have been shown to be e}
}
@inproceedings{gaspard,
	title = {GloptiNets: Scalable Non-Convex Optimization with Certificates},
	author = {Gaspard Beugnot and Julien Mairal and Alessandro Rudi},
	year = 2023,
	booktitle = {Thirty-seventh Conference on Neural Information Processing Systems}
}
@article{lbfgs,
	title = {On the limited memory BFGS method for large scale optimization},
	author = {Liu, Dong C. and Nocedal, Jorge},
	year = 1989,
	journal = {Math. Program.},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	volume = 45,
	number = {1–3},
	pages = {503–528},
	issn = {0025-5610},
	issue_date = {August    1989},
	abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
	numpages = 26,
	keywords = {Large scale nonlinear optimization, conjugate gradient method, limited memory methods, partitioned quasi-Newton method}
}
@inproceedings{ulysse-non-negative,
	title = {Non-parametric Models for Non-negative Functions},
	author = {Marteau-Ferey, Ulysse and Bach, Francis and Rudi, Alessandro},
	year = 2020,
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {12816--12826},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{less-is-more,
	title = {Less is More: Nystr\"{o}m Computational Regularization},
	author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
	year = 2015,
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 28,
	pages = {},
	editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@article{oudjane,
	title = {{Stability and uniform approximation of nonlinear filters using the Hilbert metric and application to particle filters}},
	author = {Fran{\c{c}}ois {Le Gland} and Nadia Oudjane},
	year = 2004,
	journal = {The Annals of Applied Probability},
	publisher = {Institute of Mathematical Statistics},
	volume = 14,
	number = 1,
	pages = {144 -- 187},
	doi = {10.1214/aoap/1075828050},
	keywords = {Hidden Markov model, Hilbert metric, Mixing, nonlinear filter, particle filter, regularizing kernel, stability, total variation norm}
}
@phdthesis{kim2022duality,
	title = {Duality for nonlinear filtering},
	author = {Jin Won Kim},
	year = 2022,
	school = {University of Illinois Urbana-Champaign}
}
@article{ocone,
	title = {Asymptotic Stability of the Optimal Filter with Respect to Its Initial Condition},
	author = {Ocone, Daniel and Pardoux, Etienne},
	year = 1996,
	journal = {SIAM Journal on Control and Optimization},
	volume = 34,
	number = 1,
	pages = {226--243},
	doi = {10.1137/S0363012993256617},
	eprint = {https://doi.org/10.1137/S0363012993256617}
}
@article{kalman-bucy,
	title = {New Results in Linear Filtering and Prediction Theory},
	author = {Rudolf E. K{\'a}lm{\'a}n and Richard S. Bucy},
	year = 1961,
	journal = {Journal of Basic Engineering},
	volume = 83,
	pages = {95--108}
}
@book{sarkka,
	title = {Bayesian Filtering and Smoothing},
	author = {Särkkä, Simo},
	year = 2013,
	publisher = {Cambridge University Press},
	series = {Institute of Mathematical Statistics Textbooks},
	place = {Cambridge},
	collection = {Institute of Mathematical Statistics Textbooks}
}
@article{ukf,
	title = {Unscented filtering and nonlinear estimation},
	author = {Julier, S.J. and Uhlmann, J.K.},
	year = 2004,
	journal = {Proceedings of the IEEE},
	volume = 92,
	number = 3,
	pages = {401--422},
	doi = {10.1109/JPROC.2003.823141},
	keywords = {Filtering;Nonlinear systems;Target tracking;Control systems;Particle tracking;Kalman filters;Vehicles;Navigation;Chemical processes;Nonlinear control systems}
}
@inproceedings{falkon,
	title = {FALKON: An Optimal Large Scale Kernel Method},
	author = {Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},
	year = 2017,
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 30,
	pages = {},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@misc{cohen,
	title = {Hyperbolic contractivity and the Hilbert metric on probability measures},
	author = {Samuel N. Cohen and Eliana Fausti},
	year = 2023,
	eprint = {2309.02413},
	archiveprefix = {arXiv},
	primaryclass = {math.PR}
}
