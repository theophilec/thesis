@article{1806.01261,
	title        = {Relational inductive biases, deep learning, and graph networks},
	author       = {Peter Battaglia and Jessica Blake Chandler Hamrick and Victor Bapst and Alvaro Sanchez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andy Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Jayne Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
	year         = 2018,
	journal      = {arXiv},
}
@article{2106.11911,
	title        = {Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment},
	author       = {Hao Huang and Boulbaba Ben Amor and Xichan Lin and Fan Zhu and Yi Fang},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2106.11911},
	url          = {https://arxiv.org/abs/2106.11911},
	eprinttype   = {arXiv},
	eprint       = {2106.11911},
	timestamp    = {Thu, 17 Nov 2022 14:15:41 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2106-11911.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
}
@article{2108.05634,
	title        = {Registration for Incomplete Non-Gaussian Functional Data},
	author       = {Bauer, Alexander  and Scheipl, Fabian and K\"uchenhoff, Helmut and Gabriel, Alice-Agnes},
	year         = 2021,
	journal      = {under review},
	doi          = {arxiv:2108.05634},
	url          = {https://arxiv.org/abs/2108.05634},
	language     = {en},
}
@book{adams2003sobolev,
	title        = {Sobolev spaces},
	author       = {Adams, Robert A and Fournier, John JF},
	year         = 2003,
	publisher    = {Elsevier},
}
@book{advancedStructuredPrediction2014MIT,
	title        = {Advanced Structured Prediction},
	author       = {Nowozin, Sebastian and Gehler, Peter V. and Jancsary, Jeremy and Lampert, Christoph H.},
	year         = 2014,
	booktitle    = {Advanced Structured Prediction},
	publisher    = {MIT Press},
	series       = {Neural Information Processing Series},
	pages        = 432,
}
@inproceedings{align-incomparable,
	title        = {Aligning Time Series on Incomparable Spaces},
	author       = {Cohen, Samuel and Luise, Giulia and Terenin, Alexander and Amos, Brandon and Deisenroth, Marc},
	year         = 2021,
	booktitle    = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
}
@book{aliprantis1998principles,
	title        = {Principles of real analysis},
	author       = {Aliprantis, Charalambos D and Burkinshaw, Owen},
	year         = 1998,
	publisher    = {Gulf Professional Publishing},
}
@article{alphafold,
	title        = {Improved protein structure prediction using potentials from deep learning},
	author       = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v{Z}}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	year         = 2020,
	day          = {01},
	journal      = {Nature},
	volume       = 577,
	number       = 7792,
	pages        = {706--710},
	doi          = {10.1038/s41586-019-1923-7},
	issn         = {1476-4687},
}
@article{aronszajn1950theory,
	title        = {Theory of reproducing kernels},
	author       = {Aronszajn, Nachman},
	year         = 1950,
	journal      = {Transactions of the American mathematical society},
	volume       = 68,
	number       = 3,
	pages        = {337--404},
}
@article{atar1,
	title        = {Exponential stability for nonlinear filtering},
	author       = {Rami Atar and Ofer Zeitouni},
	year         = 1997,
	journal      = {Annales de l'Institut Henri Poincare (B) Probability and Statistics},
	volume       = 33,
	number       = 6,
	pages        = {697--725},
	doi          = {https://doi.org/10.1016/S0246-0203(97)80110-0},
	issn         = {0246-0203},
	url          = {https://www.sciencedirect.com/science/article/pii/S0246020397801100},
	keywords     = {Nonlinear filtering, nonlinear smoothing, exponential stability, Birkhoff contraction coefficient},
	abstract     = {We study the a.s. exponential stability of the optimal filter w.r.t. its initial conditions. A bound is provided on the exponential rate (equivalently, on the memory length of the filter) for a general setting both in discrete and in continuous time, in terms of Birkhoff's contraction coefficient. Criteria for exponential stability and explicit bounds on the rate are given in the specific cases of a diffusion process on a compact manifold, and discrete time Markov chains on both continuous and discrete-countable state spaces. A similar question regarding the optimal smoother is investigated and a stability criterion is provided. Résumé Nous étudions la stabilité du filtre optimal par rapport à ses conditions initiales. Le taux de décroissance exponentielle est calculé dans un cadre général, pour temps discret et temps continu, en terme du coefficient de contraction de Birkhoff. Des critères de stabilité exponentielle et des bornes explicites sur le taux sont calculés pour les cas particuliers d'une diffusion sur une variété compacte, ainsi que pour des chaînes de Markov sur un espace discret ou continu.},
}
@book{bakir2007predicting,
	title        = {{Predicting Structured Data}},
	author       = {Bakir, Gökhan and Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J. and Taskar, Ben and Vishwanathan, S.V.N},
	year         = 2007,
	publisher    = {The MIT Press},
	doi          = {10.7551/mitpress/7443.001.0001},
}
@inproceedings{bietti,
	title        = {On the Sample Complexity of Learning under Invariance and Geometric Stability},
	author       = {Alberto Bietti and Luca Venturi and Joan Bruna},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems 34 - 35th Conference on Neural Information Processing Systems, NeurIPS 2021},
	publisher    = {Neural information processing systems foundation},
	series       = {Advances in Neural Information Processing Systems},
	pages        = {18673--18684},
	note         = {Funding Information: LV and JB acknowledge partial support from the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CAREER CIF 1845360, NSF CHS-1901091, and Samsung Electronics. Publisher Copyright: {\textcopyright} 2021 Neural information processing systems foundation. All rights reserved.; 35th Conference on Neural Information Processing Systems, NeurIPS 2021 ; Conference date: 06-12-2021 Through 14-12-2021},
	abstract     = {Many supervised learning problems involve high-dimensional data such as images, text, or graphs. In order to make efficient use of data, it is often useful to leverage certain geometric priors in the problem at hand, such as invariance to translations, permutation subgroups, or stability to small deformations. We study the sample complexity of learning problems where the target function presents such invariance and stability properties, by considering spherical harmonic decompositions of such functions on the sphere. We provide non-parametric rates of convergence for kernel methods, and show improvements in sample complexity by a factor equal to the size of the group when using an invariant kernel over the group, compared to the corresponding non-invariant kernel. These improvements are valid when the sample size is large enough, with an asymptotic behavior that depends on spectral properties of the group. Finally, these gains are extended beyond invariance groups to also cover geometric stability to small deformations, modeled here as subsets (not necessarily subgroups) of permutations.},
	language     = {English (US)},
	editor       = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann Dauphin and Liang, {Percy S.} and {Wortman Vaughan}, Jenn},
}
@article{bietti2021,
	title        = {On the Sample Complexity of Learning under Geometric Stability},
	author       = {Bietti, Alberto and Venturi, Luca and Bruna, Joan},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems 34 (NeurIPS 2021)},
}
@article{blanchard,
	title        = {Optimal Rates for Regularization of Statistical Inverse Learning Problems},
	author       = {Blanchard, Gilles and M{\"u}cke, Nicole},
	year         = 2018,
	day          = {01},
	journal      = {Foundations of Computational Mathematics},
	volume       = 18,
	number       = 4,
	pages        = {971--1013},
	doi          = {10.1007/s10208-017-9359-7},
	issn         = {1615-3383},
	url          = {https://doi.org/10.1007/s10208-017-9359-7},
	abstract     = {We consider a statistical inverse learning (also called inverse regression) problem, where we observe the image of a function f through a linear operator A at i.i.d. random design points {\$}{\$}X{\_}i{\$}{\$}, superposed with an additive noise. The distribution of the design points is unknown and can be very general. We analyze simultaneously the direct (estimation of Af) and the inverse (estimation of f) learning problems. In this general framework, we obtain strong and weak minimax optimal rates of convergence (as the number of observations n grows large) for a large class of spectral regularization methods over regularity classes defined through appropriate source conditions. This improves on or completes previous results obtained in related settings. The optimality of the obtained rates is shown not only in the exponent in n but also in the explicit dependency of the constant factor in the variance of the noise and the radius of the source condition set.},
}
@article{bourdaud2011composition,
	title        = {Composition Operators on Function Spaces with Fractional Order of Smoothness},
	author       = {Bourdaud, G{\'e}rard and Sickel, Winfried},
	year         = 2011,
	journal      = {Harmonic Analysis and Nonlinear Partial Differential Equations},
	volume       = 26,
	pages        = {93--132},
}
@article{bronstein,
	title        = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
	author       = {Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Velickovic},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2104.13478},
	url          = {https://arxiv.org/abs/2104.13478},
	eprinttype   = {arXiv},
	eprint       = {2104.13478},
	timestamp    = {Tue, 04 May 2021 15:12:43 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2104-13478.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
}
@inbook{bruna-book,
	title        = {The Scattering Transform},
	author       = {Bruna, Joan},
	year         = 2022,
	booktitle    = {Mathematical Aspects of Deep Learning},
	publisher    = {Cambridge University Press},
	pages        = {338–399},
	place        = {Cambridge},
	editor       = {Grohs, Philipp and Kutyniok, GittaEditors},
}
@article{bruna2013invariant,
	title        = {Invariant Scattering Convolution Networks},
	author       = {Bruna, Joan and Mallat, Stéphane},
	year         = 2013,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 35,
	number       = 8,
	pages        = {1872--1886},
}
@article{bruveris2017regularity,
	title        = {Regularity of maps between Sobolev spaces},
	author       = {Bruveris, Martins},
	year         = 2017,
	journal      = {Annals of Global Analysis and Geometry},
	publisher    = {Springer},
	volume       = 52,
	number       = 1,
	pages        = {11--24},
}
@book{cappehmm,
	title        = {Inference in Hidden Markov Models (Springer Series in Statistics)},
	author       = {Capp\'{e}, Olivier and Moulines, Eric and Ryden, Tobias},
	year         = 2005,
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	isbn         = {0387402640},
}
@inproceedings{cdtw,
	title        = {{Computing Continuous Dynamic Time Warping of Time Series in Polynomial Time}},
	author       = {Buchin, Kevin and Nusser, Andr\'{e} and Wong, Sampson},
	year         = 2022,
	booktitle    = {38th International Symposium on Computational Geometry (SoCG 2022)},
	publisher    = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	address      = {Dagstuhl, Germany},
	series       = {Leibniz International Proceedings in Informatics (LIPIcs)},
	volume       = 224,
	pages        = {22:1--22:16},
	doi          = {10.4230/LIPIcs.SoCG.2022.22},
	isbn         = {978-3-95977-227-3},
	issn         = {1868-8969},
	url          = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.SoCG.2022.22},
	editor       = {Goaoc, Xavier and Kerber, Michael},
	urn          = {urn:nbn:de:0030-drops-160307},
	annote       = {Keywords: Computational Geometry, Curve Similarity, Fr\'{e}chet distance, Dynamic Time Warping, Continuous Dynamic Time Warping},
}
@inbook{chigansky,
	title        = {Intrinsic methods in filter stability},
	author       = {Chigansky, P. and Liptser, R. and Van Handel, R.},
	year         = 2011,
	booktitle    = {The Oxford handbook of nonlinear filtering},
	publisher    = {Oxford Univ. Press, Oxford},
	series       = {The Oxford handbook of nonlinear filtering},
}
@inproceedings{ciliberto2021,
	title        = {PSD Representations for Effective Probability Models},
	author       = {Rudi, Alessandro and Ciliberto, Carlo},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {19411--19422},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
}
@inproceedings{CKN,
	title        = {Convolutional Kernel Networks},
	author       = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
	year         = 2014,
	booktitle    = {Advances in Neural Information Processing Systems},
	volume       = 27,
}
@article{CKNInvariance,
	title        = {Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations},
	author       = {Alberto Bietti and Julien Mairal},
	year         = 2019,
	journal      = {Journal of Machine Learning Research},
	volume       = 20,
	number       = 25,
	pages        = {1--49},
}
@misc{cohen,
	title        = {Hyperbolic contractivity and the Hilbert metric on probability measures},
	author       = {Samuel N. Cohen and Eliana Fausti},
	year         = 2023,
	eprint       = {2309.02413},
	archiveprefix = {arXiv},
	primaryclass = {math.PR},
}
@article{curve-moments,
	title        = {{Curve alignment by moments}},
	author       = {Gareth M. James},
	year         = 2007,
	journal      = {The Annals of Applied Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 1,
	number       = 2,
	pages        = {480 -- 501},
	doi          = {10.1214/07-aoas127},
	url          = {https://doi.org/10.1214/07-AOAS127},
	keywords     = {continuous monotone registration, Curve registration, landmark registration, moments},
}
@article{dba-petitjean,
	title        = {A global averaging method for dynamic time warping, with applications to clustering},
	author       = {François Petitjean and Alain Ketterlin and Pierre Gançarski},
	year         = 2011,
	journal      = {Pattern Recognition},
	volume       = 44,
	number       = 3,
	pages        = {678--693},
	doi          = {https://doi.org/10.1016/j.patcog.2010.09.013},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/S003132031000453X},
	keywords     = {Sequence analysis, Time series clustering, Dynamic time warping, Distance-based clustering, Time series averaging, DTW barycenter averaging, Global averaging, Satellite image time series},
	abstract     = {Mining sequential data is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. Most works in this field are centred on the definition and use of a distance (or, at least, a similarity measure) between sequences of elements. A measure called dynamic time warping (DTW) seems to be currently the most relevant for a large panel of applications. This article is about the use of DTW in data mining algorithms, and focuses on the computation of an average of a set of sequences. Averaging is an essential tool for the analysis of data. For example, the K-means clustering algorithm repeatedly computes such an average, and needs to provide a description of the clusters it forms. Averaging is here a crucial step, which must be sound in order to make algorithms work accurately. When dealing with sequences, especially when sequences are compared with DTW, averaging is not a trivial task. Starting with existing techniques developed around DTW, the article suggests an analysis framework to classify averaging techniques. It then proceeds to study the two major questions lifted by the framework. First, we develop a global technique for averaging a set of sequences. This technique is original in that it avoids using iterative pairwise averaging. It is thus insensitive to ordering effects. Second, we describe a new strategy to reduce the length of the resulting average sequence. This has a favourable impact on performance, but also on the relevance of the results. Both aspects are evaluated on standard datasets, and the evaluation shows that they compare favourably with existing methods. The article ends by describing the use of averaging in clustering. The last section also introduces a new application domain, namely the analysis of satellite image time series, where data mining techniques provide an original approach.},
}
@article{decastro2017,
	title        = {Consistent Estimation of the Filtering and Marginal Smoothing Distributions in Nonparametric Hidden Markov Models},
	author       = {De Castro, Yohann and Gassiat, Élisabeth and Le Corff, Sylvain},
	year         = 2017,
	journal      = {IEEE Transactions on Information Theory},
	volume       = 63,
	number       = 8,
	pages        = {4758--4777},
	doi          = {10.1109/tit.2017.2696959},
}
@article{decoste2002training,
	title        = {Training invariant support vector machines},
	author       = {DeCoste, Dennis and Sch{\"o}lkopf, Bernhard},
	year         = 2002,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 46,
	number       = 1,
	pages        = {161--190},
}
@inproceedings{deepmind-traffic,
	title        = {ETA Prediction with Graph Neural Networks in Google Maps},
	author       = {Derrow-Pinion, Austin and She, Jennifer and Wong, David and Lange, Oliver and Hester, Todd and Perez, Luis and Nunkesser, Marc and Lee, Seongjae and Guo, Xueying and Wiltshire, Brett and Battaglia, Peter W. and Gupta, Vishal and Li, Ang and Xu, Zhongwen and Sanchez-Gonzalez, Alvaro and Li, Yujia and Velickovic, Petar},
	year         = 2021,
	booktitle    = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
	location     = {Virtual Event, Queensland, Australia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {Cikm '21},
	pages        = {3767–3776},
	doi          = {10.1145/3459637.3481916},
	numpages     = 10,
}
@inproceedings{diff-dtw,
	title        = {Differentiable Divergences Between Time Series},
	author       = {Blondel, Mathieu and Mensch, Arthur and Vert, Jean-Philippe},
	year         = 2021,
	booktitle    = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
}
@article{drineas2005nystrom,
	title        = {On the {Nystr{\"o}m} Method for Approximating a {Gram} Matrix for Improved Kernel-Based Learning.},
	author       = {Drineas, Petros and Mahoney, Michael W and Cristianini, Nello},
	year         = 2005,
	journal      = {journal of machine learning research},
	volume       = 6,
	number       = 12,
}
@inproceedings{dtan,
	title        = {Diffeomorphic Temporal Alignment Nets},
	author       = {Shapira Weber, Ron A and Eyal, Matan and Skafte, Nicki and Shriki, Oren and Freifeld, Oren},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
}
@article{dtw,
	title        = {Dynamic programming algorithm optimization for spoken word recognition},
	author       = {Sakoe, H. and Chiba, S.},
	year         = 1978,
	journal      = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	volume       = 26,
	number       = 1,
	pages        = {43--49},
}
@inproceedings{dtw-baseline-1,
	title        = {Dynamic Time Warping: Itakura vs Sakoe-Chiba},
	author       = {Geler, Zoltan and Kurbalija, Vladimir and Ivanović, Mirjana and Radovanović, Miloš and Dai, Weihui},
	year         = 2019,
	booktitle    = {2019 IEEE International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)},
	pages        = {1--6},
	doi          = {10.1109/inista.2019.8778300},
	keywords     = {Time series analysis;Time measurement;Microsoft Windows;Task analysis;Classification algorithms;Heuristic algorithms;Prediction algorithms;time series;distances;DTW;constraints},
}
@article{dtw-baseline-2,
	title        = {Deep learning for time series classification: a review},
	author       = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	year         = 2019,
	journal      = {Data Min. Knowl. Discov.},
	publisher    = {Kluwer Academic Publishers},
	address      = {Usa},
	volume       = 33,
	number       = 4,
	pages        = {917–963},
	doi          = {10.1007/s10618-019-00619-1},
	issn         = {1384-5810},
	url          = {https://doi.org/10.1007/s10618-019-00619-1},
	issue_date   = {Jul 2019},
	abstract     = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
	numpages     = 47,
	keywords     = {Review, Classification, Time series, Deep learning},
}
@article{dtw-gesture,
	title        = {Multi-dimensional dynamic time warping for gesture recognition},
	author       = {ten Holt, Gineke and Reinders, Marcel and Hendriks, Emile},
	year         = 2007,
	journal      = {Annual Conference of the Advanced School for Computing and Imaging},
}
@article{dtw-incomparable,
	title        = {Aligning Time Series on Incomparable Spaces},
	author       = {Samuel Cohen and Giulia Luise and Alexander Terenin and Brandon Amos and Marc P. Deisenroth},
	year         = 2020,
	journal      = {arXiv:2006.12648},
	url          = {https://arxiv.org/abs/2006.12648},
	abstract     = {Dynamic time warping (DTW) is a useful method for aligning, comparing and combining time series, but it requires them to live in comparable spaces. In this work, we consider a setting in which time series live on different spaces without a sensible ground metric, causing DTW to become ill-defined. To alleviate this, we propose Gromov dynamic time warping (GDTW), a distance between time series on potentially incomparable spaces that avoids the comparability requirement by instead considering intra-relational geometry. We derive a Frank-Wolfe algorithm for computing it and demonstrate its effectiveness at aligning, combining and comparing time series living on incomparable spaces. We further propose a smoothed version of GDTW as a differentiable loss and assess its properties in a variety of settings, including barycentric averaging, generative modeling and imitation learning.},
	date         = {2020-06-22},
	journaltitle = {arXiv:2006.12648},
}
@article{dtw-sakoe,
	title        = {Dynamic programming algorithm optimization for spoken word recognition},
	author       = {Sakoe, H. and Chiba, S.},
	year         = 1978,
	journal      = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	volume       = 26,
	number       = 1,
	pages        = {43--49},
	doi          = {10.1109/tassp.1978.1163055},
	keywords     = {Dynamic programming;Heuristic algorithms;Fluctuations;Timing;Signal processing algorithms;Speech processing;Pattern matching;Constraint optimization;Feature extraction;Acoustics},
}
@inproceedings{duchi2010,
	title        = {On the consistency of ranking algorithms},
	author       = {Duchi, John C. and Mackey, Lester W. and Jordan, Michael I.},
	year         = 2010,
	booktitle    = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	location     = {Haifa, Israel},
	publisher    = {Omnipress},
	address      = {Madison, WI, USA},
	series       = {Icml'10},
	pages        = {327–334},
	numpages     = 8,
}
@article{Durrleman2013,
	title        = {Toward a Comprehensive Framework for the Spatiotemporal Statistical Analysis of Longitudinal Shape Data},
	author       = {Durrleman, Stanley and Pennec, Xavier and Trouv{\'e}, Alain and Braga, Jos{\'e} and Gerig, Guido and Ayache, Nicholas},
	year         = 2013,
	day          = {01},
	journal      = {International Journal of Computer Vision},
	volume       = 103,
	number       = 1,
	pages        = {22--59},
	doi          = {10.1007/s11263-012-0592-x},
	issn         = {1573-1405},
	url          = {https://doi.org/10.1007/s11263-012-0592-x},
	abstract     = {This paper proposes an original approach for the statistical analysis of longitudinal shape data. The proposed method allows the characterization of typical growth patterns and subject-specific shape changes in repeated time-series observations of several subjects. This can be seen as the extension of usual longitudinal statistics of scalar measurements to high-dimensional shape or image data. The method is based on the estimation of continuous subject-specific growth trajectories and the comparison of such temporal shape changes across subjects. Differences between growth trajectories are decomposed into morphological deformations, which account for shape changes independent of the time, and time warps, which account for different rates of shape changes over time. Given a longitudinal shape data set, we estimate a mean growth scenario representative of the population, and the variations of this scenario both in terms of shape changes and in terms of change in growth speed. Then, intrinsic statistics are derived in the space of spatiotemporal deformations, which characterize the typical variations in shape and in growth speed within the studied population. They can be used to detect systematic developmental delays across subjects. In the context of neuroscience, we apply this method to analyze the differences in the growth of the hippocampus in children diagnosed with autism, developmental delays and in controls. Result suggest that group differences may be better characterized by a different speed of maturation rather than shape differences at a given age. In the context of anthropology, we assess the differences in the typical growth of the endocranium between chimpanzees and bonobos. We take advantage of this study to show the robustness of the method with respect to change of parameters and perturbation of the age estimates.},
}
@unpublished{fabian,
	title        = {{PROXQP: an Efficient and Versatile Quadratic Programming Solver for Real-Time Robotics Applications and Beyond}},
	author       = {Bambade, Antoine and Schramm, Fabian and Kazdadi, Sarah El and Caron, St{\'e}phane and Taylor, Adrien and Carpentier, Justin},
	year         = 2023,
	note         = {working paper or preprint},
	pdf          = {https://inria.hal.science/hal-04198663v2/file/ProxQP.pdf},
	hal_id       = {hal-04198663},
	hal_version  = {v2},
}
@inproceedings{falkon,
	title        = {FALKON: An Optimal Large Scale Kernel Method},
	author       = {Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
}
@inproceedings{frechet-clustering,
	title        = {Clustering time series under the fr\'{e}achet distance},
	author       = {Driemel, Anne and Krivo\v{s}ija, Amer and Sohler, Christian},
	year         = 2016,
	booktitle    = {Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms},
	location     = {Arlington, Virginia},
	publisher    = {Society for Industrial and Applied Mathematics},
	address      = {Usa},
	series       = {Soda '16},
	pages        = {766–785},
	isbn         = 9781611974331,
	abstract     = {The Fr\'{e}chet distance is a popular distance measure for curves. We study the problem of clustering time series under the Fr\'{e}chet distance. In particular, we give (1 + ε)-approximation algorithms for variations of the following problem with parameters k and ℓ. Given n univariate time series P, each of complexity at most m, we find k time series, not necessarily from P, which we call cluster centers and which each have complexity at most ℓ, such that (a) the maximum distance of an element of P to its nearest cluster center or (b) the sum of these distances is minimized. Our algorithms have running time near-linear in the input size for constant ε, k and ℓ. To the best of our knowledge, our algorithms are the first clustering algorithms for the Fr\'{e}chet distance which achieve an approximation factor of (1 + ε) or better.},
	numpages     = 20,
	keywords     = {time series, longitudinal data, functional data, dynamic time warping, clustering, approximation algorithms, Fr\'{e}chet distance},
}
@inproceedings{gaspard,
	title        = {GloptiNets: Scalable Non-Convex Optimization with Certificates},
	author       = {Gaspard Beugnot and Julien Mairal and Alessandro Rudi},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
}
@article{gordon-pf,
	title        = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
	author       = {N.J. Gordon},
	year         = 1993,
	journal      = {IEE Proceedings F (Radar and Signal Processing)},
	volume       = 140,
	pages        = {107--113(6)},
	issn         = {0956-375x},
	url          = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015},
	copyright    = {© The Institution of Electrical Engineers},
	keywords     = {state transition model;algorithm;bootstrap filter;recursive Bayesian filters;extended Kalman filter;simulation;measurement model;Gaussian noise;nonGaussian Bayesian state estimation;bearings only tracking problem;nonlinear Bayesian state estimation;state vector density;random samples;},
	language     = {English},
	abstract     = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
	issue        = 2,
}
@article{haasdonk2007invariant,
	title        = {Invariant kernel functions for pattern analysis and machine learning},
	author       = {Haasdonk, Bernard and Burkhardt, Hans},
	year         = 2007,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 68,
	number       = 1,
	pages        = {35--61},
}
@article{kalman-bucy,
	title        = {New Results in Linear Filtering and Prediction Theory},
	author       = {Rudolf E. K{\'a}lm{\'a}n and Richard S. Bucy},
	year         = 1961,
	journal      = {Journal of Basic Engineering},
	volume       = 83,
	pages        = {95--108},
}
@thesis{kim2022duality,
	title        = {Duality for nonlinear filtering},
	author       = {Jin Won Kim},
	year         = 2022,
	school       = {University of Illinois Urbana-Champaign},
}
@article{kondor2008group,
	title        = {Group theoretical methods in machine learning},
	author       = {Kondor, Imre Risi},
	year         = 2008,
	journal      = {PhD thesis},
	publisher    = {Columbia University},
}
@inproceedings{kunita,
	title        = {Asymptotic behavior of nonlinear filters},
	author       = {Blankenship, G. L. and Hopkins, W. E. and Marcus, S. I.},
	year         = 1981,
	booktitle    = {1981 20th IEEE Conference on Decision and Control including the Symposium on Adaptive Processes},
	pages        = {89--89},
	doi          = {10.1109/cdc.1981.269451},
	keywords     = {Nonlinear filters;Educational institutions;Filtering;Markov processes;Steady-state;State estimation;Nonlinear equations},
}
@article{lbfgs,
	title        = {On the limited memory BFGS method for large scale optimization},
	author       = {Liu, Dong C. and Nocedal, Jorge},
	year         = 1989,
	journal      = {Math. Program.},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	volume       = 45,
	number       = {1–3},
	pages        = {503–528},
	issn         = {0025-5610},
	issue_date   = {August    1989},
	abstract     = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
	numpages     = 26,
	keywords     = {Large scale nonlinear optimization, conjugate gradient method, limited memory methods, partitioned quasi-Newton method},
}
@book{lee,
	title        = {Quadratic Programming and Affine Variational Inequalities},
	author       = {Gue Myung Lee and Nguyen Nang Tam and Nguyen Dong Yen},
	year         = 2005,
	editor       = {Springer},
}
@inproceedings{legland99,
	title        = {Stability and approximation of nonlinear filters: an information theoretic approach},
	author       = {Fran{\c{c}}ois {Le Gland}},
	year         = 1999,
	booktitle    = {Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304)},
	volume       = 2,
	pages        = {1889--1894 vol.2},
	doi          = {10.1109/cdc.1999.830910},
}
@inproceedings{less-is-more,
	title        = {Less is More: Nystr\"{o}m Computational Regularization},
	author       = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 28,
	editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
}
@article{mallat-scattering,
	title        = {Group Invariant Scattering},
	author       = {Mallat, Stéphane},
	year         = 2012,
	journal      = {Communications on Pure and Applied Mathematics},
	volume       = 65,
	number       = 10,
	pages        = {1331--1398},
	doi          = {https://doi.org/10.1002/cpa.21413},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21413},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21413},
	abstract     = {Abstract This paper constructs translation-invariant operators on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$, which are Lipschitz-continuous to the action of diffeomorphisms. A scattering propagator is a path-ordered product of nonlinear and noncommuting operators, each of which computes the modulus of a wavelet transform. A local integration defines a windowed scattering transform, which is proved to be Lipschitz-continuous to the action of C2 diffeomorphisms. As the window size increases, it converges to a wavelet scattering transform that is translation invariant. Scattering coefficients also provide representations of stationary processes. Expected values depend upon high-order moments and can discriminate processes having the same power spectrum. Scattering operators are extended on L2(G), where G is a compact Lie group, and are invariant under the action of G. Combining a scattering on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$ and on L2(SO(d)) defines a translation- and rotation-invariant scattering on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$. © 2012 Wiley Periodicals, Inc.},
}
@inproceedings{martinez22a,
	title        = {Closed-Form Diffeomorphic Transformations for Time Series Alignment},
	author       = {Martinez, I{\~n}igo and Viles, Elisabeth and Olaizola, Igor G.},
	year         = 2022,
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {15122--15158},
	url          = {https://proceedings.mlr.press/v162/martinez22a.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/martinez22a/martinez22a.pdf},
	abstract     = {Time series alignment methods call for highly expressive, differentiable and invertible warping functions which preserve temporal topology, i.e diffeomorphisms. Diffeomorphic warping functions can be generated from the integration of velocity fields governed by an ordinary differential equation (ODE). Gradient-based optimization frameworks containing diffeomorphic transformations require to calculate derivatives to the differential equation’s solution with respect to the model parameters, i.e. sensitivity analysis. Unfortunately, deep learning frameworks typically lack automatic-differentiation-compatible sensitivity analysis methods; and implicit functions, such as the solution of ODE, require particular care. Current solutions appeal to adjoint sensitivity methods, ad-hoc numerical solvers or ResNet’s Eulerian discretization. In this work, we present a closed-form expression for the ODE solution and its gradient under continuous piecewise-affine (CPA) velocity functions. We present a highly optimized implementation of the results on CPU and GPU. Furthermore, we conduct extensive experiments on several datasets to validate the generalization ability of our model to unseen data for time-series joint alignment. Results show significant improvements both in terms of efficiency and accuracy.},
}
@article{mcdonald2020,
	title        = {{Exponential filter stability via Dobrushin’s coefficient}},
	author       = {Curtis McDonald and Serdar Y{\"u}ksel},
	year         = 2020,
	journal      = {Electronic Communications in Probability},
	publisher    = {Institute of Mathematical Statistics and Bernoulli Society},
	volume       = 25,
	number       = {none},
	pages        = {1 -- 13},
	doi          = {10.1214/20-ecp333},
	keywords     = {Dobrushin coefficient, filter stability, geometric convergence, non-linear filtering},
}
@report{mitchell-inductive,
	title        = {{The Need for Biases in Learning Generalizations}},
	author       = {{Tom Mitchell}},
	year         = 1980,
	institution  = {{Departement of Computer Science, Laboratory for Computer Science Research, Rutgers University}},
}
@article{mitrophanov-hmm-stability-2005,
	title        = {Sensitivity of hidden Markov models},
	author       = {Mitrophanov, Alexander Yu. and Lomsadze, Alexandre and Borodovsky, Mark},
	year         = 2005,
	journal      = {Journal of Applied Probability},
	publisher    = {Cambridge University Press},
	volume       = 42,
	number       = 3,
	pages        = {632–642},
	doi          = {10.1239/jap/1127322017},
}
@inproceedings{mroueh2015,
	title        = {Learning with Group Invariant Features: A Kernel Perspective.},
	author       = {Mroueh, Youssef and Voinea, Stephen and Poggio, Tomaso A},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems},
	volume       = 28,
}
@article{narcowich2005sobolev,
	title        = {Sobolev bounds on functions with scattered zeros, with applications to radial basis function surface fitting},
	author       = {Narcowich, Francis and Ward, Joseph and Wendland, Holger},
	year         = 2005,
	journal      = {Mathematics of Computation},
	volume       = 74,
	number       = 250,
	pages        = {743--763},
}
@article{ocone,
	title        = {Asymptotic Stability of the Optimal Filter with Respect to Its Initial Condition},
	author       = {Ocone, Daniel and Pardoux, Etienne},
	year         = 1996,
	journal      = {SIAM Journal on Control and Optimization},
	volume       = 34,
	number       = 1,
	pages        = {226--243},
	doi          = {10.1137/s0363012993256617},
	eprint       = {https://doi.org/10.1137/S0363012993256617},
}
@inproceedings{oh-invariances-clinical,
	title        = {Learning to Exploit Invariances in Clinical Time-Series Data using Sequence Transformer Networks},
	author       = {Oh, Jeeheh and Wang, Jiaxuan and Wiens, Jenna},
	year         = 2018,
	booktitle    = {Proceedings of the 3rd Machine Learning for Healthcare Conference},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 85,
	pages        = {332--347},
	url          = {https://proceedings.mlr.press/v85/oh18a.html},
	editor       = {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
	pdf          = {http://proceedings.mlr.press/v85/oh18a/oh18a.pdf},
	abstract     = {Recently, researchers have started applying convolutional neural networks (CNNs) with one-dimensional convolutions to clinical tasks involving time-series data. This is due, in part, to their computational efficiency, relative to recurrent neural networks and their ability to efficiently exploit certain temporal invariances, (\textit{e.g.}, phase invariance). However, it is well-established that clinical data may exhibit many other types of invariances (\textit{e.g.}, scaling). While preprocessing techniques, (\textit{e.g.,} dynamic time warping) may successfully transform and align inputs, their use often requires one to identify the types of invariances in advance. In contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable architecture that learns to identify and account for invariances in clinical time-series data. Applied to the task of predicting in-hospital mortality, our proposed approach achieves an improvement in the area under the receiver operating characteristic curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our results suggest that a variety of valuable invariances can be learned directly from the data.},
}
@book{oppenheim99,
	title        = {Discrete-Time Signal Processing},
	author       = {Oppenheim, Alan V. and Schafer, Ronald W. and Buck, John R.},
	year         = 1999,
	publisher    = {Prentice-hall Englewood Cliffs},
	edition      = {Second},
}
@inproceedings{optnet,
	title        = {OptNet: differentiable optimization as a layer in neural networks},
	author       = {Amos, Brandon and Kolter, J. Zico},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	location     = {Sydney, NSW, Australia},
	publisher    = {JMLR.org},
	series       = {Icml'17},
	pages        = {136–145},
	abstract     = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end train-able deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
	numpages     = 10,
}
@inproceedings{ortiz-ieee,
	title        = {Heart sound classification based on temporal alignment techniques},
	author       = {González Ortiz, José Javier and Phoo, Cheng Perng and Wiens, Jenna},
	year         = 2016,
	booktitle    = {2016 Computing in Cardiology Conference (CinC)},
	pages        = {589--592},
	keywords     = {Sociology;Statistics;Mel frequency cepstral coefficient;Phonocardiography;Heart rate variability;Training},
}
@article{oudjane,
	title        = {{Stability and uniform approximation of nonlinear filters using the Hilbert metric and application to particle filters}},
	author       = {Fran{\c{c}}ois {Le Gland} and Nadia Oudjane},
	year         = 2004,
	journal      = {The Annals of Applied Probability},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 14,
	number       = 1,
	pages        = {144 -- 187},
	doi          = {10.1214/aoap/1075828050},
	keywords     = {Hidden Markov model, Hilbert metric, Mixing, nonlinear filter, particle filter, regularizing kernel, stability, total variation norm},
}
@article{pavf,
	title        = {Transformations Based on Continuous Piecewise-Affine Velocity Fields},
	author       = {Freifeld, Oren and Hauberg, Søren and Batmanghelich, Kayhan and Fisher, Jonn W.},
	year         = 2017,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 39,
	number       = 12,
	pages        = {2496--2509},
	doi          = {10.1109/tpami.2016.2646685},
}
@unpublished{QP-Layer,
	title        = {{QPLayer: efficient differentiation of convex quadratic optimization}},
	author       = {Bambade, Antoine and Schramm, Fabian and Taylor, Adrien and Carpentier, Justin},
	year         = 2023,
	url          = {https://inria.hal.science/hal-04133055},
	note         = {working paper or preprint},
	keywords     = {Machine Learning ; Optimization ; Differentiable Optimization ; Optimization layers},
	pdf          = {https://inria.hal.science/hal-04133055/file/QPLayer_Preprint.pdf},
	hal_id       = {hal-04133055},
	hal_version  = {v1},
}
@misc{qpax,
	title        = {On the Differentiability of the Primal-Dual Interior-Point Method},
	author       = {Kevin Tracy and Zachary Manchester},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.11749},
	eprint       = {2406.11749},
	archiveprefix = {arXiv},
	primaryclass = {math.OC},
}
@article{registration1,
	title        = {Registration of Translated and Rotated Images Using Finite {Fourier} Transforms},
	author       = {De Castro, E. and Morandi, C.},
	year         = 1987,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {Pami-9},
	number       = 5,
	pages        = {700--703},
}
@article{registration2,
	title        = {An {FFT}-based technique for translation, rotation, and scale-invariant image registration},
	author       = {Reddy, B.S. and Chatterji, B.N.},
	year         = 1996,
	journal      = {IEEE Transactions on Image Processing},
	volume       = 5,
	number       = 8,
	pages        = {1266--1271},
}
@inproceedings{rudi2015less,
	title        = {Less is More: Nystr{\"o}m Computational Regularization.},
	author       = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
	year         = 2015,
	booktitle    = {Nips},
	pages        = {1657--1665},
}
@article{rudi2020finding,
	title        = {Finding Global Minima via Kernel Approximations},
	author       = {Rudi, Alessandro and Marteau-Ferey, Ulysse and Bach, Francis},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2012.11978},
}
@article{rudi2021psd,
	title        = {{PSD} representations for effective probability models},
	author       = {Rudi, Alessandro and Ciliberto, Carlo},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
}
@book{runst2011sobolev,
	title        = {Sobolev spaces of fractional order, Nemytskij operators, and nonlinear partial differential equations},
	author       = {Runst, Thomas and Sickel, Winfried},
	year         = 2011,
	publisher    = {de Gruyter},
}
@inproceedings{sampling-ulysse,
	title        = {Sampling from Arbitrary Functions via PSD Models},
	author       = {Marteau-Ferey, Ulysse and Bach, Francis and Rudi, Alessandro},
	year         = 2022,
	booktitle    = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 151,
	pages        = {2823--2861},
	editor       = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
	pdf          = {https://proceedings.mlr.press/v151/marteau-ferey22a/marteau-ferey22a.pdf},
	abstract     = {In many areas of applied statistics and machine learning, generating an arbitrary number of inde- pendent and identically distributed (i.i.d.) samples from a given distribution is a key task. When the distribution is known only through evaluations of the density, current methods either scale badly with the dimension or require very involved implemen- tations. Instead, we take a two-step approach by first modeling the probability distribution and then sampling from that model. We use the recently introduced class of positive semi-definite (PSD) models which have been shown to be e},
}
@book{sarkka,
	title        = {Bayesian Filtering and Smoothing},
	author       = {Särkkä, Simo},
	year         = 2013,
	publisher    = {Cambridge University Press},
	series       = {Institute of Mathematical Statistics Textbooks},
	place        = {Cambridge},
	collection   = {Institute of Mathematical Statistics Textbooks},
}
@book{scholkopf-kernels,
	title        = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
	author       = {Schölkopf, Bernhard and Smola, Alexander J.},
	year         = 2001,
	publisher    = {The MIT Press},
	doi          = {10.7551/mitpress/4175.001.0001},
	isbn         = 9780262256933,
	url          = {https://doi.org/10.7551/mitpress/4175.001.0001},
	abstract     = {{A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs—-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}},
}
@book{shapiro,
	title        = {Perturbation analysis of optimization problems},
	author       = {Bonnans, J Fr{\'e}d{\'e}ric and Shapiro, Alexander},
	year         = 2013,
	publisher    = {Springer Science \& Business Media},
}
@book{shawe-taylor,
	title        = {Kernel Methods for Pattern Analysis},
	author       = {Shawe-Taylor, John and Cristianini, Nello},
	year         = 2004,
	publisher    = {Cambridge University Press},
	place        = {Cambridge},
}
@book{shawe-taylor2004,
	title        = {Kernel Methods for Pattern Analysis},
	author       = {Shawe-Taylor, John and Cristianini, Nello},
	year         = 2004,
	publisher    = {Cambridge University Press},
	place        = {Cambridge},
}
@article{smale,
	title        = {On the mathematical foundations of learning},
	author       = {Cucker, Felipe and Smale, Steve},
	year         = 2002,
	journal      = {Bulletin of the American mathematical society},
	volume       = 39,
	number       = 1,
	pages        = {1--49},
}
@inproceedings{soft-dtw,
	title        = {Soft-{DTW}: a Differentiable Loss Function for Time-Series},
	author       = {Marco Cuturi and Mathieu Blondel},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 70,
	pages        = {894--903},
}
@inproceedings{squared-neural-families,
	title        = {Squared Neural Families: A New Class of Tractable Density Models},
	author       = {Tsuchida, Russell and Ong, Cheng Soon and Sejdinovic, Dino},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {73943--73968},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
}
@article{srvf,
	title        = {Shape Analysis of Elastic Curves in Euclidean Spaces},
	author       = {Srivastava, Anuj and Klassen, Eric and Joshi, Shantanu H. and Jermyn, Ian H.},
	year         = 2011,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 33,
	number       = 7,
	pages        = {1415--1428},
	doi          = {10.1109/tpami.2010.184},
}

@article{stokes-antibiotics,
	title        = {A Deep Learning Approach to Antibiotic Discovery},
	author       = {Stokes, Jonathan M and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M and MacNair, Craig R and French, Shawn and Carfrae, Lindsey A and Bloom-Ackermann, Zohar and Tran, Victoria M and Chiappino-Pepe, Anush and Badran, Ahmed H and Andrews, Ian W and Chory, Emma J and Church, George M and Brown, Eric D and Jaakkola, Tommi S and Barzilay, Regina and Collins, James J},
	year         = 2020,
	journal      = {Cell},
	address      = {United States},
	volume       = 180,
	number       = 4,
	pages        = {688--702.e13},
	language     = {en},
}
@misc{tavenard-dtw-diff,
	title        = {Differentiability of DTW and the case of soft-DTW},
	author       = {Romain Tavenard},
	year         = 2021,
	howpublished = {\url{https://rtavenar.github.io/blog/softdtw.html}},
}
@inproceedings{thiry,
	title        = {{The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods}},
	author       = {Thiry, Louis and Arbel, Michael and Belilovsky, Eugene and Oyallon, Edouard},
	year         = 2021,
	booktitle    = {{International Conference on Learning Representation (ICLR 2021)}},
	address      = {Vienna (online), Austria},
	pdf          = {https://hal.science/hal-03114389/file/iclr2021_conference.pdf},
	hal_id       = {hal-03114389},
	hal_version  = {v1},
}
@book{trefethen1997numerical,
	title        = {Numerical linear algebra},
	author       = {Trefethen, Lloyd N and Bau III, David},
	year         = 1997,
	publisher    = {Siam},
	volume       = 50,
}
@misc{ucr,
	title        = {The UCR Time Series Classification Archive},
	author       = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo, and Hexagon-ML},
	year         = 2018,
}
@article{ukf,
	title        = {Unscented filtering and nonlinear estimation},
	author       = {Julier, S.J. and Uhlmann, J.K.},
	year         = 2004,
	journal      = {Proceedings of the IEEE},
	volume       = 92,
	number       = 3,
	pages        = {401--422},
	doi          = {10.1109/jproc.2003.823141},
	keywords     = {Filtering;Nonlinear systems;Target tracking;Control systems;Particle tracking;Kalman filters;Vehicles;Navigation;Chemical processes;Nonlinear control systems},
}
@inproceedings{ulysse-non-negative,
	title        = {Non-parametric Models for Non-negative Functions},
	author       = {Marteau-Ferey, Ulysse and Bach, Francis and Rudi, Alessandro},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {12816--12826},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
}
@article{vacher2021dimension,
	title        = {A Dimension-free Computational Upper-bound for Smooth Optimal Transport Estimation},
	author       = {Vacher, Adrien and Muzellec, Boris and Rudi, Alessandro and Bach, Francis and Vialard, Francois-Xavier},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2101.05380},
}
@unpublished{vayer,
	title        = {Time Series Alignment with Global Invariances},
	author       = {Vayer, Titouan and Chapel, Laetitia and Courty, Nicolas and Flamary, R{\'e}mi and Soullard, Yann and Tavenard, Romain},
	year         = 2020,
	note         = {preprint},
	hal_id       = {hal-02473959},
	hal_version  = {v1},
}
@article{vayer2022time,
	title        = {Time Series Alignment with Global Invariances},
	author       = {Vayer, Titouan and Tavenard, Romain and Chapel, Laetitia and Flamary, R{\'e}mi and Courty, Nicolas and Soullard, Yann},
	year         = 2022,
	journal      = {Transactions on Machine Learning Research},
	note         = {https://openreview.net/forum?id=JXCH5N4Ujy},
}
@book{wendland2004scattered,
	title        = {Scattered data approximation},
	author       = {Wendland, Holger},
	year         = 2004,
	publisher    = {Cambridge university press},
	volume       = 17,
}
@inproceedings{williams2001using,
	title        = {Using the {Nystr{\"o}m} method to speed up kernel machines},
	author       = {Williams, Christopher and Seeger, Matthias},
	year         = 2001,
	booktitle    = {Proceedings of the 14th annual conference on neural information processing systems},
	pages        = {682--688},
}
@article{wyartdiffeo,
	title        = {Relative stability toward diffeomorphisms indicates performance in deep nets},
	author       = {Petrini, Leonardo and Favero, Alessandro and Geiger, Mario and Wyart, Matthieu},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems 34 (NeurIPS 2021)},
}
@book{younes,
	title        = {Shapes and Diffeomorphisms},
	author       = {Younes, L.},
	year         = 2010,
	publisher    = {Springer Berlin Heidelberg},
	series       = {Applied Mathematical Sciences},
}
