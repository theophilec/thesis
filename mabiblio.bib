@inproceedings{duchi2010,
	title        = {On the consistency of ranking algorithms},
	author       = {Duchi, John C. and Mackey, Lester W. and Jordan, Michael I.},
	year         = 2010,
	booktitle    = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	location     = {Haifa, Israel},
	publisher    = {Omnipress},
	address      = {Madison, WI, USA},
	series       = {Icml'10},
	pages        = {327–334},
	numpages     = 8
}
@article{stokes-antibiotics,
	title        = {A Deep Learning Approach to Antibiotic Discovery},
	author       = {Stokes, Jonathan M and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M and MacNair, Craig R and French, Shawn and Carfrae, Lindsey A and Bloom-Ackermann, Zohar and Tran, Victoria M and Chiappino-Pepe, Anush and Badran, Ahmed H and Andrews, Ian W and Chory, Emma J and Church, George M and Brown, Eric D and Jaakkola, Tommi S and Barzilay, Regina and Collins, James J},
	year         = 2020,
	journal      = {Cell},
	address      = {United States},
	volume       = 180,
	number       = 4,
	pages        = {688--702.e13},
	language     = {en}
}
@book{younes,
	title        = {Shapes and Diffeomorphisms},
	author       = {Younes, L.},
	year         = 2010,
	publisher    = {Springer Berlin Heidelberg},
	series       = {Applied Mathematical Sciences}
}
@inproceedings{deepmind-traffic,
	title        = {ETA Prediction with Graph Neural Networks in Google Maps},
	author       = {Derrow-Pinion, Austin and She, Jennifer and Wong, David and Lange, Oliver and Hester, Todd and Perez, Luis and Nunkesser, Marc and Lee, Seongjae and Guo, Xueying and Wiltshire, Brett and Battaglia, Peter W. and Gupta, Vishal and Li, Ang and Xu, Zhongwen and Sanchez-Gonzalez, Alvaro and Li, Yujia and Velickovic, Petar},
	year         = 2021,
	booktitle    = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
	location     = {Virtual Event, Queensland, Australia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {Cikm '21},
	pages        = {3767–3776},
	doi          = {10.1145/3459637.3481916},
	numpages     = 10,
}
@article{1806.01261,
	title        = {Relational inductive biases, deep learning, and graph networks},
	author       = {Peter Battaglia and Jessica Blake Chandler Hamrick and Victor Bapst and Alvaro Sanchez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andy Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Jayne Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
	year         = 2018,
	journal      = {arXiv}
}
@article{alphafold,
	title        = {Improved protein structure prediction using potentials from deep learning},
	author       = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v{Z}}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	year         = 2020,
	day          = {01},
	journal      = {Nature},
	volume       = 577,
	number       = 7792,
	pages        = {706--710},
	doi          = {10.1038/s41586-019-1923-7},
	issn         = {1476-4687}
}
@report{mitchell-inductive,
	title        = {{The Need for Biases in Learning Generalizations}},
	author       = {{Tom Mitchell}},
	year         = 1980,
	institution  = {{Departement of Computer Science, Laboratory for Computer Science Research, Rutgers University}}
}
@inproceedings{thiry,
	title        = {{The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods}},
	author       = {Thiry, Louis and Arbel, Michael and Belilovsky, Eugene and Oyallon, Edouard},
	year         = 2021,
	booktitle    = {{International Conference on Learning Representation (ICLR 2021)}},
	address      = {Vienna (online), Austria},
	pdf          = {https://hal.science/hal-03114389/file/iclr2021_conference.pdf},
	hal_id       = {hal-03114389},
	hal_version  = {v1}
}
@article{stokes-antibiotic,
	title        = {A Deep Learning Approach to Antibiotic Discovery},
	author       = {Jonathan M. Stokes and Kevin Yang and Kyle Swanson and Wengong Jin and Andres Cubillos-Ruiz and Nina M. Donghia and Craig R. MacNair and Shawn French and Lindsey A. Carfrae and Zohar Bloom-Ackermann and Victoria M. Tran and Anush Chiappino-Pepe and Ahmed H. Badran and Ian W. Andrews and Emma J. Chory and George M. Church and Eric D. Brown and Tommi S. Jaakkola and Regina Barzilay and James J. Collins},
	year         = 2020,
	journal      = {Cell},
	volume       = 180,
	number       = 4,
	pages        = {688--702.e13},
	doi          = {https://doi.org/10.1016/j.cell.2020.01.021},
	issn         = {0092-8674},
}
@book{advancedStructuredPrediction2014MIT,
	title        = {Advanced Structured Prediction},
	author       = {Nowozin, Sebastian and Gehler, Peter V. and Jancsary, Jeremy and Lampert, Christoph H.},
	year         = 2014,
	booktitle    = {Advanced Structured Prediction},
	publisher    = {MIT Press},
	series       = {Neural Information Processing Series},
	pages        = 432,
}
@book{bakir2007predicting,
	title        = {{Predicting Structured Data}},
	author       = {Bakir, Gökhan and Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J. and Taskar, Ben and Vishwanathan, S.V.N},
	year         = 2007,
	publisher    = {The MIT Press},
	doi          = {10.7551/mitpress/7443.001.0001}
}
@article{dtw-trajectories,
	title        = {A comparative analysis of trajectory similarity measures},
	author       = {Yaguang Tao, Alan Both, Rodrigo I. Silveira, Kevin Buchin, Stef Sijben, Ross S. Purves, Patrick Laube, Dongliang Peng, Kevin Toohey and Matt Duckham},
	year         = 2021,
	journal      = {GIScience \& Remote Sensing},
	publisher    = {Taylor \& Francis},
	volume       = 58,
	number       = 5,
	pages        = {643--669},
	doi          = {10.1080/15481603.2021.1908927},
	eprint       = {https://doi.org/10.1080/15481603.2021.1908927}
}
@inproceedings{squared-neural-families,
	title        = {Squared Neural Families: A New Class of Tractable Density Models},
	author       = {Tsuchida, Russell and Ong, Cheng Soon and Sejdinovic, Dino},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {73943--73968},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@article{srvf,
	title        = {Shape Analysis of Elastic Curves in Euclidean Spaces},
	author       = {Srivastava, Anuj and Klassen, Eric and Joshi, Shantanu H. and Jermyn, Ian H.},
	year         = 2011,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 33,
	number       = 7,
	pages        = {1415--1428},
	doi          = {10.1109/tpami.2010.184},
}
@article{pavf,
	title        = {Transformations Based on Continuous Piecewise-Affine Velocity Fields},
	author       = {Freifeld, Oren and Hauberg, Søren and Batmanghelich, Kayhan and Fisher, Jonn W.},
	year         = 2017,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 39,
	number       = 12,
	pages        = {2496--2509},
	doi          = {10.1109/tpami.2016.2646685},
}
@inproceedings{dtan,
	title        = {Diffeomorphic Temporal Alignment Nets},
	author       = {Shapira Weber, Ron A and Eyal, Matan and Skafte, Nicki and Shriki, Oren and Freifeld, Oren},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@article{dtw-gesture,
	title        = {Multi-dimensional dynamic time warping for gesture recognition},
	author       = {ten Holt, Gineke and Reinders, Marcel and Hendriks, Emile},
	year         = 2007,
	journal      = {Annual Conference of the Advanced School for Computing and Imaging}
}
@unpublished{fabian,
	title        = {{PROXQP: an Efficient and Versatile Quadratic Programming Solver for Real-Time Robotics Applications and Beyond}},
	author       = {Bambade, Antoine and Schramm, Fabian and Kazdadi, Sarah El and Caron, St{\'e}phane and Taylor, Adrien and Carpentier, Justin},
	year         = 2023,
	note         = {working paper or preprint},
	pdf          = {https://inria.hal.science/hal-04198663v2/file/ProxQP.pdf},
	hal_id       = {hal-04198663},
	hal_version  = {v2}
}


@article{bronstein,
  author       = {Michael M. Bronstein and
                  Joan Bruna and
                  Taco Cohen and
                  Petar Velickovic},
  title        = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  journal      = {CoRR},
  volume       = {abs/2104.13478},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.13478},
  eprinttype    = {arXiv},
  eprint       = {2104.13478},
  timestamp    = {Tue, 04 May 2021 15:12:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-13478.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bietti,
title = "On the Sample Complexity of Learning under Invariance and Geometric Stability",
abstract = "Many supervised learning problems involve high-dimensional data such as images, text, or graphs. In order to make efficient use of data, it is often useful to leverage certain geometric priors in the problem at hand, such as invariance to translations, permutation subgroups, or stability to small deformations. We study the sample complexity of learning problems where the target function presents such invariance and stability properties, by considering spherical harmonic decompositions of such functions on the sphere. We provide non-parametric rates of convergence for kernel methods, and show improvements in sample complexity by a factor equal to the size of the group when using an invariant kernel over the group, compared to the corresponding non-invariant kernel. These improvements are valid when the sample size is large enough, with an asymptotic behavior that depends on spectral properties of the group. Finally, these gains are extended beyond invariance groups to also cover geometric stability to small deformations, modeled here as subsets (not necessarily subgroups) of permutations.",
author = "Alberto Bietti and Luca Venturi and Joan Bruna",
note = "Funding Information: LV and JB acknowledge partial support from the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CAREER CIF 1845360, NSF CHS-1901091, and Samsung Electronics. Publisher Copyright: {\textcopyright} 2021 Neural information processing systems foundation. All rights reserved.; 35th Conference on Neural Information Processing Systems, NeurIPS 2021 ; Conference date: 06-12-2021 Through 14-12-2021",
year = "2021",
language = "English (US)",
series = "Advances in Neural Information Processing Systems",
publisher = "Neural information processing systems foundation",
pages = "18673--18684",
editor = "Marc'Aurelio Ranzato and Alina Beygelzimer and Yann Dauphin and Liang, {Percy S.} and {Wortman Vaughan}, Jenn",
booktitle = "Advances in Neural Information Processing Systems 34 - 35th Conference on Neural Information Processing Systems, NeurIPS 2021",

}
@article{mallat-scattering,
author = {Mallat, Stéphane},
title = {Group Invariant Scattering},
journal = {Communications on Pure and Applied Mathematics},
volume = {65},
number = {10},
pages = {1331-1398},
doi = {https://doi.org/10.1002/cpa.21413},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21413},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21413},
abstract = {Abstract This paper constructs translation-invariant operators on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$, which are Lipschitz-continuous to the action of diffeomorphisms. A scattering propagator is a path-ordered product of nonlinear and noncommuting operators, each of which computes the modulus of a wavelet transform. A local integration defines a windowed scattering transform, which is proved to be Lipschitz-continuous to the action of C2 diffeomorphisms. As the window size increases, it converges to a wavelet scattering transform that is translation invariant. Scattering coefficients also provide representations of stationary processes. Expected values depend upon high-order moments and can discriminate processes having the same power spectrum. Scattering operators are extended on L2(G), where G is a compact Lie group, and are invariant under the action of G. Combining a scattering on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$ and on L2(SO(d)) defines a translation- and rotation-invariant scattering on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$. © 2012 Wiley Periodicals, Inc.},
year = {2012}
}

@inbook{bruna-book, place={Cambridge}, title={The Scattering Transform}, booktitle={Mathematical Aspects of Deep Learning}, publisher={Cambridge University Press}, author={Bruna, Joan}, editor={Grohs, Philipp and Kutyniok, GittaEditors}, year={2022}, pages={338–399}}

@InProceedings{oh-invariances-clinical,
  title = 	 {Learning to Exploit Invariances in Clinical Time-Series Data using Sequence Transformer Networks},
  author =       {Oh, Jeeheh and Wang, Jiaxuan and Wiens, Jenna},
  booktitle = 	 {Proceedings of the 3rd Machine Learning for Healthcare Conference},
  pages = 	 {332--347},
  year = 	 {2018},
  editor = 	 {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume = 	 {85},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v85/oh18a/oh18a.pdf},
  url = 	 {https://proceedings.mlr.press/v85/oh18a.html},
  abstract = 	 {Recently, researchers have started applying convolutional neural networks (CNNs) with one-dimensional convolutions to clinical tasks involving time-series data. This is due, in part, to their computational efficiency, relative to recurrent neural networks and their ability to efficiently exploit certain temporal invariances, (\textit{e.g.}, phase invariance). However, it is well-established that clinical data may exhibit many other types of invariances (\textit{e.g.}, scaling). While preprocessing techniques, (\textit{e.g.,} dynamic time warping) may successfully transform and align inputs, their use often requires one to identify the types of invariances in advance. In contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable architecture that learns to identify and account for invariances in clinical time-series data. Applied to the task of predicting in-hospital mortality, our proposed approach achieves an improvement in the area under the receiver operating characteristic curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our results suggest that a variety of valuable invariances can be learned directly from the data.}
}
@INPROCEEDINGS{ortiz-ieee,
  author={González Ortiz, José Javier and Phoo, Cheng Perng and Wiens, Jenna},
  booktitle={2016 Computing in Cardiology Conference (CinC)},
  title={Heart sound classification based on temporal alignment techniques},
  year={2016},
  volume={},
  number={},
  pages={589-592},
  keywords={Sociology;Statistics;Mel frequency cepstral coefficient;Phonocardiography;Heart rate variability;Training},
  doi={}}

﻿@Article{Durrleman2013,
author={Durrleman, Stanley
and Pennec, Xavier
and Trouv{\'e}, Alain
and Braga, Jos{\'e}
and Gerig, Guido
and Ayache, Nicholas},
title={Toward a Comprehensive Framework for the Spatiotemporal Statistical Analysis of Longitudinal Shape Data},
journal={International Journal of Computer Vision},
year={2013},
day={01},
volume={103},
number={1},
pages={22-59},
abstract={This paper proposes an original approach for the statistical analysis of longitudinal shape data. The proposed method allows the characterization of typical growth patterns and subject-specific shape changes in repeated time-series observations of several subjects. This can be seen as the extension of usual longitudinal statistics of scalar measurements to high-dimensional shape or image data. The method is based on the estimation of continuous subject-specific growth trajectories and the comparison of such temporal shape changes across subjects. Differences between growth trajectories are decomposed into morphological deformations, which account for shape changes independent of the time, and time warps, which account for different rates of shape changes over time. Given a longitudinal shape data set, we estimate a mean growth scenario representative of the population, and the variations of this scenario both in terms of shape changes and in terms of change in growth speed. Then, intrinsic statistics are derived in the space of spatiotemporal deformations, which characterize the typical variations in shape and in growth speed within the studied population. They can be used to detect systematic developmental delays across subjects. In the context of neuroscience, we apply this method to analyze the differences in the growth of the hippocampus in children diagnosed with autism, developmental delays and in controls. Result suggest that group differences may be better characterized by a different speed of maturation rather than shape differences at a given age. In the context of anthropology, we assess the differences in the typical growth of the endocranium between chimpanzees and bonobos. We take advantage of this study to show the robustness of the method with respect to change of parameters and perturbation of the age estimates.},
issn={1573-1405},
doi={10.1007/s11263-012-0592-x},
url={https://doi.org/10.1007/s11263-012-0592-x}
}
@article{2106.11911,
  author       = {Hao Huang and
                  Boulbaba Ben Amor and
                  Xichan Lin and
                  Fan Zhu and
                  Yi Fang},
  title        = {Residual Networks as Flows of Velocity Fields for Diffeomorphic Time
                  Series Alignment},
  journal      = {CoRR},
  volume       = {abs/2106.11911},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.11911},
  eprinttype    = {arXiv},
  eprint       = {2106.11911},
  timestamp    = {Thu, 17 Nov 2022 14:15:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-11911.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{martinez22a,
  title = 	 {Closed-Form Diffeomorphic Transformations for Time Series Alignment},
  author =       {Martinez, I{\~n}igo and Viles, Elisabeth and Olaizola, Igor G.},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15122--15158},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/martinez22a/martinez22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/martinez22a.html},
  abstract = 	 {Time series alignment methods call for highly expressive, differentiable and invertible warping functions which preserve temporal topology, i.e diffeomorphisms. Diffeomorphic warping functions can be generated from the integration of velocity fields governed by an ordinary differential equation (ODE). Gradient-based optimization frameworks containing diffeomorphic transformations require to calculate derivatives to the differential equation’s solution with respect to the model parameters, i.e. sensitivity analysis. Unfortunately, deep learning frameworks typically lack automatic-differentiation-compatible sensitivity analysis methods; and implicit functions, such as the solution of ODE, require particular care. Current solutions appeal to adjoint sensitivity methods, ad-hoc numerical solvers or ResNet’s Eulerian discretization. In this work, we present a closed-form expression for the ODE solution and its gradient under continuous piecewise-affine (CPA) velocity functions. We present a highly optimized implementation of the results on CPU and GPU. Furthermore, we conduct extensive experiments on several datasets to validate the generalization ability of our model to unseen data for time-series joint alignment. Results show significant improvements both in terms of efficiency and accuracy.}
}
@article{smale,
  title={On the mathematical foundations of learning},
  author={Cucker, Felipe and Smale, Steve},
  journal={Bulletin of the American mathematical society},
  volume={39},
  number={1},
  pages={1--49},
  year={2002}
}
