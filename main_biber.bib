@BOOK{adams2003sobolev,
  AUTHOR = {Adams, Robert A and Fournier, John JF},
  PUBLISHER = {Elsevier},
  DATE = {2003},
  TITLE = {Sobolev spaces},
}

@BOOK{aliprantis1998principles,
  AUTHOR = {Aliprantis, Charalambos D and Burkinshaw, Owen},
  PUBLISHER = {Gulf Professional Publishing},
  DATE = {1998},
  TITLE = {Principles of real analysis},
}

@INPROCEEDINGS{optnet,
  ABSTRACT = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end train-able deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
  AUTHOR = {Amos, Brandon and Kolter, J. Zico},
  LOCATION = {Sydney, NSW, Australia},
  PUBLISHER = {JMLR.org},
  BOOKTITLE = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  DATE = {2017},
  PAGES = {136--145},
  SERIES = {Icml'17},
  TITLE = {OptNet: differentiable optimization as a layer in neural networks},
}

@ARTICLE{aronszajn1950theory,
  AUTHOR = {Aronszajn, Nachman},
  DATE = {1950},
  JOURNALTITLE = {Transactions of the American mathematical society},
  NUMBER = {3},
  PAGES = {337--404},
  TITLE = {Theory of reproducing kernels},
  VOLUME = {68},
}

@ARTICLE{atar1,
  ABSTRACT = {We study the a.s. exponential stability of the optimal filter w.r.t. its initial conditions. A bound is provided on the exponential rate (equivalently, on the memory length of the filter) for a general setting both in discrete and in continuous time, in terms of Birkhoff's contraction coefficient. Criteria for exponential stability and explicit bounds on the rate are given in the specific cases of a diffusion process on a compact manifold, and discrete time Markov chains on both continuous and discrete-countable state spaces. A similar question regarding the optimal smoother is investigated and a stability criterion is provided. Résumé Nous étudions la stabilité du filtre optimal par rapport à ses conditions initiales. Le taux de décroissance exponentielle est calculé dans un cadre général, pour temps discret et temps continu, en terme du coefficient de contraction de Birkhoff. Des critères de stabilité exponentielle et des bornes explicites sur le taux sont calculés pour les cas particuliers d'une diffusion sur une variété compacte, ainsi que pour des chaînes de Markov sur un espace discret ou continu.},
  AUTHOR = {Atar, Rami and Zeitouni, Ofer},
  URL = {https://www.sciencedirect.com/science/article/pii/S0246020397801100},
  DATE = {1997},
  DOI = {https://doi.org/10.1016/S0246-0203(97)80110-0},
  ISSN = {0246-0203},
  JOURNALTITLE = {Annales de l'Institut Henri Poincare (B) Probability and Statistics},
  KEYWORDS = {Nonlinear filtering,nonlinear smoothing,exponential stability,Birkhoff contraction coefficient},
  NUMBER = {6},
  PAGES = {697--725},
  TITLE = {Exponential stability for nonlinear filtering},
  VOLUME = {33},
}

@BOOK{bakir2007predicting,
  AUTHOR = {Bakir, Gökhan and Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J. and Taskar, Ben and Vishwanathan, S.V.N},
  PUBLISHER = {The MIT Press},
  DATE = {2007},
  DOI = {10.7551/mitpress/7443.001.0001},
  TITLE = {{Predicting Structured Data}},
}

@UNPUBLISHED{fabian,
  AUTHOR = {Bambade, Antoine and Schramm, Fabian and Kazdadi, Sarah El and Caron, Stéphane and Taylor, Adrien and Carpentier, Justin},
  DATE = {2023},
  FILE = {https://inria.hal.science/hal-04198663v2/file/ProxQP.pdf},
  NOTE = {working paper or preprint},
  TITLE = {{PROXQP: an Efficient and Versatile Quadratic Programming Solver for Real-Time Robotics Applications and Beyond}},
}

@UNPUBLISHED{QP-Layer,
  AUTHOR = {Bambade, Antoine and Schramm, Fabian and Taylor, Adrien and Carpentier, Justin},
  URL = {https://inria.hal.science/hal-04133055},
  DATE = {2023},
  FILE = {https://inria.hal.science/hal-04133055/file/QPLayer_Preprint.pdf},
  KEYWORDS = {Machine Learning ; Optimization ; Differentiable Optimization ; Optimization layers},
  NOTE = {working paper or preprint},
  TITLE = {{QPLayer: efficient differentiation of convex quadratic optimization}},
}

@ARTICLE{1806.01261,
  AUTHOR = {Battaglia, Peter and Hamrick, Jessica Blake Chandler and Bapst, Victor and Sanchez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andy and Gilmer, Justin and Dahl, George E. and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria Jayne and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  DATE = {2018},
  JOURNALTITLE = {arXiv},
  TITLE = {Relational inductive biases, deep learning, and graph networks},
}

@ARTICLE{2108.05634,
  AUTHOR = {Bauer, Alexander and Scheipl, Fabian and Küchenhoff, Helmut and Gabriel, Alice-Agnes},
  LANGUAGE = {en},
  URL = {https://arxiv.org/abs/2108.05634},
  DATE = {2021},
  DOI = {arxiv:2108.05634},
  JOURNALTITLE = {under review},
  TITLE = {Registration for Incomplete Non-Gaussian Functional Data},
}

@INPROCEEDINGS{gaspard,
  AUTHOR = {Beugnot, Gaspard and Mairal, Julien and Rudi, Alessandro},
  BOOKTITLE = {Thirty-seventh Conference on Neural Information Processing Systems},
  DATE = {2023},
  TITLE = {GloptiNets: Scalable Non-Convex Optimization with Certificates},
}

@ARTICLE{CKNInvariance,
  AUTHOR = {Bietti, Alberto and Mairal, Julien},
  DATE = {2019},
  JOURNALTITLE = {Journal of Machine Learning Research},
  NUMBER = {25},
  PAGES = {1--49},
  TITLE = {Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations},
  VOLUME = {20},
}

@ARTICLE{bietti2021,
  AUTHOR = {Bietti, Alberto and Venturi, Luca and Bruna, Joan},
  DATE = {2021},
  JOURNALTITLE = {Advances in Neural Information Processing Systems 34 (NeurIPS 2021)},
  TITLE = {On the Sample Complexity of Learning under Geometric Stability},
}

@INPROCEEDINGS{bietti,
  ABSTRACT = {Many supervised learning problems involve high-dimensional data such as images, text, or graphs. In order to make efficient use of data, it is often useful to leverage certain geometric priors in the problem at hand, such as invariance to translations, permutation subgroups, or stability to small deformations. We study the sample complexity of learning problems where the target function presents such invariance and stability properties, by considering spherical harmonic decompositions of such functions on the sphere. We provide non-parametric rates of convergence for kernel methods, and show improvements in sample complexity by a factor equal to the size of the group when using an invariant kernel over the group, compared to the corresponding non-invariant kernel. These improvements are valid when the sample size is large enough, with an asymptotic behavior that depends on spectral properties of the group. Finally, these gains are extended beyond invariance groups to also cover geometric stability to small deformations, modeled here as subsets (not necessarily subgroups) of permutations.},
  AUTHOR = {Bietti, Alberto and Venturi, Luca and Bruna, Joan},
  EDITOR = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, {Percy S.} and {Wortman Vaughan}, Jenn},
  LANGUAGE = {English (US)},
  PUBLISHER = {Neural information processing systems foundation},
  BOOKTITLE = {Advances in Neural Information Processing Systems 34 - 35th Conference on Neural Information Processing Systems, NeurIPS 2021},
  DATE = {2021},
  NOTE = {Funding Information: LV and JB acknowledge partial support from the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CAREER CIF 1845360, NSF CHS-1901091, and Samsung Electronics. Publisher Copyright: {©} 2021 Neural information processing systems foundation. All rights reserved.; 35th Conference on Neural Information Processing Systems, NeurIPS 2021 ; Conference date: 06-12-2021 Through 14-12-2021},
  PAGES = {18673--18684},
  SERIES = {Advances in Neural Information Processing Systems},
  TITLE = {On the Sample Complexity of Learning under Invariance and Geometric Stability},
}

@ARTICLE{blanchard,
  ABSTRACT = {We consider a statistical inverse learning (also called inverse regression) problem, where we observe the image of a function f through a linear operator A at i.i.d. random design points {\$}{\$}X{\_}i{\$}{\$}, superposed with an additive noise. The distribution of the design points is unknown and can be very general. We analyze simultaneously the direct (estimation of Af) and the inverse (estimation of f) learning problems. In this general framework, we obtain strong and weak minimax optimal rates of convergence (as the number of observations n grows large) for a large class of spectral regularization methods over regularity classes defined through appropriate source conditions. This improves on or completes previous results obtained in related settings. The optimality of the obtained rates is shown not only in the exponent in n but also in the explicit dependency of the constant factor in the variance of the noise and the radius of the source condition set.},
  AUTHOR = {Blanchard, Gilles and Mücke, Nicole},
  URL = {https://doi.org/10.1007/s10208-017-9359-7},
  DATE = {2018},
  DOI = {10.1007/s10208-017-9359-7},
  ISSN = {1615-3383},
  JOURNALTITLE = {Foundations of Computational Mathematics},
  NUMBER = {4},
  PAGES = {971--1013},
  TITLE = {Optimal Rates for Regularization of Statistical Inverse Learning Problems},
  VOLUME = {18},
}

@INPROCEEDINGS{kunita,
  AUTHOR = {Blankenship, G. L. and Hopkins, W. E. and Marcus, S. I.},
  BOOKTITLE = {1981 20th IEEE Conference on Decision and Control including the Symposium on Adaptive Processes},
  DATE = {1981},
  DOI = {10.1109/cdc.1981.269451},
  KEYWORDS = {Nonlinear filters;Educational institutions;Filtering;Markov processes;Steady-state;State estimation;Nonlinear equations},
  PAGES = {89--89},
  TITLE = {Asymptotic behavior of nonlinear filters},
}

@INPROCEEDINGS{diff-dtw,
  AUTHOR = {Blondel, Mathieu and Mensch, Arthur and Vert, Jean-Philippe},
  BOOKTITLE = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  DATE = {2021},
  TITLE = {Differentiable Divergences Between Time Series},
}

@BOOK{shapiro,
  AUTHOR = {Bonnans, J Frédéric and Shapiro, Alexander},
  PUBLISHER = {Springer Science \& Business Media},
  DATE = {2013},
  TITLE = {Perturbation analysis of optimization problems},
}

@ARTICLE{bourdaud2011composition,
  AUTHOR = {Bourdaud, Gérard and Sickel, Winfried},
  DATE = {2011},
  JOURNALTITLE = {Harmonic Analysis and Nonlinear Partial Differential Equations},
  PAGES = {93--132},
  TITLE = {Composition Operators on Function Spaces with Fractional Order of Smoothness},
  VOLUME = {26},
}

@ARTICLE{bronstein,
  AUTHOR = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Velickovic, Petar},
  URL = {https://arxiv.org/abs/2104.13478},
  DATE = {2021},
  EPRINT = {2104.13478},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  VOLUME = {abs/2104.13478},
}

@INBOOK{bruna-book,
  AUTHOR = {Bruna, Joan},
  EDITOR = {Grohs, Philipp and Kutyniok, GittaEditors},
  PUBLISHER = {Cambridge University Press},
  BOOKTITLE = {Mathematical Aspects of Deep Learning},
  DATE = {2022},
  PAGES = {338--399},
  TITLE = {The Scattering Transform},
}

@ARTICLE{bruna2013invariant,
  AUTHOR = {Bruna, Joan and Mallat, Stéphane},
  DATE = {2013},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {8},
  PAGES = {1872--1886},
  TITLE = {Invariant Scattering Convolution Networks},
  VOLUME = {35},
}

@ARTICLE{bruveris2017regularity,
  AUTHOR = {Bruveris, Martins},
  PUBLISHER = {Springer},
  DATE = {2017},
  JOURNALTITLE = {Annals of Global Analysis and Geometry},
  NUMBER = {1},
  PAGES = {11--24},
  TITLE = {Regularity of maps between Sobolev spaces},
  VOLUME = {52},
}

@INPROCEEDINGS{cdtw,
  AUTHOR = {Buchin, Kevin and Nusser, André and Wong, Sampson},
  EDITOR = {Goaoc, Xavier and Kerber, Michael},
  LOCATION = {Dagstuhl, Germany},
  PUBLISHER = {Schloss Dagstuhl -- Leibniz-Zentrum für Informatik},
  URL = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.SoCG.2022.22},
  ANNOTATION = {Keywords: Computational Geometry, Curve Similarity, Fréchet distance, Dynamic Time Warping, Continuous Dynamic Time Warping},
  BOOKTITLE = {38th International Symposium on Computational Geometry (SoCG 2022)},
  DATE = {2022},
  DOI = {10.4230/LIPIcs.SoCG.2022.22},
  ISBN = {978-3-95977-227-3},
  ISSN = {1868-8969},
  PAGES = {22:1--22:16},
  SERIES = {Leibniz International Proceedings in Informatics (LIPIcs)},
  TITLE = {{Computing Continuous Dynamic Time Warping of Time Series in Polynomial Time}},
  VOLUME = {224},
}

@BOOK{cappehmm,
  AUTHOR = {Cappé, Olivier and Moulines, Eric and Ryden, Tobias},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer-Verlag},
  DATE = {2005},
  ISBN = {0387402640},
  TITLE = {Inference in Hidden Markov Models (Springer Series in Statistics)},
}

@INBOOK{chigansky,
  AUTHOR = {Chigansky, P. and Liptser, R. and Van Handel, R.},
  PUBLISHER = {Oxford Univ. Press, Oxford},
  BOOKTITLE = {The Oxford handbook of nonlinear filtering},
  DATE = {2011},
  SERIES = {The Oxford handbook of nonlinear filtering},
  TITLE = {Intrinsic methods in filter stability},
}

@INPROCEEDINGS{align-incomparable,
  AUTHOR = {Cohen, Samuel and Luise, Giulia and Terenin, Alexander and Amos, Brandon and Deisenroth, Marc},
  BOOKTITLE = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  DATE = {2021},
  TITLE = {Aligning Time Series on Incomparable Spaces},
}

@ARTICLE{dtw-incomparable,
  ABSTRACT = {Dynamic time warping (DTW) is a useful method for aligning, comparing and combining time series, but it requires them to live in comparable spaces. In this work, we consider a setting in which time series live on different spaces without a sensible ground metric, causing DTW to become ill-defined. To alleviate this, we propose Gromov dynamic time warping (GDTW), a distance between time series on potentially incomparable spaces that avoids the comparability requirement by instead considering intra-relational geometry. We derive a Frank-Wolfe algorithm for computing it and demonstrate its effectiveness at aligning, combining and comparing time series living on incomparable spaces. We further propose a smoothed version of GDTW as a differentiable loss and assess its properties in a variety of settings, including barycentric averaging, generative modeling and imitation learning.},
  AUTHOR = {Cohen, Samuel and Luise, Giulia and Terenin, Alexander and Amos, Brandon and Deisenroth, Marc P.},
  URL = {https://arxiv.org/abs/2006.12648},
  DATE = {2020-06-22},
  JOURNALTITLE = {arXiv:2006.12648},
  TITLE = {Aligning Time Series on Incomparable Spaces},
}

@MISC{cohen,
  AUTHOR = {Cohen, Samuel N. and Fausti, Eliana},
  DATE = {2023},
  EPRINT = {2309.02413},
  EPRINTCLASS = {math.PR},
  EPRINTTYPE = {arXiv},
  TITLE = {Hyperbolic contractivity and the Hilbert metric on probability measures},
}

@ARTICLE{smale,
  AUTHOR = {Cucker, Felipe and Smale, Steve},
  DATE = {2002},
  JOURNALTITLE = {Bulletin of the American mathematical society},
  NUMBER = {1},
  PAGES = {1--49},
  TITLE = {On the mathematical foundations of learning},
  VOLUME = {39},
}

@INPROCEEDINGS{soft-dtw,
  AUTHOR = {Cuturi, Marco and Blondel, Mathieu},
  PUBLISHER = {Pmlr},
  BOOKTITLE = {Proceedings of the 34th International Conference on Machine Learning},
  DATE = {2017},
  PAGES = {894--903},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {Soft-{DTW}: a Differentiable Loss Function for Time-Series},
  VOLUME = {70},
}

@MISC{ucr,
  AUTHOR = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo and Hexagon-ML},
  DATE = {2018},
  TITLE = {The UCR Time Series Classification Archive},
}

@ARTICLE{registration1,
  AUTHOR = {De Castro, E. and Morandi, C.},
  DATE = {1987},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {5},
  PAGES = {700--703},
  TITLE = {Registration of Translated and Rotated Images Using Finite {Fourier} Transforms},
  VOLUME = {Pami-9},
}

@ARTICLE{decastro2017,
  AUTHOR = {De Castro, Yohann and Gassiat, Élisabeth and Le Corff, Sylvain},
  DATE = {2017},
  DOI = {10.1109/tit.2017.2696959},
  JOURNALTITLE = {IEEE Transactions on Information Theory},
  NUMBER = {8},
  PAGES = {4758--4777},
  TITLE = {Consistent Estimation of the Filtering and Marginal Smoothing Distributions in Nonparametric Hidden Markov Models},
  VOLUME = {63},
}

@ARTICLE{decoste2002training,
  AUTHOR = {DeCoste, Dennis and Schölkopf, Bernhard},
  PUBLISHER = {Springer},
  DATE = {2002},
  JOURNALTITLE = {Machine learning},
  NUMBER = {1},
  PAGES = {161--190},
  TITLE = {Training invariant support vector machines},
  VOLUME = {46},
}

@INPROCEEDINGS{deepmind-traffic,
  AUTHOR = {Derrow-Pinion, Austin and She, Jennifer and Wong, David and Lange, Oliver and Hester, Todd and Perez, Luis and Nunkesser, Marc and Lee, Seongjae and Guo, Xueying and Wiltshire, Brett and Battaglia, Peter W. and Gupta, Vishal and Li, Ang and Xu, Zhongwen and Sanchez-Gonzalez, Alvaro and Li, Yujia and Velickovic, Petar},
  LOCATION = {Virtual Event, Queensland, Australia},
  PUBLISHER = {Association for Computing Machinery},
  BOOKTITLE = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  DATE = {2021},
  DOI = {10.1145/3459637.3481916},
  PAGES = {3767--3776},
  SERIES = {Cikm '21},
  TITLE = {ETA Prediction with Graph Neural Networks in Google Maps},
}

@INPROCEEDINGS{frechet-clustering,
  ABSTRACT = {The Fréchet distance is a popular distance measure for curves. We study the problem of clustering time series under the Fréchet distance. In particular, we give (1 + ε)-approximation algorithms for variations of the following problem with parameters k and ℓ. Given n univariate time series P, each of complexity at most m, we find k time series, not necessarily from P, which we call cluster centers and which each have complexity at most ℓ, such that (a) the maximum distance of an element of P to its nearest cluster center or (b) the sum of these distances is minimized. Our algorithms have running time near-linear in the input size for constant ε, k and ℓ. To the best of our knowledge, our algorithms are the first clustering algorithms for the Fréchet distance which achieve an approximation factor of (1 + ε) or better.},
  AUTHOR = {Driemel, Anne and Krivošija, Amer and Sohler, Christian},
  LOCATION = {Arlington, Virginia},
  PUBLISHER = {Society for Industrial and Applied Mathematics},
  BOOKTITLE = {Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms},
  DATE = {2016},
  ISBN = {9781611974331},
  KEYWORDS = {time series,longitudinal data,functional data,dynamic time warping,clustering,approximation algorithms,Fréchet distance},
  PAGES = {766--785},
  SERIES = {Soda '16},
  TITLE = {Clustering time series under the fréachet distance},
}

@ARTICLE{drineas2005nystrom,
  AUTHOR = {Drineas, Petros and Mahoney, Michael W and Cristianini, Nello},
  DATE = {2005},
  JOURNALTITLE = {journal of machine learning research},
  NUMBER = {12},
  TITLE = {On the {Nyström} Method for Approximating a {Gram} Matrix for Improved Kernel-Based Learning.},
  VOLUME = {6},
}

@INPROCEEDINGS{duchi2010,
  AUTHOR = {Duchi, John C. and Mackey, Lester W. and Jordan, Michael I.},
  LOCATION = {Haifa, Israel},
  PUBLISHER = {Omnipress},
  BOOKTITLE = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  DATE = {2010},
  PAGES = {327--334},
  SERIES = {Icml'10},
  TITLE = {On the consistency of ranking algorithms},
}

@ARTICLE{Durrleman2013,
  ABSTRACT = {This paper proposes an original approach for the statistical analysis of longitudinal shape data. The proposed method allows the characterization of typical growth patterns and subject-specific shape changes in repeated time-series observations of several subjects. This can be seen as the extension of usual longitudinal statistics of scalar measurements to high-dimensional shape or image data. The method is based on the estimation of continuous subject-specific growth trajectories and the comparison of such temporal shape changes across subjects. Differences between growth trajectories are decomposed into morphological deformations, which account for shape changes independent of the time, and time warps, which account for different rates of shape changes over time. Given a longitudinal shape data set, we estimate a mean growth scenario representative of the population, and the variations of this scenario both in terms of shape changes and in terms of change in growth speed. Then, intrinsic statistics are derived in the space of spatiotemporal deformations, which characterize the typical variations in shape and in growth speed within the studied population. They can be used to detect systematic developmental delays across subjects. In the context of neuroscience, we apply this method to analyze the differences in the growth of the hippocampus in children diagnosed with autism, developmental delays and in controls. Result suggest that group differences may be better characterized by a different speed of maturation rather than shape differences at a given age. In the context of anthropology, we assess the differences in the typical growth of the endocranium between chimpanzees and bonobos. We take advantage of this study to show the robustness of the method with respect to change of parameters and perturbation of the age estimates.},
  AUTHOR = {Durrleman, Stanley and Pennec, Xavier and Trouvé, Alain and Braga, José and Gerig, Guido and Ayache, Nicholas},
  URL = {https://doi.org/10.1007/s11263-012-0592-x},
  DATE = {2013},
  DOI = {10.1007/s11263-012-0592-x},
  ISSN = {1573-1405},
  JOURNALTITLE = {International Journal of Computer Vision},
  NUMBER = {1},
  PAGES = {22--59},
  TITLE = {Toward a Comprehensive Framework for the Spatiotemporal Statistical Analysis of Longitudinal Shape Data},
  VOLUME = {103},
}

@ARTICLE{pavf,
  AUTHOR = {Freifeld, Oren and Hauberg, Søren and Batmanghelich, Kayhan and Fisher, Jonn W.},
  DATE = {2017},
  DOI = {10.1109/tpami.2016.2646685},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {12},
  PAGES = {2496--2509},
  TITLE = {Transformations Based on Continuous Piecewise-Affine Velocity Fields},
  VOLUME = {39},
}

@INPROCEEDINGS{dtw-baseline-1,
  AUTHOR = {Geler, Zoltan and Kurbalija, Vladimir and Ivanović, Mirjana and Radovanović, Miloš and Dai, Weihui},
  BOOKTITLE = {2019 IEEE International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)},
  DATE = {2019},
  DOI = {10.1109/inista.2019.8778300},
  KEYWORDS = {Time series analysis;Time measurement;Microsoft Windows;Task analysis;Classification algorithms;Heuristic algorithms;Prediction algorithms;time series;distances;DTW;constraints},
  PAGES = {1--6},
  TITLE = {Dynamic Time Warping: Itakura vs Sakoe-Chiba},
}

@INPROCEEDINGS{ortiz-ieee,
  AUTHOR = {González Ortiz, José Javier and Phoo, Cheng Perng and Wiens, Jenna},
  BOOKTITLE = {2016 Computing in Cardiology Conference (CinC)},
  DATE = {2016},
  KEYWORDS = {Sociology;Statistics;Mel frequency cepstral coefficient;Phonocardiography;Heart rate variability;Training},
  PAGES = {589--592},
  TITLE = {Heart sound classification based on temporal alignment techniques},
}

@ARTICLE{gordon-pf,
  ABSTRACT = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
  AUTHOR = {Gordon, N.J.},
  LANGUAGE = {English},
  URL = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015},
  DATE = {1993},
  ISSN = {0956-375x},
  ISSUE = {2},
  JOURNALTITLE = {IEE Proceedings F (Radar and Signal Processing)},
  KEYWORDS = {state transition model;algorithm;bootstrap filter;recursive Bayesian filters;extended Kalman filter;simulation;measurement model;Gaussian noise;nonGaussian Bayesian state estimation;bearings only tracking problem;nonlinear Bayesian state estimation;state vector density;random samples;},
  PAGES = {107--113(6)},
  TITLE = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
  VOLUME = {140},
}

@ARTICLE{haasdonk2007invariant,
  AUTHOR = {Haasdonk, Bernard and Burkhardt, Hans},
  PUBLISHER = {Springer},
  DATE = {2007},
  JOURNALTITLE = {Machine learning},
  NUMBER = {1},
  PAGES = {35--61},
  TITLE = {Invariant kernel functions for pattern analysis and machine learning},
  VOLUME = {68},
}

@ARTICLE{dtw-gesture,
  AUTHOR = {ten Holt, Gineke and Reinders, Marcel and Hendriks, Emile},
  DATE = {2007},
  JOURNALTITLE = {Annual Conference of the Advanced School for Computing and Imaging},
  TITLE = {Multi-dimensional dynamic time warping for gesture recognition},
}

@ARTICLE{2106.11911,
  AUTHOR = {Huang, Hao and Amor, Boulbaba Ben and Lin, Xichan and Zhu, Fan and Fang, Yi},
  URL = {https://arxiv.org/abs/2106.11911},
  DATE = {2021},
  EPRINT = {2106.11911},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment},
  VOLUME = {abs/2106.11911},
}

@ARTICLE{dtw-baseline-2,
  ABSTRACT = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  AUTHOR = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  LOCATION = {Usa},
  PUBLISHER = {Kluwer Academic Publishers},
  URL = {https://doi.org/10.1007/s10618-019-00619-1},
  DATE = {2019},
  DOI = {10.1007/s10618-019-00619-1},
  ISSN = {1384-5810},
  JOURNALTITLE = {Data Min. Knowl. Discov.},
  KEYWORDS = {Review,Classification,Time series,Deep learning},
  NUMBER = {4},
  PAGES = {917--963},
  TITLE = {Deep learning for time series classification: a review},
  VOLUME = {33},
}

@ARTICLE{curve-moments,
  AUTHOR = {James, Gareth M.},
  PUBLISHER = {Institute of Mathematical Statistics},
  URL = {https://doi.org/10.1214/07-AOAS127},
  DATE = {2007},
  DOI = {10.1214/07-aoas127},
  JOURNALTITLE = {The Annals of Applied Statistics},
  KEYWORDS = {continuous monotone registration,Curve registration,landmark registration,moments},
  NUMBER = {2},
  PAGES = {480--501},
  TITLE = {{Curve alignment by moments}},
  VOLUME = {1},
}

@ARTICLE{ukf,
  AUTHOR = {Julier, S.J. and Uhlmann, J.K.},
  DATE = {2004},
  DOI = {10.1109/jproc.2003.823141},
  JOURNALTITLE = {Proceedings of the IEEE},
  KEYWORDS = {Filtering;Nonlinear systems;Target tracking;Control systems;Particle tracking;Kalman filters;Vehicles;Navigation;Chemical processes;Nonlinear control systems},
  NUMBER = {3},
  PAGES = {401--422},
  TITLE = {Unscented filtering and nonlinear estimation},
  VOLUME = {92},
}

@ARTICLE{kalman-bucy,
  AUTHOR = {Kálmán, Rudolf E. and Bucy, Richard S.},
  DATE = {1961},
  JOURNALTITLE = {Journal of Basic Engineering},
  PAGES = {95--108},
  TITLE = {New Results in Linear Filtering and Prediction Theory},
  VOLUME = {83},
}

@THESIS{kim2022duality,
  AUTHOR = {Kim, Jin Won},
  INSTITUTION = {University of Illinois Urbana-Champaign},
  DATE = {2022},
  TITLE = {Duality for nonlinear filtering},
}

@ARTICLE{kondor2008group,
  AUTHOR = {Kondor, Imre Risi},
  PUBLISHER = {Columbia University},
  DATE = {2008},
  JOURNALTITLE = {PhD thesis},
  TITLE = {Group theoretical methods in machine learning},
}

@INPROCEEDINGS{legland99,
  AUTHOR = {{Le Gland}, François},
  BOOKTITLE = {Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304)},
  DATE = {1999},
  DOI = {10.1109/cdc.1999.830910},
  PAGES = {1889--1894 vol.2},
  TITLE = {Stability and approximation of nonlinear filters: an information theoretic approach},
  VOLUME = {2},
}

@ARTICLE{oudjane,
  AUTHOR = {{Le Gland}, François and Oudjane, Nadia},
  PUBLISHER = {Institute of Mathematical Statistics},
  DATE = {2004},
  DOI = {10.1214/aoap/1075828050},
  JOURNALTITLE = {The Annals of Applied Probability},
  KEYWORDS = {Hidden Markov model,Hilbert metric,Mixing,nonlinear filter,particle filter,regularizing kernel,stability,total variation norm},
  NUMBER = {1},
  PAGES = {144--187},
  TITLE = {{Stability and uniform approximation of nonlinear filters using the Hilbert metric and application to particle filters}},
  VOLUME = {14},
}

@BOOK{lee,
  AUTHOR = {Lee, Gue Myung and Tam, Nguyen Nang and Yen, Nguyen Dong},
  EDITOR = {Springer},
  DATE = {2005},
  TITLE = {Quadratic Programming and Affine Variational Inequalities},
}

@ARTICLE{lbfgs,
  ABSTRACT = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  AUTHOR = {Liu, Dong C. and Nocedal, Jorge},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer-Verlag},
  DATE = {1989},
  ISSN = {0025-5610},
  JOURNALTITLE = {Math. Program.},
  KEYWORDS = {Large scale nonlinear optimization,conjugate gradient method,limited memory methods,partitioned quasi-Newton method},
  NUMBER = {1–3},
  PAGES = {503--528},
  TITLE = {On the limited memory BFGS method for large scale optimization},
  VOLUME = {45},
}

@INPROCEEDINGS{CKN,
  AUTHOR = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2014},
  TITLE = {Convolutional Kernel Networks},
  VOLUME = {27},
}

@ARTICLE{mallat-scattering,
  ABSTRACT = {Abstract This paper constructs translation-invariant operators on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$, which are Lipschitz-continuous to the action of diffeomorphisms. A scattering propagator is a path-ordered product of nonlinear and noncommuting operators, each of which computes the modulus of a wavelet transform. A local integration defines a windowed scattering transform, which is proved to be Lipschitz-continuous to the action of C2 diffeomorphisms. As the window size increases, it converges to a wavelet scattering transform that is translation invariant. Scattering coefficients also provide representations of stationary processes. Expected values depend upon high-order moments and can discriminate processes having the same power spectrum. Scattering operators are extended on L2(G), where G is a compact Lie group, and are invariant under the action of G. Combining a scattering on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$ and on L2(SO(d)) defines a translation- and rotation-invariant scattering on \$\font\open=msbm10 at 10pt\def\R{\hbox{\open R}}{\bf L}^2({{{\R}}}^d)\$. © 2012 Wiley Periodicals, Inc.},
  AUTHOR = {Mallat, Stéphane},
  URL = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21413},
  DATE = {2012},
  DOI = {https://doi.org/10.1002/cpa.21413},
  EPRINT = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21413},
  JOURNALTITLE = {Communications on Pure and Applied Mathematics},
  NUMBER = {10},
  PAGES = {1331--1398},
  TITLE = {Group Invariant Scattering},
  VOLUME = {65},
}

@INPROCEEDINGS{ulysse-non-negative,
  AUTHOR = {Marteau-Ferey, Ulysse and Bach, Francis and Rudi, Alessandro},
  EDITOR = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2020},
  PAGES = {12816--12826},
  TITLE = {Non-parametric Models for Non-negative Functions},
  VOLUME = {33},
}

@INPROCEEDINGS{sampling-ulysse,
  ABSTRACT = {In many areas of applied statistics and machine learning, generating an arbitrary number of inde- pendent and identically distributed (i.i.d.) samples from a given distribution is a key task. When the distribution is known only through evaluations of the density, current methods either scale badly with the dimension or require very involved implemen- tations. Instead, we take a two-step approach by first modeling the probability distribution and then sampling from that model. We use the recently introduced class of positive semi-definite (PSD) models which have been shown to be e},
  AUTHOR = {Marteau-Ferey, Ulysse and Bach, Francis and Rudi, Alessandro},
  EDITOR = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  PUBLISHER = {Pmlr},
  BOOKTITLE = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  DATE = {2022},
  FILE = {https://proceedings.mlr.press/v151/marteau-ferey22a/marteau-ferey22a.pdf},
  PAGES = {2823--2861},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {Sampling from Arbitrary Functions via PSD Models},
  VOLUME = {151},
}

@INPROCEEDINGS{martinez22a,
  ABSTRACT = {Time series alignment methods call for highly expressive, differentiable and invertible warping functions which preserve temporal topology, i.e diffeomorphisms. Diffeomorphic warping functions can be generated from the integration of velocity fields governed by an ordinary differential equation (ODE). Gradient-based optimization frameworks containing diffeomorphic transformations require to calculate derivatives to the differential equation’s solution with respect to the model parameters, i.e. sensitivity analysis. Unfortunately, deep learning frameworks typically lack automatic-differentiation-compatible sensitivity analysis methods; and implicit functions, such as the solution of ODE, require particular care. Current solutions appeal to adjoint sensitivity methods, ad-hoc numerical solvers or ResNet’s Eulerian discretization. In this work, we present a closed-form expression for the ODE solution and its gradient under continuous piecewise-affine (CPA) velocity functions. We present a highly optimized implementation of the results on CPU and GPU. Furthermore, we conduct extensive experiments on several datasets to validate the generalization ability of our model to unseen data for time-series joint alignment. Results show significant improvements both in terms of efficiency and accuracy.},
  AUTHOR = {Martinez, Iñigo and Viles, Elisabeth and Olaizola, Igor G.},
  EDITOR = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  PUBLISHER = {Pmlr},
  URL = {https://proceedings.mlr.press/v162/martinez22a.html},
  BOOKTITLE = {Proceedings of the 39th International Conference on Machine Learning},
  DATE = {2022},
  FILE = {https://proceedings.mlr.press/v162/martinez22a/martinez22a.pdf},
  PAGES = {15122--15158},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {Closed-Form Diffeomorphic Transformations for Time Series Alignment},
  VOLUME = {162},
}

@ARTICLE{mcdonald2020,
  AUTHOR = {McDonald, Curtis and Yüksel, Serdar},
  PUBLISHER = {Institute of Mathematical Statistics and Bernoulli Society},
  DATE = {2020},
  DOI = {10.1214/20-ecp333},
  JOURNALTITLE = {Electronic Communications in Probability},
  KEYWORDS = {Dobrushin coefficient,filter stability,geometric convergence,non-linear filtering},
  NUMBER = {none},
  PAGES = {1--13},
  TITLE = {{Exponential filter stability via Dobrushin’s coefficient}},
  VOLUME = {25},
}

@ARTICLE{mitrophanov-hmm-stability-2005,
  AUTHOR = {Mitrophanov, Alexander Yu. and Lomsadze, Alexandre and Borodovsky, Mark},
  PUBLISHER = {Cambridge University Press},
  DATE = {2005},
  DOI = {10.1239/jap/1127322017},
  JOURNALTITLE = {Journal of Applied Probability},
  NUMBER = {3},
  PAGES = {632--642},
  TITLE = {Sensitivity of hidden Markov models},
  VOLUME = {42},
}

@INPROCEEDINGS{mroueh2015,
  AUTHOR = {Mroueh, Youssef and Voinea, Stephen and Poggio, Tomaso A},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2015},
  TITLE = {Learning with Group Invariant Features: A Kernel Perspective.},
  VOLUME = {28},
}

@ARTICLE{narcowich2005sobolev,
  AUTHOR = {Narcowich, Francis and Ward, Joseph and Wendland, Holger},
  DATE = {2005},
  JOURNALTITLE = {Mathematics of Computation},
  NUMBER = {250},
  PAGES = {743--763},
  TITLE = {Sobolev bounds on functions with scattered zeros, with applications to radial basis function surface fitting},
  VOLUME = {74},
}

@BOOK{advancedStructuredPrediction2014MIT,
  AUTHOR = {Nowozin, Sebastian and Gehler, Peter V. and Jancsary, Jeremy and Lampert, Christoph H.},
  PUBLISHER = {MIT Press},
  BOOKTITLE = {Advanced Structured Prediction},
  DATE = {2014},
  PAGES = {432},
  SERIES = {Neural Information Processing Series},
  TITLE = {Advanced Structured Prediction},
}

@ARTICLE{ocone,
  AUTHOR = {Ocone, Daniel and Pardoux, Etienne},
  DATE = {1996},
  DOI = {10.1137/s0363012993256617},
  EPRINT = {https://doi.org/10.1137/S0363012993256617},
  JOURNALTITLE = {SIAM Journal on Control and Optimization},
  NUMBER = {1},
  PAGES = {226--243},
  TITLE = {Asymptotic Stability of the Optimal Filter with Respect to Its Initial Condition},
  VOLUME = {34},
}

@INPROCEEDINGS{oh-invariances-clinical,
  ABSTRACT = {Recently, researchers have started applying convolutional neural networks (CNNs) with one-dimensional convolutions to clinical tasks involving time-series data. This is due, in part, to their computational efficiency, relative to recurrent neural networks and their ability to efficiently exploit certain temporal invariances, (\textit{e.g.}, phase invariance). However, it is well-established that clinical data may exhibit many other types of invariances (\textit{e.g.}, scaling). While preprocessing techniques, (\textit{e.g.,} dynamic time warping) may successfully transform and align inputs, their use often requires one to identify the types of invariances in advance. In contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable architecture that learns to identify and account for invariances in clinical time-series data. Applied to the task of predicting in-hospital mortality, our proposed approach achieves an improvement in the area under the receiver operating characteristic curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our results suggest that a variety of valuable invariances can be learned directly from the data.},
  AUTHOR = {Oh, Jeeheh and Wang, Jiaxuan and Wiens, Jenna},
  EDITOR = {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  PUBLISHER = {Pmlr},
  URL = {https://proceedings.mlr.press/v85/oh18a.html},
  BOOKTITLE = {Proceedings of the 3rd Machine Learning for Healthcare Conference},
  DATE = {2018},
  FILE = {http://proceedings.mlr.press/v85/oh18a/oh18a.pdf},
  PAGES = {332--347},
  SERIES = {Proceedings of Machine Learning Research},
  TITLE = {Learning to Exploit Invariances in Clinical Time-Series Data using Sequence Transformer Networks},
  VOLUME = {85},
}

@BOOK{oppenheim99,
  AUTHOR = {Oppenheim, Alan V. and Schafer, Ronald W. and Buck, John R.},
  PUBLISHER = {Prentice-hall Englewood Cliffs},
  DATE = {1999},
  EDITION = {Second},
  TITLE = {Discrete-Time Signal Processing},
}

@ARTICLE{dba-petitjean,
  ABSTRACT = {Mining sequential data is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. Most works in this field are centred on the definition and use of a distance (or, at least, a similarity measure) between sequences of elements. A measure called dynamic time warping (DTW) seems to be currently the most relevant for a large panel of applications. This article is about the use of DTW in data mining algorithms, and focuses on the computation of an average of a set of sequences. Averaging is an essential tool for the analysis of data. For example, the K-means clustering algorithm repeatedly computes such an average, and needs to provide a description of the clusters it forms. Averaging is here a crucial step, which must be sound in order to make algorithms work accurately. When dealing with sequences, especially when sequences are compared with DTW, averaging is not a trivial task. Starting with existing techniques developed around DTW, the article suggests an analysis framework to classify averaging techniques. It then proceeds to study the two major questions lifted by the framework. First, we develop a global technique for averaging a set of sequences. This technique is original in that it avoids using iterative pairwise averaging. It is thus insensitive to ordering effects. Second, we describe a new strategy to reduce the length of the resulting average sequence. This has a favourable impact on performance, but also on the relevance of the results. Both aspects are evaluated on standard datasets, and the evaluation shows that they compare favourably with existing methods. The article ends by describing the use of averaging in clustering. The last section also introduces a new application domain, namely the analysis of satellite image time series, where data mining techniques provide an original approach.},
  AUTHOR = {Petitjean, François and Ketterlin, Alain and Gançarski, Pierre},
  URL = {https://www.sciencedirect.com/science/article/pii/S003132031000453X},
  DATE = {2011},
  DOI = {https://doi.org/10.1016/j.patcog.2010.09.013},
  ISSN = {0031-3203},
  JOURNALTITLE = {Pattern Recognition},
  KEYWORDS = {Sequence analysis,Time series clustering,Dynamic time warping,Distance-based clustering,Time series averaging,DTW barycenter averaging,Global averaging,Satellite image time series},
  NUMBER = {3},
  PAGES = {678--693},
  TITLE = {A global averaging method for dynamic time warping, with applications to clustering},
  VOLUME = {44},
}

@ARTICLE{wyartdiffeo,
  AUTHOR = {Petrini, Leonardo and Favero, Alessandro and Geiger, Mario and Wyart, Matthieu},
  DATE = {2021},
  JOURNALTITLE = {Advances in Neural Information Processing Systems 34 (NeurIPS 2021)},
  TITLE = {Relative stability toward diffeomorphisms indicates performance in deep nets},
}

@ARTICLE{registration2,
  AUTHOR = {Reddy, B.S. and Chatterji, B.N.},
  DATE = {1996},
  JOURNALTITLE = {IEEE Transactions on Image Processing},
  NUMBER = {8},
  PAGES = {1266--1271},
  TITLE = {An {FFT}-based technique for translation, rotation, and scale-invariant image registration},
  VOLUME = {5},
}

@INPROCEEDINGS{rudi2015less,
  AUTHOR = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
  BOOKTITLE = {Nips},
  DATE = {2015},
  PAGES = {1657--1665},
  TITLE = {Less is More: Nyström Computational Regularization.},
}

@INPROCEEDINGS{falkon,
  AUTHOR = {Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},
  EDITOR = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2017},
  TITLE = {FALKON: An Optimal Large Scale Kernel Method},
  VOLUME = {30},
}

@INPROCEEDINGS{rudi2021psd,
  AUTHOR = {Rudi, Alessandro and Ciliberto, Carlo},
  EDITOR = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2021},
  PAGES = {19411--19422},
  TITLE = {PSD Representations for Effective Probability Models},
  VOLUME = {34},
}

@ARTICLE{rudi2021psd,
  AUTHOR = {Rudi, Alessandro and Ciliberto, Carlo},
  DATE = {2021},
  JOURNALTITLE = {Advances in Neural Information Processing Systems},
  TITLE = {{PSD} representations for effective probability models},
  VOLUME = {34},
}

@ARTICLE{rudi2020finding,
  AUTHOR = {Rudi, Alessandro and Marteau-Ferey, Ulysse and Bach, Francis},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2012.11978},
  TITLE = {Finding Global Minima via Kernel Approximations},
}

@BOOK{runst2011sobolev,
  AUTHOR = {Runst, Thomas and Sickel, Winfried},
  PUBLISHER = {de Gruyter},
  DATE = {2011},
  TITLE = {Sobolev spaces of fractional order, Nemytskij operators, and nonlinear partial differential equations},
}

@ARTICLE{dtw,
  AUTHOR = {Sakoe, H. and Chiba, S.},
  DATE = {1978},
  JOURNALTITLE = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  NUMBER = {1},
  PAGES = {43--49},
  TITLE = {Dynamic programming algorithm optimization for spoken word recognition},
  VOLUME = {26},
}

@ARTICLE{dtw-sakoe,
  AUTHOR = {Sakoe, H. and Chiba, S.},
  DATE = {1978},
  DOI = {10.1109/tassp.1978.1163055},
  JOURNALTITLE = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  KEYWORDS = {Dynamic programming;Heuristic algorithms;Fluctuations;Timing;Signal processing algorithms;Speech processing;Pattern matching;Constraint optimization;Feature extraction;Acoustics},
  NUMBER = {1},
  PAGES = {43--49},
  TITLE = {Dynamic programming algorithm optimization for spoken word recognition},
  VOLUME = {26},
}

@BOOK{sarkka,
  AUTHOR = {Särkkä, Simo},
  PUBLISHER = {Cambridge University Press},
  DATE = {2013},
  SERIES = {Institute of Mathematical Statistics Textbooks},
  TITLE = {Bayesian Filtering and Smoothing},
}

@BOOK{scholkopf-kernels,
  ABSTRACT = {{A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs—-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}},
  AUTHOR = {Schölkopf, Bernhard and Smola, Alexander J.},
  PUBLISHER = {The MIT Press},
  URL = {https://doi.org/10.7551/mitpress/4175.001.0001},
  DATE = {2001},
  DOI = {10.7551/mitpress/4175.001.0001},
  ISBN = {9780262256933},
  TITLE = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
}

@ARTICLE{alphafold,
  AUTHOR = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and Žídek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
  DATE = {2020},
  DOI = {10.1038/s41586-019-1923-7},
  ISSN = {1476-4687},
  JOURNALTITLE = {Nature},
  NUMBER = {7792},
  PAGES = {706--710},
  TITLE = {Improved protein structure prediction using potentials from deep learning},
  VOLUME = {577},
}

@INPROCEEDINGS{dtan,
  AUTHOR = {Shapira Weber, Ron A and Eyal, Matan and Skafte, Nicki and Shriki, Oren and Freifeld, Oren},
  EDITOR = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alché-Buc, F. and Fox, E. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2019},
  TITLE = {Diffeomorphic Temporal Alignment Nets},
  VOLUME = {32},
}

@BOOK{shawe-taylor,
  AUTHOR = {Shawe-Taylor, John and Cristianini, Nello},
  PUBLISHER = {Cambridge University Press},
  DATE = {2004},
  TITLE = {Kernel Methods for Pattern Analysis},
}

@BOOK{shawe-taylor2004,
  AUTHOR = {Shawe-Taylor, John and Cristianini, Nello},
  PUBLISHER = {Cambridge University Press},
  DATE = {2004},
  TITLE = {Kernel Methods for Pattern Analysis},
}

@ARTICLE{srvf,
  AUTHOR = {Srivastava, Anuj and Klassen, Eric and Joshi, Shantanu H. and Jermyn, Ian H.},
  DATE = {2011},
  DOI = {10.1109/tpami.2010.184},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {7},
  PAGES = {1415--1428},
  TITLE = {Shape Analysis of Elastic Curves in Euclidean Spaces},
  VOLUME = {33},
}

@ARTICLE{stokes-antibiotics,
  AUTHOR = {Stokes, Jonathan M and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M and MacNair, Craig R and French, Shawn and Carfrae, Lindsey A and Bloom-Ackermann, Zohar and Tran, Victoria M and Chiappino-Pepe, Anush and Badran, Ahmed H and Andrews, Ian W and Chory, Emma J and Church, George M and Brown, Eric D and Jaakkola, Tommi S and Barzilay, Regina and Collins, James J},
  LANGUAGE = {en},
  LOCATION = {United States},
  DATE = {2020},
  JOURNALTITLE = {Cell},
  NUMBER = {4},
  PAGES = {688--702.e13},
  TITLE = {A Deep Learning Approach to Antibiotic Discovery},
  VOLUME = {180},
}

@MISC{tavenard-dtw-diff,
  AUTHOR = {Tavenard, Romain},
  DATE = {2021},
  HOWPUBLISHED = {\url{https://rtavenar.github.io/blog/softdtw.html}},
  TITLE = {Differentiability of DTW and the case of soft-DTW},
}

@INPROCEEDINGS{thiry,
  AUTHOR = {Thiry, Louis and Arbel, Michael and Belilovsky, Eugene and Oyallon, Edouard},
  LOCATION = {Vienna (online), Austria},
  BOOKTITLE = {{International Conference on Learning Representation (ICLR 2021)}},
  DATE = {2021},
  FILE = {https://hal.science/hal-03114389/file/iclr2021_conference.pdf},
  TITLE = {{The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods}},
}

@REPORT{mitchell-inductive,
  AUTHOR = {{Tom Mitchell}},
  INSTITUTION = {Departement of Computer Science, Laboratory for Computer Science Research, Rutgers University},
  DATE = {1980},
  TITLE = {{The Need for Biases in Learning Generalizations}},
}

@MISC{qpax,
  AUTHOR = {Tracy, Kevin and Manchester, Zachary},
  URL = {https://arxiv.org/abs/2406.11749},
  DATE = {2024},
  EPRINT = {2406.11749},
  EPRINTCLASS = {math.OC},
  EPRINTTYPE = {arXiv},
  TITLE = {On the Differentiability of the Primal-Dual Interior-Point Method},
}

@BOOK{trefethen1997numerical,
  AUTHOR = {Trefethen, Lloyd N and Bau III, David},
  PUBLISHER = {Siam},
  DATE = {1997},
  TITLE = {Numerical linear algebra},
  VOLUME = {50},
}

@INPROCEEDINGS{squared-neural-families,
  AUTHOR = {Tsuchida, Russell and Ong, Cheng Soon and Sejdinovic, Dino},
  EDITOR = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2023},
  PAGES = {73943--73968},
  TITLE = {Squared Neural Families: A New Class of Tractable Density Models},
  VOLUME = {36},
}

@ARTICLE{vacher2021dimension,
  AUTHOR = {Vacher, Adrien and Muzellec, Boris and Rudi, Alessandro and Bach, Francis and Vialard, Francois-Xavier},
  DATE = {2021},
  JOURNALTITLE = {arXiv preprint arXiv:2101.05380},
  TITLE = {A Dimension-free Computational Upper-bound for Smooth Optimal Transport Estimation},
}

@UNPUBLISHED{vayer,
  AUTHOR = {Vayer, Titouan and Chapel, Laetitia and Courty, Nicolas and Flamary, Rémi and Soullard, Yann and Tavenard, Romain},
  DATE = {2020},
  NOTE = {preprint},
  TITLE = {Time Series Alignment with Global Invariances},
}

@ARTICLE{vayer2022time,
  AUTHOR = {Vayer, Titouan and Tavenard, Romain and Chapel, Laetitia and Flamary, Rémi and Courty, Nicolas and Soullard, Yann},
  DATE = {2022},
  JOURNALTITLE = {Transactions on Machine Learning Research},
  NOTE = {https://openreview.net/forum?id=JXCH5N4Ujy},
  TITLE = {Time Series Alignment with Global Invariances},
}

@BOOK{wendland2004scattered,
  AUTHOR = {Wendland, Holger},
  PUBLISHER = {Cambridge university press},
  DATE = {2004},
  TITLE = {Scattered data approximation},
  VOLUME = {17},
}

@INPROCEEDINGS{williams2001using,
  AUTHOR = {Williams, Christopher and Seeger, Matthias},
  BOOKTITLE = {Proceedings of the 14th annual conference on neural information processing systems},
  DATE = {2001},
  PAGES = {682--688},
  TITLE = {Using the {Nyström} method to speed up kernel machines},
}

@BOOK{younes,
  AUTHOR = {Younes, L.},
  PUBLISHER = {Springer Berlin Heidelberg},
  DATE = {2010},
  SERIES = {Applied Mathematical Sciences},
  TITLE = {Shapes and Diffeomorphisms},
}
