\section{Introduction}

Time-series are ubiquitous in data analysis applications with tasks such as forecasting, classification or clustering and in fields such as medecine, atmospheric sciences and weather, finance and economics, or robotics.

At its heart, time-series analysis factors in the time-dependent structure of collected data\footnote{We will also study this problem for filtering in \cref{ch:hmm}.}. If this structure is to be exploited, time-series cannot be compared as if they were vectors in Euclidean space. Indeed, the points of two time-series do not necessarily portray data captured at comparable times (we say they are not \emph{aligned}). In fact, the instances can have a different number of points!

Dynamic Time Warping (DTW) was introduced by \cite{dtw-sakoe} to compare time series and account for misalignment. DTW became the \emph{de facto} standard for comparing time series in analysis tasks, as well as a strong baseline for classification when combined with the nearest neighbor classifier\cite{dtw-baseline-1, dtw-baseline-2}. Intuitively, Dynamic Time Warping finds a minimal-cost alignment between two time series, while respecting the order of the points. In practice refinements of the DTW are used that constrain the possible matchings\cite{dtw-baseline-1}. This minimization is carried out in $O(nm)$ where the compared time-series are of length $n$ and $m$. DTW does not take into account when the samples occur, only their relative order. DTW is differentiable with respect to one of its inputs as soon as there is a unique solution to the matching (that does not make the DTW zero). Non-differentiability can happen in innocuous situations as illustrated in \cite{tavenard-dtw-diff}. Despite being non-smooth, DTW-based averaging methods such as DBA have been proposed\cite{dba-petitjean}.

Soft Dynamic Time Warping (Soft-DTW) was proposed in \cite{soft-dtw} as a smooth replacement for DTW. Empirically, Soft-DTW behaves similarly to DTW, has equivalent computational complexity, is differentiable, and works better than DTW-based DBA for averaging based methods like clustering.

\paragraph{Related work}
While DTW and Soft-DTW are the most widely used elastic distances on time-series, other distances include: Fr√©chet distance \citep{frechet-clustering}, the Continuous-DTW distance\citep{cdtw}, extensions of (soft)DTW to comparing the feature space as well \citep{dtw-incomparable,vayer2022time}.

Functional data analysis methods have also been proposed, especiallly around the seminal contribution of the Square-Root Velocity representation\citep{srvf}. Another approach is to use the moments of a functions, depending on its higher-order derivatives\citep{curve-moments}. Shape analysis methods have also been applied to time-series of shapes\citep{2108.05634,Durrleman2013}. Neural network methods including with neural networks that learn an alignment function from the dataset\citep{2106.11911,dtan,martinez22a}. These methods focus on the alignement task and do not develop dissimilarities between time-series.

\paragraph{Approach \& Contributions}
DiffyTW is divergence between functions on $[0,1]$ representing time-series. It is provably invariant to subset of smooth reparametrizations. The set of transformations it is invariant to can be tuned. It can be robustly and efficiently used to compare time-series, by solving a quadratic program. DiffyTW produces a dissimilarity score and a reparametrization that aligns the time series. DiffyTW is differentiable with respect to its input and can be integrated in wider machine learning workflows. We illustrate and study DiffyTW's behavior on time series empirically.

\paragraph{Notation} In this chapter, a time series is represented by a sample pattern $(t_i)_{1 \leq i \leq n}$ such that $0 = t_1 < \ldots < t_n = 1$ (not necessarily random) and values $(f_i)_{1\leq i\leq n}$ where $f_i \in\mathbb R^d$. $k$ is a positive definite kernel on $\mathbb R$ with reproducing kernel Hilbert space $\mathcal F$. $K$ is a positive definite kernel with reproducing kernel Hilbert space $\mathcal H$. Functions $f,g:[0,1] \to \mathcal X$, where $\mathcal X$ is a metric space, generally $\mathcal R^d$.

\section{DiffyTW}
In this section, we introduce DiffyTW, a divergence between functions $[0,1] \to \mathcal X$ which is invariant to a chosen class of diffeomorphic reparametrizations of $[0,1]$.

\subsection{Intuition}\label{sec:diffytw-intuition}
We call \emph{reparametrization} any increasing, diffeomorphic function $Q:[0,1] \to [0,1]$. Let $\mathcal Q$ be a set of such functions. Our goal is to concieve a divergence such that $d(f\circ Q, f) = 0$ for any $Q\in\mathcal Q$ and any $f:[0,1] \to \mathbb R^d$.

A key idea is to compare the integrals of $f\circ Q$ and $f$. Indeed, this allows one to apply the change of variable theorem from \cite{aliprantis1998principles} recalled in \cref{thm:change-of-variable} in \cref{ch:diffy}:
\begin{equation}
    \int_0^1 f\circ Q(x)Q^\prime(x)dx = \int_0^1 f(x)dx.
\end{equation}
The intuition is then that finding $q(x) = Q^\prime(x)$ makes the integrals match, and makes the difference between both integrals small. This was also proposed in \cref{ch:diffy} for handling invariances on general data spaces. We highlight differences with \cref{ch:diffy} in \cref{sec:diffytw-vs-diffy}.

If $q = Q^\prime$ is contained in the space we optimized over,
\begin{equation}
    q \mapsto \left\vert\int_0^1 f\circ Q(x)q(x) dx - \int_0^1 f(x)dx\right\vert
\end{equation}
is minimized by $q=Q^\prime$. However, in general, we cannot exhaustively specify the set of transformations the problem is invariant to, and $q=Q^\prime$ may not be the only minimizer in the space. Indeed, if $f:[0,1] \to \mathbb R^+$, then $q(x) = f(x) / f\circ Q(x)$ also minimizes this quantity.

\paragraph{}
Such spurious minimizers can be avoided with two refinements.

First, to ensure matchings work for all features of the time series, we embed their values with $\phi$, chosen as a high-dimensional embedding. This avoids the minimizer above, but also handles the case where $f(x)\in\mathbb R^d$. Indeed, if the match is not perfect, then $f$ and $g$ can be made close by favoring the coordinates with the highest magnitude. This is avoided when embedding with $\phi$.

Second, we enforce the search space to contain only derivatives of reparametrizations, i.e. we restrict functions $q$ to verify the two following constraints: $q(x) \geq 0, \forall x\in[0,1]$ and $\int_0^1 q(u)du=1$. As a byproduct, retieving $q$ allows us to reconstruct a reparametrization $Q$ and align the time series.

\subsection{Definition of DiffyTW}
In this work, we call reparametrization any function $Q: [0,1] \to [0,1]$ that is continuously differentiable on $[0,1]$, bijective and inscreasing. Note that we do not require $Q$ to be a diffeomorphism, since we make no assumption about the derivative of its inverse.

Let $\mathcal F$ be a space of continuous functions $[0, 1] \to \mathbb R$ with isolated zeros. Let $\mathcal F_{0,1} \subset \mathcal F$ a subset of $\mathcal F$ such that if $q\in\mathcal F_{0,1}$, $q \geq 0$ and $\int_0^1q(t)dt =1$. We denote $\mathcal Q$ the set of reparametrizations obtained by integrating the elements of $\mathcal F_{0,1}$, i.e.
\begin{equation}\label{eq:Q-from-F}
    \mathcal Q = \left\lbrace Q: x\in[0,1] \mapsto \int_0^x q(x)dx \mid q \in \mathcal F_{0,1}\right\rbrace.
\end{equation}
Note that all elements of $Q$ are indeed reparametrizations. If $Q\in\mathcal Q$, then $Q^\prime(x) \in\mathcal F_{0,1}$ and $Q^\prime$ is continuous, so $Q$ is continuously differentiable. Additionally, since $Q^\prime \geq 0$ except at isolated points, $Q$ is increasing. By definition, $Q(0)=0$ and $Q(1)=\int_0^1 Q^\prime(x)dx =1$, and $Q$ is a bijection.

\begin{definition}[DiffyTW]\label{def:diffytw}
Let $d: \mathcal X^{[0,1]} \times \mathcal X^{[0,1]} \to \mathbb R^+$ be for any $f, g\in\mathcal X^{[0,1]}$,
\begin{equation}\label{eq:diffytw}
    d(f, g) = \min_{q \in \mathcal F_{0,1}}\left\Vert \int_0^1 \phi(f(x))q(x)dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H.
\end{equation}
We call this divergence DiffyTW.
\end{definition}
Note that as soon as $f$ and $g$ and measurable, then $d$ is well defined. Indeed, since $\phi$ is bounded on $[0,1]$ and $q$ is integrable, the integrals are well-defined Bochner integrals and $d(f, g)$ is finite and well-defined.

We adopt the following notation for convenience:
\begin{equation}\label{def:diffytw-delta}
\Delta(f , g) = \left\Vert \int_0^1 \phi(f(x))dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H
\end{equation}


\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.45\textwidth]{ch3-diffytw/figures/trace-demo.pdf}
\includegraphics[width=0.45\textwidth]{ch3-diffytw/figures/trace-demo-Q.pdf}
\end{center}
\caption[Time series alignment with DiffyTW]{Time series aligment with DiffyTW: in this example, time series $g$ is transformed into $f=g\circ Q$ by reparametrization $Q$. The reparametrization computed by DiffyTW is $\hat Q$, and reconstruction $\hat f$ of $f$, which agrees with $f$.\textsuperscript{*}}
\small\textsuperscript{*} Details: An instance $g$ of the Trace dataset from the UCR repository is transformed into $f$ by a reparametrization $Q$ induced by a NNLM from $\mathcal F_{0,1}^{M_w}$ with $M_{w}=30$ and $\eta_w= 100$ for the RBF kernel (see right plot, continous line). $\hat d_M$ is computed using parameters $M=100$, $\eta =100$ for the Laplace kernel (kernel $k$ that over time) and $\gamma=10$ for the RBF kernel (kernel $K$ over features). $\hat Q$ is the reconstruction of $Q$ from the solution to DiffyTW.
\end{figure}

\subsection{Invariance to time-warping}
Following \cref{sec:diffytw-intuition}, we show that for any function $f: [0,1] \to \mathbb R^d$ and $Q\in\mathcal Q$, $d(f\circ Q, f)=0$.
\begin{theorem}\label{thm:diffytw-invariance}
Let $Q$ be a reparametrization of $[0,1]$ such that $Q^\prime \in \mathcal F_{0, 1}$. Then, for any $f: [0,1] \to \mathcal X$,
\begin{equation}
    d(f\circ Q, f) = 0.
\end{equation}
\end{theorem}

\begin{proof}
We apply the Change of variable theorem \citep{aliprantis1998principles}:
\begin{equation}
\int_0^1 \phi(f\circ Q(x))\vert Q^\prime(x)\vert dx = \int_0^1 \phi(f(x))dx.
\end{equation}
Since $Q$ is a reparametrization, it is increasing and $Q^\prime(x) \geq 0$ so we can drop the absolute value.

Because $Q^\prime \in \mathcal F_{0,1}$,
\begin{equation}
    d(f\circ Q, f) \leq \left \Vert \int_0^1 \phi(f\circ Q(x))Q^\prime(x)dx - \int_0^1 \phi(f(x))dx\right\Vert_\mathcal H^2
\end{equation}
and finally, $d(f\circ Q, f) \leq 0$. Since $d(f\circ Q, f) \geq 0$, we have proven the result.
\end{proof}

Intuitively, if $Q^\prime$ is an optimal solution to the optimization problem defined by $d(f, g)$, then $Q$ aligns $f$ and $g$. At this stage, we do not consider whether a solution is attained or if it is unique. But this intuition is important for the following properties, and will be formally true when we consider Discrete DiffyTW.

\begin{proposition}\label{prop:diffytw-properties} Let $f, g:[0,1] \to\mathbb R^d$. Then, DiffyTW verifies the following properties:
\begin{enumerate}
\item[(a)]if $id \in Q$, $d$ is a semi pseudo-distance, i.e. for any $f, g: [0,1] \to\mathcal X$,  $d(f, g)\geq 0$ and $d(f, f)=0$.
\item[(b)]if $id \in \mathcal Q$ and $\varepsilon >0$, for any $Q_0\in\mathcal Q$ such that $\Delta(f\circ Q_0^{-1}, g) \leq d(f, g) + \varepsilon$, then $d(f \circ Q_0^{-1}, g) \leq d(f, g) + \varepsilon$. In particular, if $\Delta(f\circ Q_0^{-1}, g) = d(f,g)$ then $d(f\circ Q_0^{-1}, g)=d(f, g)$.
\item[(c)] if $Q_0$ is a reparametrization that leaves $\mathcal Q$ invariant (i.e. $\lbrace Q\circ Q_0 ~\vert~ Q\in \mathcal Q \rbrace = \mathcal Q$), then $d(f\circ Q_0^{-1}, g) = d(f, g)$.
\end{enumerate}
\end{proposition}
\begin{proof}

To prove (a), notice that $d(f,g)$ is always verified and if $id\in\mathcal Q$, $d(f,f) \leq \Delta(f, f)=0$.
The proof of (b) follows from noticing that: $d(f\circ Q_0^{-1}, g) \leq \Delta(f\circ Q_0^{-1}, g)$ because $id\in\mathcal Q$.
The proof of (c) follows from noticing that:
\begin{align}
d(f \circ Q_0^{-1}, g) &= \min_{Q\in\mathcal Q}\Delta(f\circ Q_0^{-1} \circ Q^{-1}, g)\\
& = \min_{R\in\mathcal Q} \Delta(F\circ R^{-1}, g)\\
& = d(f, g),
\end{align} where the second equality comes from the change of variable $Q \to R \circ Q_0$, which does not modify the set we optimize over by hypothesis.
\end{proof}


\section{Computing DiffyTW in practice}\label{sec:computing-diffytw}
DiffyTW is defined over the space of functions $[0,1] \to \mathcal X$ and the optimization problem is defined over an arbitrary set of functions $\mathcal F_{0,1}$, which determines the set of transformations $\mathcal Q$ the divergence is invariant to. In this section, we show DiffyTW can be computed for sampled time-series and with an expressive set of reparametrizations $\mathcal F_{0,1}^M$ by solving a Quadratic program. We call this divergence Discrete DiffyTW.


\subsection{Intuitive derivation for Discrete DiffyTW}
We define Discrete DiffyTW is such a way that it is consistent with Diffy, if we consider piece-wise constant functions that interpolate the time-series we consider. We derive Discrete DiffyTW intuitively below before defining it formally in \cref{sec:consistent}.

\paragraph{Time-series as piece-wise constant functions} Times series are discrete objectifs, with data arriving at different time points. By representing them as piece-wise constant functions (i.e. step functions), we take into account the relative position between the sample points and their values. %Consider $f$ and $g$ two piece-wise constant functions adapted to $(x_1, \ldots, x_n)$ and $(y_1, \ldots, y_m)$. A piece-wise constant function is adapted to an increasing sequence of distinct points in its domain if $f$ is a constant between each point.
Thus we define $\hat d(\hat f, \hat g) =d(f, g)$ where $f$ and $g$ are piece-wise constant functions that interpolate $\hat f$ and $\hat g$, i.e. such that $f([x_i, x_{i+1}[)=\lbrace f_i\rbrace$ and $g([y_j, y_{j+1}[ = \lbrace g_j \rbrace$. By doing so, we can rewrite the integrals in the definition of \cref{eq:diffytw} as follows:
\begin{align}
    d(f, g) &= \min_{q \in \mathcal F_{0,1}}\left\Vert \int_0^1 \phi(f(x))q(x)dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H\label{eq:diffytw-step-1}\\
            &\label{eq:diffytw-step-2}= \min_{q\in\mathcal F_{0,1}} \left \Vert \sum_{i=1}^{n-1} \underbrace{\int_{x_i}^{x_{i+1}}q(u)du}_{I(q, x_i, x_{i+1})} \phi(f(x_i)) - \sum_{j=1}^{m-1} (y_{j+1} - y_j)\phi(g(y_j))\right\Vert_\mathcal H^2
\end{align}

\paragraph{Linear parametrization of $\mathcal F_{0,1}$} If $\mathcal F_{0,1}$ is chosen to be very general, then the optimization problem in \cref{eq:diffytw} is intractable. Furthermore, from a modelling perspective, very large reparametrization classes may encompass transformations that are not representative of the invariance structure present in data. We propose to choose $\mathcal F_{0,1}^M\subset \mathcal F_{0,1}$ such that solving \cref{eq:diffytw} is feasible and such that $\mathcal F_{0,1}^M$ is expressive enough to encompass a wide class of transformations.
%Anotherway of seeing things -- in the spirit of the approximation litterature -- is to consider how $\mathcal F_{0, 1}^M$ grows as $M \to \infty$.

In this chapter, we rely on Non-Negative Linear Models in the style of Gaussian Mixture Models of the form
\begin{equation}\label{eq:nnlm}
\mathcal F_{0,1}^M = \left\lbrace q(x) = \sum_{k=1}^M q_k k(x, \tilde x_k) \mid q_k\in\mathbb R^+, \int_0^1q(u)du=1\right\rbrace.
\end{equation}
%$q(x) = \sum_{k=1}^M q_k k(x, \tilde x_k)$ where $q_k$, $k$ and $\tilde x$ are chosen such that $q\in\mathcal F_{0,1}$.
For $q\in\mathcal F_{0,1}^M$, $\int_x^y q(u)du)$ can be computed in closed-form (see \cref{ex:H-rbf,ex:H-laplace}) and is linear in the coordinates $q$. This also implies that \cref{eq:nnlm} are linear equality and inequality constraints in $q$.

We define $d_M(f, g)$ as $d(f,g)$ replacing $\mathcal F_{0,1}$ by $\mathcal F_{0,1}^M$ in the optimization problem, i.e.:
\begin{equation}\label{eq:hat-d_M}
d_M(f, g) = \min_{q \in\mathcal F_{0,1}^M} \Vert \int \phi(f(x))q(x) - \int \phi(g(x))dx\Vert_\mathcal H^2
\end{equation}


\paragraph{Quadratric Program} We can now find a natural expression for $\hat d_M(\hat f, \hat g)$ which combines representing $\hat f$ and $\hat g$ by interpolating piece-wise constant functions and optimizing over $\mathcal F_{0,1}^M$. We exploit the linearity in the coordinates on $q$ in \cref{eq:diffytw-step-2} and develop the squared-norm. This leads to defining $\hat d_M(\hat f, \hat g)$ as the optimal value to a Quadratic program over the coordinates $q$ with a single equality constraint (corresponding to $\int_0^1q(x)dx = 1$) and $M$ inequality constraints ($q \geq 0$):
\begin{equation}\label{eq:discrete-diffytw-informal}
\hat d_M(\hat f, \hat g) :=\min_{\substack{q\in\mathbb R^{M}\\h^\top q=1\\0 \leq q}}\frac{1}{2}q^\top Pq - v^\top q + C
\end{equation}
We define $\hat d_M(\hat f, \hat g)$ formally in \cref{sec:consistent}. We then prove that \cref{eq:discrete-diffytw-informal} is consistent with \cref{def:diffytw}, that (under mild hypotheses) it is not sensitive to the sampling patterns of $\hat f$ and $\hat g$ as $n, m\to\infty$. Finally, we discuss computing it in practice.

\subsection{Discrete DiffyTW is consistent with DiffyTW}\label{sec:consistent}
We begin by formally defining Discrete DiffyTW, as motivated above:
\begin{definition}[Discrete DiffyTW]\label{def:discrete-diffytw} Let $\hat f = (x_i, f_i)_{1 \leq i\leq n}$ and $\hat g= (y_j, g_j)_{1\leq j\leq m}$. We define the discrete DiffyTW between discrete time series as the optimal value of the Quadratic program:
\begin{equation}\label{prob:qp}
    \hat d_M(\hat f, \hat g) :=\min_{\substack{q\in\mathbb R^{M}\\h^\top q=1\\0 \leq q}}\frac{1}{2}q^\top Pq - v^\top q + C
\end{equation}
where $P= 2\tilde H^\top K_{f, f}\tilde H$, $C= \delta^\top K_{g,g}\delta$ and $v= 2 \delta^\top K_{g,f}\tilde H$. $\delta\in\mathbb R^{m-1}$ is defined by $\delta_j = y_{j+1} - y_j$. $\tilde H$ is defined in \cref{eq:H}, $[K_{gf}]_{ij} = K(g_i, f_j)$, $[K_{ff}]_{ij} = K(f_i, f_j)$, and $[K_{gg}]_{ij} = K(g_i, g_j)$, where $K(z_1, z_2) = \langle \phi(z_1), \phi(z_2)\rangle_\mathcal H$.
\end{definition}

By definition, $P$ is symmetric, positive semi-definite. In practice, we consider regularization to ensure QP solvers find a solution. We discuss this in \cref{sec:solving-qp}.

Our definition of $\hat d_M(\hat f, \hat g)$ coincides with that of DiffyTW when applied to piece-wise constant functions that interpolate $\hat f$ and $\hat g$ as defined in the \cref{thm:prob-qp}.

\begin{theorem}\label{thm:prob-qp}
Let $\hat f = (x_i, f_i)_{1 \leq i\leq n}$ and $\hat g = (y_j, g_j)_{1 \leq j\leq m}$ be two sampled time series. We define $f[0, 1]\to\mathbb R^d$ (resp. $g$) as the piece-wise constant function defined by $f([x_i, x_{i+1}[) = \lbrace f_i\rbrace$ (resp. $g([y_i, y_{j+1}[) = \lbrace g_j\rbrace$). Then:
\begin{equation}
d_M(f, g) = \hat d_M(\hat f, \hat g)
\end{equation}
where $d_M(f, g)$ is defined in \cref{eq:hat-d_M} and $\hat d_M(\hat f, \hat g)$ is defined in \cref{def:discrete-diffytw}.
\end{theorem}

The proof of \cref{thm:prob-qp} stems from the development at the beginning of \cref{sec:computing-diffytw} and is presented in \cref{proof:prob-qp}. A consequence of \cref{thm:prob-qp} is that some of the invariance properties of DiffyTW transfer to Discrete DiffyTW. For instance:

\begin{corollary}\label{corollary:discrete-invariance}
Let $h: [0, 1] \to \mathbb R^d$ be a piece-wise constant function adapted to the sampling pattern $0 \leq x_1 < \ldots < x_n \leq 1$. Let $Q$ be a reparametrization such that $Q^\prime \in \mathcal F_{0,1}^M$. Consider $\hat f = (Q(x_i), h(x_i))$ and $\hat g = (x_i, h(x_i))$. Then,
\begin{equation}
    \hat d_M(\hat f, \hat g) = 0.
\end{equation}
\end{corollary}
\begin{proof}
$h \circ Q$ interpolates $\hat f$ and $h$ interpolates $\hat g$, so from \cref{thm:prob-qp}:
\begin{equation}
    \hat d_M(\hat f, \hat g) = d_M(h\circ Q, h) = 0
\end{equation}
\end{proof}


\subsection{Computing Discrete DiffyTW}\label{sec:solving-qp}

DiffyTW is defined by \Cref{prob:qp} as the solution to a quadratic program with $M$ variables and linear inequality and equality constraints. The problem is convex and strongly convex as soon as $K_{f,f}$ is positive definite.

It can be solved using an interior point Newton method, for which several open-source, off-the-shelf solvers exist. We use \texttt{Prox-QP} a solver based on the primal-dual proximal augmented Lagrangian method \citep{fabian}. The time complexity of this kind of method for our situation is $O(M^3)$. As a point of comparison, DTW has time complexity $O(nm)$ where $n, m$ are the lengths of the time series. We also incur quadratic cost of this order to build $K_{ff}$. This cost can be reduced using low-rank approximations for the kernel matrices, and at this stage the computation cost of solving a QP is not a limiting factor for DiffyTW.

Numerically, this sort of solver works best when $P$ is positive definite. To enforce this, we replace $P$ with $P + \lambda I$ in the defintion of Discrete DiffyTW. We analyse the effect of this regularization empricially in \cref{sec:diffytw-experiments}.

\subsection{Approximation error}
\cref{corollary:discrete-invariance} illustrates that the definition of Discrete DiffyTW is ``tight'' with respect to that of DiffyTW if the underlying signals are piece-wise constant and diffeomorphic. In this section, we raise three questions : how good is this approximation for arbitrary signals $f$ and $g$? how dependent is $\hat d_M(\hat f, \hat g)$ on the sampling patterns? how should one go about choosing $k$, $\tilde x$ and $K$? These questions are answered in \cref{thm:rectangle-approx}:

\begin{theorem}\label{thm:rectangle-approx}
Let $f,g:[0,1] \to \mathbb R^d$ be two $L$-Lipchitz functions. Let $(x_i)_{1\leq i \leq n}$ and $(y_j)_{1\leq j\leq m}$ two sampling patterns. We denote $\hat f = (x_i, f(x_i))$ and $\hat g = (y_j, g(y_j))$.
If $K$ is the RBF kernel with $\gamma > 0$,
\begin{equation}
    \left\vert \sqrt{d_M(f, g)} - \sqrt{\hat d_M(\hat f_n, \hat g_m)}\right\vert \leq \sqrt{2\gamma L}\left(\sum_{i=1}^{n-1} (x_{i+1} - x_i)^{3/2} + \sum_{j=1}^{m-1} (y_{j+1} - y_j)^{3/2}\right)
\end{equation}

In particular, if $(x_i)$ and $(y_j)$ are evenly spaced in $[0,1]$, then:
\begin{equation}
    \left\vert \sqrt{d_M(f, g)} - \sqrt{\hat d_M(\hat f_n, \hat g_m)}\right\vert \leq \sqrt{2\gamma L}\left(\frac{1}{\sqrt{n}} + \frac{1}{\sqrt{m}}\right)
\end{equation}
\end{theorem}

The proof on \cref{thm:rectangle-approx} is included in \cref{sec:proof-rectangle-approx}.

\paragraph{}

In particular, if $f = g \circ Q$ where $Q^\prime \in\mathcal F_{0, 1}^M$ then $d_M(f, g)=0$ and $n=m$, then \cref{thm:rectangle-approx} implies that
\begin{equation}
\hat d_M(\hat f_n, \hat g_n) = O\left(\frac{1}{n}\right).
\end{equation}

\paragraph{Parameter choice}
Discrete DiffyTW has several hyper-parameters in particular the choice of $\mathcal F_{0,1}^M$, i.e. the basis function kernel $k(\cdot, \tilde x)$ and the anchor points $(\tilde x_k)_{1\leq j\leq M}$, and of $K$ the kernel on time-series features. When Discrete DiffyTW is integrated into a machine learning algorithm, typical hyperparameter tuning strategies such as cross-validation can be applied. Below, we give intuition into the impact of the different hyperparameters.

\subparagraph{Choosing $\mathcal F_{0,1}^M$} The choice of basis function and the number and positions of the anchor points control the richness of the set of transformations that can be identified in data. The parameters of $k$ as well as $M$ should be chosen as a function of prior knowledge about the transformations present. For $k$ the RBF or Laplace kernel of parameter $\eta>0$, smaller values of $\eta$ and larger values of $M$ lead to richer function spaces. In addition, choosing the Laplace kernel makes for a much richer space than choosing the RBF kernel\cite{shawe-taylor}. Generally, $M$ is chosen to be much smaller than $n$ and $m$. Note that a richer space is not necessarily the goal as ``simple'' reparametrizations may make the most sense for an application.

\subparagraph{Choosing $\phi$} The choice of kernel on time-series features has no impact on invariance for DiffyTW in \cref{thm:diffytw-invariance}. However, empirically, we observe that it plays a role with Discrete DiffyTW. This is confirmed in \cref{thm:rectangle-approx} as $\gamma$, the RBF kernel parameter intervenes controls the value of the bound. Large values of $\gamma$ lead to larger values of the bound. Intuitively, this is explained by the fact that large values of $\gamma$ lead to a very discriminative kernel $K$. Loosely, any two distinct points seem very different. This implies having a large number of samples, in order to have sufficient overlap between the time-series. On the other hand, a smaller $\gamma$ leads to a smaller bound, which means that $d$ and $\hat d_M$ are close. A small value for $\gamma$ means that $K$ is not very discriminative and any two values seem close. This applies to both $d$ and $\hat d_M$. In other words, $d$ (and $\hat d_M$) will be small for both pairs of diffeomorphic time-series and non-diffeomorphic time-series.

\section{Differentiability}
Differentiability makes Discrete DiffyTW effective as a building block inside of machine learning algorithms, for example as a loss function for training time-series generation algorithms.

We consider differentiability of $V(\theta) = \hat d_M(\hat f, \theta)$ and our goal is to compute $\nabla V(\theta)$, the gradient of $V$ with respect to $\theta$. Here, $\theta$ takes the place of $\hat g$ but we consider its sample pattern fixed. Additionally, $\nabla V(\theta)$ can be chained to extend to the (more common) case where we wish to compare $\hat f$ to $\hat g(\theta)$, a time-series generated by a model.

\paragraph{}
In this section, we denote:
\begin{equation}\label{prob:fullqp}
    V(\theta) = \min_{q\in\mathcal C} \ell(q, \theta)
\end{equation}
where $\ell(q, \theta) = \frac{1}{2} q^\top Pq - v(\theta)^\top q + C(\theta)$ and $\mathcal C = \left\lbrace q\in\mathbb R^N ~\vert~ h^\top q=1, q \geq 0\right\rbrace$ and $P$, $v$, $C$ and $h$ are defined as in \cref{def:discrete-diffytw}.

The difficulty is differentiating through the $\min$ operator since the optimal $q^*$ depends on $\theta$. $V$ is differentiable and its gradient can be computed from the optimal solution $q^*$. Our proof is direct and does not rely on the Implicit Function Theorem (see below for a comparaison with other approaches).

\begin{theorem}\label{thm:diffytw-grad}
    If $K$ is the RBF kernel and $\ell(\cdot, \theta)$ is strictly convex, then $V(\theta)$ is differentiable and
    \begin{equation}
         JV(\theta) = J_\theta \ell(q^*(\theta), \theta) + \nabla C(\theta),
    \end{equation} where $q^*(\theta)$ is the unique solution to \cref{prob:fullqp}.
\end{theorem}

In particular, so long as $P$ is positive definite, $V$ is differentiable. For differentiability, having a unique solution is not necessary, but multiple solutions make the expression of the gradient less straightforward and may cause non-differentiability. Following \cref{sec:solving-qp}, since $P$ is positive definite in practice, assume that $\ell(\cdot, \theta)$ is strictly convex.

We prove \cref{thm:diffytw-grad} directly, as a special case of the proof from \cite{shapiro} and \cite{lee}, which give sufficient conditions for differentiability of quadratic programs (see \cref{sec:proof-diffytw-grad}). Note the same result from \citep{shapiro} is used in \cite{tavenard-dtw-diff} for the differentiability of DTW.

\paragraph{Other methods for differentiability of QPs}
Differentiating through convex problems is a subject of interest in the machine learning and optimization communities. We recall the three main approaches to this question: analytical differentiation (when a closed-form solution to the convex problem is known, \emph{e.g.\ } unconstrained QPs), unrolling (differentiating through the operations in the algorithm used to solve the convex problem), implicit differentiation (applying the implicit function theorem).

The latter method is used in the deep learning and robotics communities to integrate QPs as layers in neural networks\citep{QP-Layer,optnet}. These works study differentiation the optimal solution of a Quadratic Program for which the constraints also depend on $\theta$. To handle this, the KKT conditions can be expressed as an implicit function problem, and the implicit function theorem applied to differentiate with repect to all parameters implicitly. Handling constraints can make differentiation impossible since when using QPs in a iterative optimization method while training a neural network the constraints may become infeasible. In this case, Extended Conservative Jacobians of the closest feasible QP can be computed instead \citep{QP-Layer}. \citet{optnet} take another approach to handling infeasibility by adding slack variables. As a sanity check, we verify that we recover our result using their method for our setting in \cref{sec:lagrangian}.

\section{Experiments}
DiffyTW is implemented in the \texttt{difftw} library, open-sourced at \url{github.com/theophilec/diffytw}. This library implements the computation of DiffyTW, the gradient computation using the \texttt{jax} library. It can be used with different QP solvers. When computing DiffyTW, the \texttt{Prox-QP} solver is used \cite{fabian}. When computing the gradient, the \texttt{qpax} solver, introduced by \citet{qpax}.

%In this section, we demonstrate and study DiffyTW's behavior on the UCR dataset. Some takeaways of these experiments are:

%\begin{itemize}
%    \item DiffyTW captures synthetic reparametrizations.
%    \item DiffyTW is sensitive to the choice of $\gamma$ (and more generally to the choice of $\phi$), including with synthetic warpings. Making DiffyTW more robust to this choice is key to making it useful for applied settings (such as averaging), and this is left for future work.
%    \item Regularizing the choice of $q$ has significant effect on the robustness of DiffyTW. Regularization is not studied in the previous sections and it is incorporated in practice by padding the Hessian matrix, which is necessary for numerical reasons. Adding regularization to the definition of DiffyTW (in the spirit of Diffy in \cref{ch:diffy}) and analyzing its role is left for future work.
%\end{itemize}


\paragraph{Experiment design}
In the experiments below, $\tilde X = (\tilde x_k)_{1\leq k \leq M}$ are chosen regularly spaced in $[0,1]$. The RBF and Laplace kernels are considered on $[0,1]$ with parameter $\eta >0$. We choose $\mathcal F_{0,1}^M$ to be the set of functions of the form $q(x) = \sum_{k=1}^Mq_k k_\eta(x, \tilde x_k)$ where $q\in\mathbb R^M$ verifies: $q \geq 0$, $g^\top q =1$, and $\int_0^1 q(u)du =1$.
We consider $K$ the kernel on time-series features, either the RBF or Laplace kernel with parameter $\gamma> 0$ (this corresponds to $\phi$). We only consider unidimensional time series in this thesis, but DiffyTW can readily be applied to multidimensional time series.

\paragraph{Datasets} We rely on the UCR dataset \citep{ucr}. Specifically we use the \texttt{Trace} and \texttt{50words} datasets. We compare elements of datasets and synthetic transformations of times-series from the UCR dataset. In that case, we consider the linear interpolation of $g \circ Q$. This relates to the bound in \cref{thm:rectangle-approx}.

\paragraph{Aligning distinct time-series instances}
\todo[inline]{Write}

\paragraph{Aligning time-series instances with synthetic reparametrizations}
Consider $g$ an instance from either \texttt{Trace} and \texttt{50words}. We generate a reparametrization $Q$ from a space of the form $\mathcal F_{0,1}^{N}$ where $N = 100$, and compute $\hat d_M$ on $f$ and $g$ with $\mathcal F_{0,1}^M$.
\todo[inline]{Write}

\paragraph{Sensitivity to $\gamma$}

\todo[inline]{Write}
See \cref{fig:gamma-sensitivity-50words}.

\begin{figure}[ht!]
\label{fig:gamma-sensitivty-50words}
\begin{center}
\includegraphics[width=\textwidth]{ch3-diffytw/figures/Figure-50words-rbf-30_rbf-100-laplace-10000.0-100-1.0e-05.pdf}
\end{center}
\caption[Sensitivity to choice of $\gamma$]{Sensitivity to choice of $\gamma$. The quality of alignment by DiffyTW is variable with the choice of $\gamma$. There is an optimal choice of $\gamma$ that minimizes the relative error in $\mathcal F$ (bottom right).}
\small Details: $g$ is an instance of the 50words dataset.
$Q$ is sampled 200 times using the RBF kernel with parameter $\eta=100.0$ and $M_w=30$ anchor points.
For each $Q$, DiffyTW is computed between $g\circ Q$ and $g$.
$\mathcal F_{0,1}^M$ is defined with $M=100$ anchor points and the RBF kernel with parameter $\eta=100.0$ on $[0,1]$.
The Laplace kernel is chosen on the with varying parameter $\gamma$ between $10^{-4}$ and $10^{4}$.
\end{figure}
