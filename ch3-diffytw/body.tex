\section{Introduction}

\begin{itemize}
   \item Divergence between time-series that is robust to warping
   \item Differentiable to be able to be used in ML algorithms
   \item Justified by theory (which warps are captured, ...)
   \item Experimentally behaves like DTW (baseline)
\end{itemize}

We introduce DiffyTW a dissimilarity between time series, invariant to diffeomorphic reparametrizations of $[0,1]$. It can be computed in practice by solving a quadratic program, and is differentiable with respect to its inputs.

\subsection*{Notation}

$k$ is a kernel. We denote $t_i \in [0, 1]^n$ such that $0 \leq t_1 < \ldots < t_n \leq 1$. Time series are considered to be a couple of sample points (not necessarily random) $(t_i)_{1 \leq i \leq n}$ and values $(f_i)_{1\leq i\leq n}$ where $f_i \in\mathbb R^d$.

\section{DiffyTW}
\emph{In this section, we introduce \texttt{DiffyTW}, a dissimilarity between functions $[0,1] \to \mathcal X$ which is invariant to a chosen class of diffeomorphic reparametrizations of $[0,1]$.}

\subsection{Intuition}
Let $\mathcal Q$ be a set of increasing diffeomorphisms on $[0,1]$. Our goal is to concieve a divergence such that $d(f\circ Q, f) = 0$ for any $Q\in\mathcal Q$ and any $f:[0,1] \to \mathbb R^d$. A key idea is to compare the integrals of $f\circ Q$ and $f$. Indeed, this allows one to apply the change of variable theorem:

\begin{equation}
\int_0^1 f\circ Q(x)Q^\prime(x)dx = \int_0^1 f(x)dx.
\end{equation}

Then
\begin{equation}
\int_0^1 f\circ Q(x)q(x) dx - \int_0^1 f(x)dx
\end{equation}

is minimized by $q=Q^\prime$ over $q\in \mathcal F$ where $\mathcal F = \left\lbrace Q^\prime, Q\in\mathcal Q\right\rbrace$. However, $q=Q^\prime$ is not necessarily the only minimizer. Indeed, if $f:[0,1] \to \mathbb R$, then $q(x) = f(x) / f\circ Q(x)$ also minimizes this quantity. To avoid this spurious minimizer, we embed values of the two functions using an embedding function $\phi:\mathbb R^d \to \mathcal H$ where $\mathcal H$ is a Hilbert space. $\phi$ can be any map, for example the feature map of a deep neural network, a hand-crafted feature map, random warping map, or indeed the RKHS embedding of a positive definite kernel $k_\mathcal X$.

In the end, we arrive to the following definintion for Diffy.

\subsection{Definition of DiffyTW}

Let $\mathcal F$ be a space of functions $[0, 1] \to \mathbb R$. Let $\mathcal F_{0,1} \subset \mathcal F$ be a subset of $\mathcal F$ such that if $q\in\mathcal F_{0,1}$, $q\geq 0$ and $\int_0^1q(t)dt =1$. $\mathcal F_{0,1}$ defines the class of diffeormophic reparametrizations the divergence is invariant to through its derivatives. We denote $\mathcal Q$ the set of reparametrizations obtained by integrating the elements of $\mathcal F_{0,1}$, i.e.
\begin{equation}
    \mathcal Q = \left\lbrace Q: x\in[0,1] \mapsto \int_0^x q(x)dx ~\vert~ q \in \mathcal F_{0,1}\right\rbrace
\end{equation}

\begin{definition}[DiffyTW]\label{def:diffytw}
Let $d: \mathcal X^{[0,1]} \times \mathcal X^{[0,1]} \to \mathbb R^+$ be for any $f, g\in\mathcal X^{[0,1]}$,
\begin{equation}\label{eq:diffytw}
    d(f, g) = \min_{q \in \mathcal F_{0,1}}\left\Vert \int_0^1 \phi(f(x))q(x)dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H.
\end{equation}
We call this divergence DiffyTW.
\end{definition}

Note that as soon as $f$ and $g$ and measurable, then $d$ is well defined. Indeed, since $\phi$ is bounded on $[0,1]$ and $q$ is integrable, the integrals are well-defined Bochner integrals and $d(f, g)$ is finite and well-defined.

\paragraph{Time-series alignment}
If $f$ and $g$ are two functions representing two time-series, DiffyTW aligns $f$ and $g$. Indeed, we can --applying the change of variable theorem -- rewrite \cref{eq:diffytw} as
\begin{equation}\label{eq:diffytw-Q}
    d(f, g) = \min_{Q \in \mathcal Q} \left\Vert \int_0^1 \phi(f \circ Q^{-1}(x))dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H
\end{equation}

\noindent \Cref{eq:diffytw-Q} shows we that the minimizer of the optimization problem in \cref{eq:diffytw} is the derivative of the reparametrization that aligns $f$ with $g$ in terms of the embedding $f \mapsto \int \phi(f(x))dx$.

We adopt the following notation for convenience:
\begin{equation}\label{def:diffytw-delta}
\Delta(f , g) = \left\Vert \int_0^1 \phi(f(x))dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H
\end{equation}

\missingfigure{demo alignment}

\paragraph{Comparaison with elastic distances}
\subparagraph{Comparaison with Dynamic Time Warping} Dynamic time warping minimizes the discrete Euclidean distance (although other pairwise distances could be used) between matched data points with a bijective and non-decreasing matching:
\begin{equation}\label{eq:dtw-1}
d_{DTW}((f_1, \ldots, f_n), (g_1, \ldots, g_m)) = \min_{\sigma \in\mathcal A_{n, m}} \sum \left\Vert f_i - g_{\sigma(j)}\right\Vert_2
\end{equation}


\subparagraph{Comparaison with Diffy}
Even though its intuition is similar, DiffyTW is different from Diffy, presented in \cref{ch:ch1-diffy} and \cite{cantelobre-diffy}. Indeed,

\subparagraph{Comparaison with FrÃ©chet distance}



\subsection{Invariance to time-warping}
DiffyTW was designed to be equal to zero when comparing functions that are diffeomorphic one to the other, i.e. $d(f\circ Q, f)=0$. We show this is \cref{thm:diffytw-invariance}:


\begin{theorem}\label{thm:diffytw-invariance}
Let $Q$ be a reparametrization of $[0,1]$ such that $Q^\prime \in \mathcal F_{0, 1}$. Then, for any $f: [0,1] \to \mathcal X$,
\begin{equation}
    d(f\circ Q, f) = 0.
\end{equation}
\end{theorem}

\begin{proof}
Let $f: [0,1] \to\mathbb R^d$ a measurable function. $Q:[0,1] \to [0,1]$ is continuously differentiable, bijective and $Q^{-1}$ is continuously differentiable. We apply the Change of variable theorem:
\begin{equation}
\int_0^1 \phi(f\circ Q(x))\vert Q^\prime(x)\vert dx = \int_0^1 \phi(f(x))dx.
\end{equation}

Since $Q$ is a reparametrization, it is increasing and $Q^\prime(x) \geq 0$ so we can drop the absolute value.

Since $Q^\prime \in \mathcal F_{0,1}$,
\begin{equation}
    d(f\circ Q, f) \leq \left \Vert \int_0^1 \phi(f\circ Q(x))Q^\prime(x)dx - \int_0^1 \phi(f(x))dx\right\Vert_\mathcal H^2
\end{equation}
and finally,
\begin{equation}
    d(f\circ Q, f) \leq 0.
\end{equation}
Since $d(f\circ Q, f) \geq 0$, we have proven the result.
\end{proof}


Of course, in practice, one wishes to compute $d(f, g)$ where $f$ and $g$ are not exactly diffeomorphic. In \cref{prop:diffytw-properties} we show that some intuitive invariance properties remain:
\begin{proposition}\label{prop:diffytw-properties} Let $f, g:[0,1] \to\mathbb R^d$.
\begin{enumthm}
\item if $id \in Q$, $d$ is a semi pseudo-distance, i.e. for any $f, g: [0,1] \to\mathcal X$,  $d(f, g)\geq 0$ and $d(f, f)=0$.\label{prop:properties-1}
\item if $id \in \mathcal Q$ and $Q_0\in\mathcal Q$ is such that $\Delta(f\circ Q_0^{-1}, g) \leq d(f, g) + \varepsilon$ then, $d(f \circ Q_0^{-1}, g) \leq d(f, g) + \varepsilon$. In particular, if $\Delta(f\circ Q_0^{-1}, g) = d(f,g)$ then $d(f\circ Q_0^{-1}, g)=d(f, g)$.\label{prop:properties-2}
\item if $Q_0$ is a reparametrization that leaves $\mathcal Q$ invariant (i.e. $\lbrace Q\circ Q_0 ~\vert~ Q\in \mathcal Q \rbrace = \mathcal Q$), then $d(f\circ Q_0^{-1}, g) = d(f, g)$.\label{prop:properties-3}
\end{enumthm}
\end{proposition}

\begin{proof} \cref{prop:properties-1} is clear. Proof of \cref{prop:properties-2} follows from noticing that:
\begin{equation}
d(f\circ Q_0^{-1}) \leq \Delta(f\circ Q_0^{-1}, g)
\end{equation}
Finally, proof of \cref{prop:properties-3} follows from noticing that:
\begin{align}
d(f \circ Q_0^{-1}, g) &= \min_{Q\in\mathcal Q}\Delta(f\circ Q_0^{-1} \circ Q^{-1}, g)\\
& = \min_{R\in\mathcal Q} \Delta(F\circ R^{-1}, g)\\
& = d(f, g),
\end{align} where the second equality comes from the change of variable $Q \to R \circ Q_0^{-1}$, which does not modify the set we optimize over by hypothesis.
\end{proof}

DiffyTW is invariant to reparametrizations it searches over. However, it is also invarint to different transformations when $id \in \mathcal Q$. This means that the optimizer (when it exists) $Q \in \mathcal Q$ does not align $f$ and $g$, but $d(f,g)=0$ anyways. When $id \in \mathcal Q$, this is the case for any integral preserving transformations, such as time-flipping ($x \mapsto f(-x)$).

\section{Computing DiffyTW in practice}\label{sec:computing-diffytw}
DiffyTW is defined over the space of functions $[0,1] \to \mathcal X$ and the optimization problem is defined over an arbitrary set of functions $\mathcal F_{0,1}$, which determines the set of transformations $\mathcal Q$ the divergence is invariant to. In this section, we show DiffyTW can be computed for sampled time-series and with an expressive set of diffeomorphims $\mathcal F_{0,1}^M$ by solving a Quadratic program. We show that this approximate computation is exact if we assume the underlying signals are piece-wise constant. When the signals have regularity, we show that as the number of sample points grow, the approximated DiffyTW value approaches that compyuted on the underlying signals.

\emph{\textbf{In what follows, we highlight the main ingredients and steps in our reasoning.} Formal definitions of components, statements of results and proofs follow in the rest of the section and related appendices.}

\paragraph{Time-series as piece-wise constant functions} Times series are discrete objectifs, with data arriving at different time points. By representing them as piece-wise constant functions (i.e. step functions), we take into account the relative position between the sample points and their values. %Consider $f$ and $g$ two piece-wise constant functions adapted to $(x_1, \ldots, x_n)$ and $(y_1, \ldots, y_m)$. A piece-wise constant function is adapted to an increasing sequence of distinct points in its domain if $f$ is a constant between each point.
Thus we define $\hat f(\hat f, \hat g) =d(f, g)$ where $f$ and $g$ are piece-wise constant functions adapted to $\hat f$ and $\hat g$. By doing so, we can rewrite the integrals in the definition of \cref{eq:diffytw} as follows:
\begin{align}
    \hat d(\hat f, \hat g) &= \min_{q \in \mathcal F_{0,1}}\left\Vert \int_0^1 \phi(f(x))q(x)dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H\label{eq:diffytw-step-1}\\
            &\label{eq:diffytw-step-2}= \min_{q\in\mathcal F_{0,1}} \left \Vert \sum_{i=1}^{n-1} \underbrace{\int_{x_i}^{x_{i+1}}q(u)du}_{I(q, x_i, x_{i+1})} \phi(f(x_i)) - \sum_{j=1}^{m-1} (y_{j+1} - y_j)\phi(g(y_j))\right\Vert_\mathcal H^2
            %&\label{eq:diffytw-step-3}=\min_{\substack{q\in\mathbb R^{M}\\h^\top q=1\\0 \leq q}}\frac{1}{2}q^\top Pq - v^\top q + C
\end{align}

\paragraph{Linear parametrization of $\mathcal F_{0,1}$} If $\mathcal F_{0,1}$ is chosen to be very general, then the optimization problem in \cref{eq:diffytw} is intractable. Furthermore, from a modelling perspective, very large diffeomorphism classes may encompass transformations that are not representative of the invariance structure present in data. We propose to choose $\mathcal F_{0,1}^M\subset \mathcal F_{0,1}$ such that solving \cref{eq:diffytw} is feasible and such that $\mathcal F_{0,1}^M$ is expressive enough to encompass a wide class of transformations. Anotherway of seeing things -- in the spirit of the approximation litterature -- is to consider how $\mathcal F_{0, 1}^M$ grows as $M \to \infty$.

In this work, we rely on Non-Negative Linear Models in the style of Gaussian Mixture Models of the form
\begin{equation}\label{eq:nnlm}
q(x) = \sum_{k=1}^M q_k k(x, \tilde x_k)\text{ such that } q_k\in\mathbb R^+, \int_0^1q(u)du=1
\end{equation}
%$q(x) = \sum_{k=1}^M q_k k(x, \tilde x_k)$ where $q_k$, $k$ and $\tilde x$ are chosen such that $q\in\mathcal F_{0,1}$.
For $q\in\mathcal F_{0,1}^M$, $I(q, x, y)$ can be computed in closed-form (see \cref{ex:H-rbf,ex:H-laplace}) and is linear in the coordinates $q$. This also implies that \cref{eq:nnlm} are linear equality and inequality constraints in $q$.

We define $d_M(f, g)$ as $d(f,g)$ replacing $\mathcal F_{0,1}$ by $\mathcal F_{0,1}^M$ in the optimization problem, i.e.:
\begin{equation}\label{eq:hat-d_M}
d_M(f, g) = \min_{q \in\mathcal F_{0,1}^M} \Vert \int \phi(f(x))q(x) - \int \phi(g(x))dx\Vert_\mathcal H^2
\end{equation}

\paragraph{Quadratric Program} We can now find a natural expression for $\hat d_M(\hat f, \hat g)$ which combines representing $\hat f$ and $\hat g$ by adapted piece-wise constant functions and optimizing over $\mathcal F_{0,1}^M$. We exploit the linearity in the coordinates on $q$ in \cref{eq:diffytw-step-2} and develop the squared-norm. This leads to defining $\hat d_M(\hat f, \hat g)$ as the optimal value to a Quadratic program over the coordinates $q$ with a single equality constraint (corresponding to $\int_0^1q(x)dx = 1$) and $M$ inequality constraints ($q \geq 0$):
\begin{equation}\label{eq:discrete-diffytw-informal}
\hat d_M(\hat f, \hat g) :=min_{\substack{q\in\mathbb R^{M}\\h^\top q=1\\0 \leq q}}\frac{1}{2}q^\top Pq - v^\top q + C
\end{equation}
We define $\hat d_M(\hat f, \hat g)$ formally in \cref{sec:consistent}.

In the rest of \cref{sec:computing-diffytw}, we prove that \cref{eq:discrete-diffytw-informal} is consistent with \cref{def:diffytw}, that (under mild hypotheses) it is not sensitive to the sampling patterns of $\hat f$ and $\hat g$ as $n, m\to\infty$ and how it can be computed in practice.

%\subsection{Application to sampled time-series}

%A time series is a series of data points indexed in by time indexes. Often, the data points are taken at regular intervals. For instance, the position of a smartphone's GPS has a frequency around 1 Hz. However, this is not always the case. Indeed, in power-saving modes, a GPS position can be updated only when required by the algorithm or by the user.

%To incorporate the question of sampling pattern, we model a time series as n-tuple of couples $\left( t_i, f_i\right)_{1\leq i\leq n}$. The value $f_i$ is taken by the time-series at time $t_i$. We call $t_i$ sample points and the set $(t_i)_{1 \leq i \leq n}$ the sampling pattern. Without loss of generality, we assume that $0 \leq t_1 < \ldots < t_n \leq 1$.

%Given $\hat f_n = (t_i, f_i)_{1 \leq i\leq n}$ and $\hat g_m  = (s_j, g_j)_{1\leq j\leq m}$ taken from $f$ and $g$ we show that one can compute $d(\hat f_n, \hat g_m)$, by approximating $f$ and $g$ as rectangle functions. The approximated functions are denoted $\hat f_n$ and $\hat g_m$, by abuse of notation. We then show that $d(\hat f_n, \hat g_m)$ is close to $d(f, g)$ when $f,g$ have minimal regularity properties or if the sample pattern is random. For instance, if $f = g \circ Q$, then when $n, m \to 0$, $d(\hat f, \hat g) \to 0$.

%\subsection{Non-negative linear models}

%Non-negative linear models (NNLM) are widely used in probabilistic modeling, statistics and machine learning. One of the most well known classes are Gaussian Mixture Models (GMM).

%Let $M$ be a positive integer, the order of approximation considered. Let $(\tilde x_k)_{1\leq k\leq M}$ be $M$ distinct points in $[0, 1]$. To simplify our notations, we assume that $k < l \implies \tilde x_k < \tilde x_l$. We define the set of normalized NNLMs on $[0, 1]$ of order $M$ as the set of functions $q$ of the form
%\begin{equation}\label{eq:nnlm}
%q(x) = \sum_{k=1}^M q_k k(x, \tilde x_k)\text{ such that } q_k\in\mathbb R^+, \int_0^1q(u)du=1
%\end{equation}
%where $x \mapsto k(x, y)$ is a basis function, for instance the partial evaluation of a positive definite kernel. Normalized NNLMs are clearly a subset of $\mathcal F_{0,1}$, which we denote $\mathcal F_{0, 1}^M$. Note that $\mathcal F_{0,1}^M$ depends on the choice of kernel $k$ and of the set of anchor points $\tilde X = (\tilde x_k)_{1\leq k\leq M}$, both of which are fixed. When clear from context, we use the same notation for the function $q$ and the vector of coefficients $q\in{\mathbb R^+}^M$.


\subsection{Discrete DiffyTW is consistent with DiffyTW}\label{sec:consistent}
We begin by formally defining Discrete DiffyTW, as motivated above:
\begin{definition}[Discrete DiffyTW]\label{def:discrete-diffytw} Let $\hat f = (x_i, f_i)$ and $\hat g= (y_j, g_j)$. We define the discrete DiffyTW between discrete time series as the optimal value of the Quadratic program:
\begin{equation}\label{prob:qp}
    \hat d_M(\hat f, \hat g) :=\min_{\substack{q\in\mathbb R^{M}\\h^\top q=1\\0 \leq q}}\frac{1}{2}q^\top Pq - v^\top q + C
\end{equation}
where $P= 2\tilde H^TK_{f, f}\tilde H$, $C= \delta^\top K_{g,g}\delta$ and $v= 2 \delta^TK_{g,f}\tilde H$. $\delta\in\mathbb R^{m-1}$ is defined by $\delta_j = y_{j+1} - y_j$. $\tilde H$ is defined in \cref{eq:H}, $[K_{fg}]_{ij} = K(f_i, g_j)$, $[K_{ff}]_{ij} = K(f_i, f_j)$, and $[K_{gg}]_{ij} = K(g_i, g_j)$, where $K(z_1, z_2) = \langle \phi(z_1), \phi(z_2)\rangle_\mathcal H$.
\end{definition}
Notice that $P$ is symmetric, semi-definite positive by construction.

In \cref{thm:prob-qp}, we justify that the definition of Discrete DiffyTW coincides with that of Diffy for piece-wise constant functions, over the set of transformations induced by $\mathcal F_{0, 1}^M$.
\begin{theorem}\label{thm:prob-qp}
Let $\hat f = (x_i, f_i)$ and $\hat g = (y_j, g_j)$ be two sampled time series. Let $f$ (resp. $g$) be a piece-wise constant function adapted to $\hat f$ (resp. $\hat g$). Then:
\begin{equation}
d_M(f, g) = \hat d_M(\hat f, \hat g)
\end{equation}
where $d_M(f, g)$ is defined in \cref{eq:hat-d_M} and $\hat d_M(\hat f, \hat g)$ is defined in \cref{def:discrete-diffytw}.
\end{theorem}

\begin{proof}
\todo[inline]{Describe how to construct $f$ and $g$ from $\hat f$ and $\hat g$ then do the computation.}
\end{proof}

Because Discrete DiffyTW is defined as an extension of DiffyTW, it maintains certain invariance properties. For instance, \cref{corollary:discrete-invariance} is a consequence of \cref{thm:prob-qp} and of \cref{thm:diffytw-invariance}.

\begin{corollary}\label{corollary:discrete-invariance}
Let $h$ be a piece-wise constant and $Q$ a reparametrization such that $Q^\prime \in \mathcal F_{0,1}^M$. Let $(x_i)_{1 \leq i \leq n}$ be a sampling pattern. Consider $\hat f = (Q(x_i), h(x_i))$ and $\hat g = (x_i, h(x_i))$. Then,
\begin{equation}
\hat d_M(\hat f, \hat g) = 0.
\end{equation}
\end{corollary}
\begin{proof}
\todo[inline]{Todo, with a drawing}
\end{proof}

\subsection{Computing Discrete DiffyTW}\label{sec:solving-qp}

\Cref{prob:qp} is a quadratic program with $M$ variables and linear inequality and equality constraints. It is convex, and strongly convex as soon as $K_{f,f}$ is positive definite. The interior point Newton method, for instance in \cite{fabian} solves this type of convex problem. Their time complexity is generally in $O(M^3)$.

As a point of comparaison, Dynamic Time Warping can be computed in $O(M^2)$ time using Dynamic programming.

\subsection{Approximation \& parameter choice}
\cref{corollary:discrete-invariance} illsutrates that the definition of Discrete DiffyTW is ``tight`` with respect to that of DiffyTW if the underlying signals are piece-wise constant and diffeomorphic. In this section, we raise three questions : how good is this approximation for arbitrary signals $f$ and $g$? how dependent is $\hat d_M(\hat f, \hat g)$ on the sampling patterns? how should one go about choosing $k$, $\tilde x$ and $K$?
\begin{theorem}[Rectangle approximation]\label{thm:rectangle-approx}
Let $f,g:[0,1] \to \mathbb R^d$ be two $L$-Lipchitz functions. Let $(x_i)_{1\leq i \leq n}$ and $(y_j)_{1\leq j\leq m}$ two sampling patterns. We denote $\hat f = (x_i, f(x_i))$ and $\hat g = (y_j, g(y_j))$.
\begin{equation}
    \left\vert d_M(f, g) - \hat d_M(\hat f_n, \hat g_m)\right\vert \leq \frac{C}{n}
\end{equation}
\end{theorem}


\begin{proof}
By \cref{thm:prob-qp}, we model $\hat f$ and $\hat g$ by piecewise constant functions.
\todo[inline]{Finish this}

Let $f,g:[0,1] \to \mathbb R^d$ be two $L$-Lipchitz functions. Let $(x_i)_{1\leq i \leq n}$ and $(y_j)_{1\leq j\leq m}$ two sampling patterns. We denote $\hat f = (x_i, f(x_i))$ and $\hat g = (y_j, g(y_j))$.
Let $\mathcal F_{0,1}$. Then, for any $q\in\mathcal F_{0,1}$,
    \begin{equation}
        \vert d(f, g) - d(f\circ s, g\circ r) \vert \leq \Vert q \Vert \Vert \int \phi(f) - \phi(f\circ s)\Vert_{\mathcal H} + \Vert \int \phi(g)- \phi(g\circ s)\Vert
    \end{equation}


Let $f$ be $L$-Lipschitz on $[0,1]$ and $\phi$ be the RBF kernel with parameter $\gamma > 0$.
Then, for any $\vert x - u \vert \leq \Delta$,
\begin{equation}
    \Vert \phi(f(x)) - \phi(f(u)) \Vert^2 \leq 2\left(1 - k(f(x), f(u))\right) \leq 2 \gamma L \Delta
\end{equation}
\end{proof}




\section{Differentiability}
Differentiability makes DiffyTW effective as a building block inside of machine learning algorithms, for example as a loss function for training time-series generation algorithms.

There is a growing body of work on differentiating through convex programs, with three major approaches: analytical differentiation (when a closed-form solution to the convex problem is known, e.g. unconstrained QPs), unrolling (differentiating through the operations in the algorithm used to solve the convex problem), implicit differentiation (applying the implicit function theorem).

In this section, we show directly that $d(f, g)$ is differentiable with respect to the values taken by $g$, over the space of piece-wise continuous functions with fixed sampling pattern.

We consider differentiability of $v(\theta) = d(f, \theta)$. In this section, we denote:
\begin{equation}\label{prob:fullqp}
    v(\theta) = \min_{q\in\mathcal C} \ell(q, \theta)
\end{equation}
where $\ell(q, \theta) = \frac{1}{2} q^\top Pq - v(\theta)^\top q + C(\theta)$ and $\mathcal C = \left\lbrace q\in\mathbb R^N ~\vert~ h^Tq=1, q \geq 0\right\rbrace$.


Our goal is to compute $\nabla v(\theta)$, the gradient of $v$ with respect to $\theta$. We rely on a result from \cite{shapiro} and \cite{lee}, which gives sufficient conditions for differentiability of quadratic programs. For completeness, we prove our result directly.

\begin{theorem}
    $v(\theta)$ is differentiable and its gradient is given by
    \begin{equation}
        \nabla v(\theta) = Jv(\theta)(q^*(\theta)) + \nabla C(\theta),
    \end{equation} where $q^*(\theta)$ is the unique solution to $\mathcal P(\theta)$.
\end{theorem}

\begin{proof}
    $X$ is $\mathbb R^M$ which is a Haussdorf topological space. The set of admissible solutions $\mathcal C$ is non empty and is closed. Indeed, $\mathcal C$ is the intersection of a hyperplane ($h^\top q=1$) and of the non-negative quadrant ($q \geq 0$) ; both are closed so $\mathcal C$ is too.

    Denote $\ell(q, \theta) = \frac{1}{2}q^\top P(\theta) q - v(\theta)^Tq + C(\theta)$ where $\theta \mapsto P(\theta), \theta \mapsto v(\theta), \theta \mapsto C(\theta)$ are $\mathcal C^\infty$.

   Our goal is to show that for any $u_0\in U$, if $\bar x$ is the (unique) solution to quadratic program $\mathcal P(u_0)$, then
   \begin{equation}\label{eq:goal}
       v(u_0 + d) - v(u_0) = D_u \ell(\bar x, u)d + o\left(\Vert d\Vert\right)
   \end{equation}
   Let $u_0 \in U$, and let $\bar x$ the solution to $\mathcal P(u_0)$. Let $d$ such that $u_0 + d \in U$. Since $v(u_0) = \ell(\bar x, u_0)$, we have $v(u_0+d) \leq \ell(\bar x, u_0 + d)$. Thus,
   \begin{equation}
       v(u_0 + d) - v(u_0) - D_u\ell(\bar x, u_0)d \leq \ell(\bar x, u_0 + d) - \ell(\bar x, u_0) - D_u \ell(\bar x, u_0)d
   \end{equation}

    By the Mean Value Theorem, $\ell(\bar x, u_0 + d) - \ell(\bar x, u_0) = \int_0^1 D_u\ell(\bar x, u_0 + td)ddt$ and we can upper bound the previous equation by:
   \begin{align}
       v(u_0 + d) - v(u_0) - D_u\ell(\bar x, u_0)d&\leq \ell(\bar x, u_0 + d) - \ell(\bar x, u_0) - D_u \ell(\bar x, u_0)d\\
        &\leq \Vert d\Vert \int_0^1 \Vert D_u\ell(\bar x, u_0 +td) - D_u\ell(\bar x, u_0)\Vert dt
   \end{align}

   We denote $\varepsilon(d)= \int_0^1 \Vert D_u\ell(\bar x, u_0 +td) - D_u\ell(\bar x, u_0)\Vert dt$. By regularity of $x, u \mapsto D_u\ell(x, u)$, $u \mapsto D_u\ell(\bar x, u)$ is continuous at $u_0$ and so $ \delta \mapsto \Vert D_u\ell(\bar x, u_0 + \delta) -D_u\ell(\bar x, u_0)\Vert$ is continuous and equal to $0$ at $\delta=0$. By consequence, $\varepsilon$ is continuous and $\varepsilon(0)=0$.

   So,
   \begin{equation}
       v(u_0+d) - v(u_0) - D_u\ell(\bar x, u)d \leq \Vert d\Vert \varepsilon(d)
   \end{equation}

    Now, we assume corresponding lower bound is false, i.e. for any $\varepsilon>0$ and any $\eta >0$, there exists $d$ such that $u_0 + d\in U$ and
   \begin{equation}
       v(u_0+d) - v(u_0) - D_u\ell(\bar x, u)d < -\varepsilon\Vert d\Vert
   \end{equation}
    Let $d_n$ a sequence such that $d_n \to 0$ and
   \begin{equation}\label{eq:proof_hyp}
       v(u_0+d_n) - v(u_0) - D_u\ell(\bar x, u_0)d_n < -\varepsilon\Vert d_n\Vert
   \end{equation}

   We have a sequence $x_n\in \mathcal C$ such that $v(u_0+ d_n) = \ell(\bar x_n, u_0 + d_n)$. By Lemma 14.4 (Lee et al), there is subsequence of $(x_n)$ that converges towards $\bar x$ solution to $\mathcal P(u_0)$. By abuse of notation, we denote it $(x_n)$ as well.

   Then, by the Mean Value Theorem,
   \begin{align}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(x_n, u_0)d_n\Vert \leq \Vert d_n\Vert \int \Vert D_u \ell(x_n, u_0 + td_n) - D_u \ell(x_n, u_0)\Vert dt
   \end{align}

   For $n$ large enough, $u_0 + d_n$ is close to $u_0$ and $x_n$ is close to $\bar x$. By continuity of $x, u \mapsto D_u\ell(x, u)$,

   \begin{equation}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(x_n, u_0)d_n\Vert \leq \frac{1}{10} \varepsilon \Vert d_n \Vert
   \end{equation}

    and
    \begin{equation}
        \Vert D_u \ell(\bar x, u_0)d_n - D_u \ell(x_n, u_0)d_n\Vert \leq \frac{1}{4}\varepsilon \Vert d_n \Vert
    \end{equation}

    So, by the triangle inequality,

    \begin{equation}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(\bar x, u_0)d_n\Vert \leq \frac{1}{2} \varepsilon \Vert d_n \Vert
    \end{equation}
    Finally,
    \begin{align}
        v(u_0 + d_n) - v(u_0) - D_u \ell(x_n, u_0)&\geq\frac{1}{2}\varepsilon\Vert d_n\Vert
    \end{align}
    This contradicts \cref{eq:proof_hyp}, CQFD.
\end{proof}

\begin{remark}[Relation to implicit differentiation for QPs]
    \cite{bambade,optnet,...} study differentiation of Quadratic programs where the constraints also depend on $\theta$. They rely on implicit differentiation of the KKT conditions for the Quadratic Program and handle a more general setting, including Extended Conservative Jacobians when the QPs are not feasible. As a sanity check, we verify that we recover our result using their method for our setting in \cref{sec:lagrangian}.


\end{remark}

\section{Applications}

\subsection{Alignment with synthetic warpings}

Consider the task of aligning times series, where the warp is known and in the function space considered. Formally, let $f$ be a time-series and $Q$ an inscreasing diffeomorphism such that $q = Q^\prime$ is in the function space considered. We compute $d(f, f\circ Q)$.

\subsection{Barycenter computation}
Given a set $(f_1, \ldots, f_n)$ of time series, computing a barycenter of the data according to the geometry of an elastic distance adapted to time-series is a widely studies problem. \textcolor{red}{DBA} and \textcolor{red}{Soft-DTW} are two examples. These techniques are widely used in machine learning pipelines, for example, for set-based methods, for ...

We use a non-convex solver such as L-BFGS and the gradient expressions in \cref{sec:differentiability} to appromimately minimize the FrÃ©chet function associated to $(f_1, \ldots, f_n)$, namely:
\begin{equation}\label{eq:barycenter}
    \mathcal L(\mu) = \frac{1}{n}\sum_{i=1}^n d(f_i, \mu).
\end{equation}

We consider two variants of \cref{eq:barycenter}: $\mathcal L_1(\mu) = \frac{1}{n}\sum_{i=1}^n d(f_i,\mu)$ and $\mathcal L_2(\mu) = \frac{1}{n}\sum_{i=1}^n d(\mu, f_i)$



\subsection{Clustering: k-means \& k-medoids}

Given $(f_i)$ a set of time series and $K > 0$ an integer, the goal of clustering is to find $C_i \in \lbrace 1, \ldots, K\rbrace$ and $\mu_k$ such that the association $(f_k, i_k)$ minimizes an energy function such as:
\begin{align}
    (\mu_k), (C_i) \in \arg\min \sum_{k=1}^K\sum_{i=1}^n \mathbb I[C_i = k]d(f_i, \mu_k)
\end{align}
where $d$ is some metric. When $\mu_k$ are chosen among $(f_i)$, the task is known as k-mediods ; when the $\mu_k$ can be chosen arbitrarily, it is known as k-means.

\subsection{Learning using DiffyTW}
