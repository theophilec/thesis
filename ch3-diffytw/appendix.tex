\section{Quadrature of non-negative linear models}\label{app:diffytw-quadrature}

Given $q\in\mathcal F_{0,1}^M$, we introduce $I(q, x, y) = \int_x^y q(u)du$. By abuse of notation between $q\in\mathcal F_{0,1}^M$ and its coefficients $q\in\mathbb R^M$, $q\in{\mathbb R_{\geq 0}}^M \mapsto I(q, x, y)$ is linear in the coefficients $q$.

Consider a fixed set of points $X=(x_i)_{1\leq i\leq n-1}$. We introduce
\begin{equation}
H: q\in\mathbb R_{\geq 0}^M \mapsto (I(q, x_i, x_{i+1}))_{1\leq i\leq n-1}\in \mathbb R^{N-1}
\end{equation}

$H$ is a linear map and we can represent it by a matrix in $\mathbb R^{N-1 \times M}$ such that if $q\in \mathcal F_{0,1}^M$, then $[Hq]_{i} = \int_{x_i}^{x_{i+1}}q(u)du$. Similarly, the map $q \mapsto \int_0^1 q(u)du$ is a linear form, linear in the coefficients $q$ and can be represented by an adjoint vector $h\in\mathbb R^M$.

$H$ and $h$ are given as a function of the basis function and the anchor points. For $1 \leq i \leq n-1$ and $1\leq k\leq M$,
\begin{align}\label{eq:H}
    H_{ik}= \int_{x_i}^{x_{i+1}}k(u, \tilde x_k)du&& h_k = \int_0^1 k(u, \tilde x_k)du
\end{align}

We derive the exact expressions in three cases in the following examples.

\begin{example}[Gaussian kernel]\label{ex:H-rbf} If $k$ is the Gaussian kernel with parameter $\eta > 0$, i.e. $k(x, y) = \exp\left( - \eta (x - y)^2\right)$, then
\begin{align}
    H_{i,k} &= \frac{1}{2}\sqrt{\frac{\pi}{\eta}}\left[\mathrm{erf}(\sqrt{\eta}(x_{i+1}-\tilde x_k) - \mathrm{erf}(\sqrt{\eta}(x_i- \tilde x_k) \right]\\
    h_k &= \frac{1}{2}\sqrt{\frac{\pi}{\eta}}\left[\mathrm{erf}(\sqrt{\eta}(1-\tilde x_k) - \mathrm{erf}(\sqrt{\eta}(- \tilde x_k) \right]
\end{align}
\end{example}
\begin{example}[Laplace kernel]\label{ex:H-laplace} If $k$ is the Laplace kernel with parameter $\gamma > 0$,i.e. $k(x, y) = \exp\left( - \gamma\vert x - y\vert\right)$, then
\begin{align}
    H_{i,k} &= \varphi(\gamma(x_{i+1} - \tilde x_k)) - \varphi(\gamma(x_i - \tilde x_k))\\
    h_{k} &= \varphi(\gamma(1- \tilde x_k)) - \varphi(\gamma(- \tilde x_k))
\end{align}
where $\varphi(u) = (2 - e^u)/\gamma$ if $u \geq 0$ and $\varphi(u) = e^{-u} / \gamma$ else.
\end{example}




\section{Proof of consistency}\label{proof:prob-qp}
\begin{proof}
Our goal is to prove that our definition of $\hat d_M(\hat f, \hat g)$ coincides with that of DiffyTW when applied to piece-wise constant functions that interpolate $\hat f$ and $\hat g$.

\subparagraph{Encoding the function space} $\mathcal F_{0, 1}^M$ is the set of functions parametrized by $q\in\mathbb R^M$ such that $q(x) \geq 0, \forall x\in [0,1]$ and $\int_0^1 q(x)dx= 1$. Let $h\in\mathbb R^M$ be such that $\int_0^1q(x)dx = h^\top q$. Then, the constraint $q\in\mathcal F_{0,1}^M$ can be replaced by $q\in\mathbb R^M$, $q\geq 0$ (coordinate-wise) and $h^\top q = 1$.


\subparagraph{Computing $d_M(f, g)$} Using the fact that $K(z_1, z_2) = \langle \phi(z_1), \phi(z_2)\rangle$ and $I(q, x_i, x_{i+1}) = [Hq]_i$, for any $q\in\mathcal F_{0,1}^M$,
\begin{align}
\left \Vert \int_0^1 \phi(f(x))q(x)dx \right\Vert^2_\mathcal H&=\left\Vert\sum_{i=1}^{n-1} \underbrace{\int_{x_i}^{x_{i+1}}q(u)du}_{I(q, x_i, x_{i+1})}\phi(f(x_i))\right\Vert_\mathcal H^2\\
&=\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\underbrace{I(q, x_i, x_{i+1})}_{[Hq]_i}I(q, x_j, x_{j+1})\underbrace{\langle \phi(f(x_i)), f(x_j))\rangle}_{K(f(x_i), f(x_j))}\\
&= q^\top H^\top K_{ff} H q
\end{align}

\begin{align}
\left\Vert\int_0^1\phi(g(x))dx\right\Vert_\mathcal H^2 &= \left\Vert\sum_{j=1}^{m-1} \underbrace{(y_{j+1} - y_j)}_{\delta_j}\phi(g(y_j))\right\Vert_\mathcal H^2\\
&= \sum_{j=1}^{m-1} \sum_{i=1}^{m-1} \delta_i \delta_j K(g(y_j), g(y_i))\\
& = \delta^\top K_{gg} \delta
\end{align}

\begin{align}
\left\langle \int_0^1\phi(f(x))q(x)dx, \int_0^1\phi(g(x))\right\rangle&=\sum_{i=1}^{n-1}\sum_{j=1}^{m-1} I(q, x_i, x_{i+1}) \delta_j \langle \phi(f(x_i)), \phi(g(x_j))\rangle\\
&= \delta^\top K_{gf} Hq
\end{align}

Thus by developing the squared-norm in the definition of $d_M(f, g)$,
\begin{equation}
d_M(f, g) = \min_{\substack{q\in\mathbb R^M \\ h^\top q = 1 \\ q \geq 0}} \frac{1}{2} q^\top P q - v^\top q + C
\end{equation}
where $P = 2 H^\top K_{ff}H$, $C = \delta^\top K_{gg}\delta$ and $v = 2\delta^\top K_{gf}H$, and we recover the definition of $\hat d_M(\hat f, \hat g)$.
\end{proof}

\section{Proof of the approximation theorem}\label{sec:proof-rectangle-approx}
In this section, we prove \cref{thm:rectangle-approx}. We first introduce some useful lemmas.

\begin{lemma}\label{lemma:rectangle-approx}
Consider $f, g:[0, 1] \to \mathbb R^d$ and $s, r:[0,1] \to[0,1]$ two piece-wise constant functions such that $s(0)=r(0)=0$ and $s(1)=r(1)=1$ and assume the functions in $\mathcal F_{0,1}$ are bounded by $C_\mathcal F >0$. Then,
\begin{equation}
    \vert \sqrt{d(f, g)} - \sqrt{d(f \circ s, g\circ r)} \vert \leq C_\mathcal F \left\Vert \int_0^1\phi(f(x)) - \phi(f\circ s(x))dx\right\Vert_{\mathcal H} + \left\Vert \int_0^1\phi(g(x))- \phi(g\circ s(x))dx\right\Vert_\mathcal H.
\end{equation}

\end{lemma}
\begin{proof}[Proof of \cref{lemma:rectangle-approx}]
Indeed, let $q\in\mathcal F_{0,1}$. By the triangle inequality,
\begin{align}
\left\Vert \int_0^1\phi(f(x))q(x)dx - \int_0^1\phi(g(x))dx \right\Vert
\leq
\left\Vert \int_0^1 \phi(f(x))q(x)dx - \int_0^1 \phi(f\circ s(x))q(x)dx \right\Vert\\
+ \left\Vert \int_0^1 \phi(f\circ s(x))q(x)dx - \int_0^1 \phi(g\circ r(x))dx \right\Vert
+ \left\Vert \int_0^1 \phi(g\circ r(x))dx - \int_0^1 \phi(g(x))dx \right\Vert.
\end{align}

Then, we lower-bound the left-hand side by $\sqrt{d(f, g)}$, upper-bound the right-hand side with $C_\mathcal F$ and finally take the infimum over the remaining instance of $q$ to obtain $\sqrt{d(f\circ s, f\circ r)}$.
\end{proof}

\begin{lemma}\label{lemma:rbf-lip}
Let $\phi:\mathbb R^d \to \mathcal H$ such that $\langle
\phi(x), \phi(y)\rangle_\mathcal H = K(x, y)$ where $K$ is the Gaussian kernel with parameter $\gamma > 0$. Then,

\begin{equation}
\left\Vert \phi(z) - \phi(u)\right\Vert_\mathcal H \leq C_\gamma\sqrt{\Vert z - u \Vert_2}
\end{equation}
where $C_\gamma = \sqrt{\frac{2\sqrt{2\gamma}}{\sqrt{e}}}$.
\end{lemma}

\begin{proof}[Proof of \cref{lemma:rbf-lip}]

\begin{align}
\Vert \phi(z) - \phi(u)\Vert_\mathcal H^2 &= \Vert \phi(z) \Vert^2+ \Vert \phi(u)\Vert^2 - 2 \langle \phi(z), \phi(u)\rangle\\
&= 2 \left(1 - K(u, z)\right)
\end{align}

Denote $\varphi(x) = 1- e^{-\gamma x^2}$. $\varphi^\prime$ attains its global maxima at $x = \pm \sqrt{\frac{1}{2\gamma}}$, with value $\Vert \varphi^\prime\Vert_{\infty} = \sqrt{\frac{2\gamma}{e}}$. This implies that $K(u, z) \leq \sqrt{\frac{2 \gamma}{e}} \Vert z - u\Vert$. Finally:
\begin{align}
\Vert \phi(z) - \phi(u)\Vert_\mathcal H^2 & \leq \frac{2\sqrt{2\gamma}}{\sqrt{e}}\Vert z - u \Vert_2
\end{align}
\end{proof}

\begin{proof}[Proof of \cref{thm:rectangle-approx}]
Let $s:[0,1] \to [0,1]$ be a step function such that $f\circ s$ is adapted to $\hat f$. Similarly, let $r:[0,1]\to[0,1]$ be such that $g\circ r$ is adapted to $\hat g$. Then, by \cref{lemma:rectangle-approx},
\begin{align}
    \left\vert \sqrt{d(f, g)} - \sqrt{d(f \circ s, g\circ r)} \right\vert &\leq C_\mathcal F\left\Vert \int_0^1 \phi(f) - \phi(f\circ s)\right\Vert_{\mathcal H} + \left\Vert \int_0^1 \phi(g)- \phi(g\circ s)\right\Vert_\mathcal H.
\end{align}
We have that:
\begin{align}
\left\Vert \int_0^1\phi(f(x)) - \phi(f\circ s(x))dx\right\Vert_{\mathcal H} &\leq \int_0^1\Vert\phi(f(x)) - \phi(f\circ s(x))\Vert_\mathcal H dx,\\
&= \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}}\Vert\phi(f(x)) - \phi(f(x_i))\Vert_\mathcal H dx.
\end{align}
For $x \in [x_i, x_{i+1}]$, then by \cref{lemma:rbf-lip},
\begin{align}
\Vert \phi(f(x)) - \phi(f(x_i))\Vert_\mathcal H&\leq C_\gamma \sqrt{\Vert f(x) - f(x_i)\Vert_2},\\
& \leq C_\gamma \sqrt{L\vert x - x_i\vert},\\
& \leq C_\gamma \sqrt{L\vert x_{i+1} - x_i \vert}.
\end{align}
Putting everything together:
\begin{align}
\left\Vert \int_0^1\phi(f(x)) - \phi(f\circ s(x))dx\right\Vert_{\mathcal H} &\leq \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}}\Vert\phi(f(x)) - \phi(f(x_i))\Vert_\mathcal H dx,\\
&\leq \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}}\vert x_{i+1} - x_i\vert C_\gamma \sqrt{L\vert x_{i+1} - x_i\vert},\\
&= C_\gamma \sqrt{L}\sum_{i=1}^{n-1} \vert x_{i+1} - x_i\vert^{3/2}.
\end{align}
Following the same reasoning for $g$, we obtain:
\begin{align}
\left\Vert \int_0^1\phi(g(x)) - \phi(g\circ r(x))dx\right\Vert_{\mathcal H}  \leq C_\gamma \sqrt{L}\sum_{j=1}^{m-1} (y_{j+1} - y_{j})^{3/2}.
\end{align}

Combining the above equations, we have proven that:
\begin{align}
    \left\vert \sqrt{d(f, g)} - \sqrt{d(f \circ s, g\circ r)} \right\vert &\leq C_\gamma \sqrt{L}\left(C_\mathcal F \sum_{i=1}^{n-1} (x_{i+1} - x_i)^{3/2} + \sum_{j=1}^{m-1}(y_{j+1} - y_j)^{3/2}\right).
\end{align}

\end{proof}

\section{On the differentiability of DiffyTW}


\subsection{Proof of differentiality for DiffyTW}\label{sec:proof-diffytw-grad}
\begin{proof}

The set of admissible solutions $\mathcal C$ is non empty and is closed. Indeed, $\mathcal C$ is the intersection of a hyperplane ($h^\top q=1$) and of the non-negative quadrant ($q \geq 0$) ; both are closed so $\mathcal C$ is too.

Recall that $\ell(q, \theta) = \frac{1}{2}q^\top P(\theta) q - v(\theta)^\top q + C(\theta)$ where $\theta \mapsto P(\theta), \theta \mapsto v(\theta), \theta \mapsto C(\theta)$ are $\mathcal C^\infty$.

Our goal is to show that for any $\theta_0\in U :=\mathbb R^{n \times d}$, if $\bar q$ is the (unique) solution to quadratic program $\min_{q\in\mathcal C}\ell(q, \theta)$, which we denote as $\mathcal P(\theta_0)$ , then
   \begin{equation}\label{eq:goal}
       V(\theta_0 + d) - V(\theta_0) = D_\theta \ell(\bar q, \theta)d + o\left(\Vert d\Vert\right)
   \end{equation}
   where $D_q \ell$ is the differential of $\ell$ with respect to its first argument.

   \subparagraph{Upper bound}
   Let $\theta_0 \in U$, and let $\bar q$ the solution to $\mathcal P(\theta_0)$. Let $d$ such that $\theta_0 + d \in U$. Since $V(\theta_0) = \ell(\bar q, \theta_0)$, we have $V(\theta_0+d) \leq \ell(\bar q, \theta_0 + d)$. Thus,
   \begin{equation}
       V(\theta_0 + d) - V(\theta_0) - D_q\ell(\bar q, \theta_0)d \leq \ell(\bar q, \theta_0 + d) - \ell(\bar q, \theta_0) - D_\theta \ell(\bar q, \theta_0)d
   \end{equation}

By the Mean Value Theorem, $\ell(\bar q, \theta_0 + d) - \ell(\bar q, \theta_0) = \int_0^1 D_\theta\ell(\bar q, \theta_0 + td)ddt$ and we can upper bound the previous equation by:
   \begin{align}
       V(\theta_0 + d) - V(\theta_0) - D_\theta\ell(\bar q, \theta_0)d&\leq \ell(\bar q, \theta_0 + d) - \ell(\bar q, \theta_0) - D_\theta \ell(\bar q, \theta_0)d\\
       &\leq \int_0^1 D_\theta\ell(\bar q, \theta_0 + td)ddt  - D_\theta \ell(\bar q, \theta_0)d\\
        &\leq \Vert d\Vert \int_0^1 \Vert D_\theta\ell(\bar q, \theta_0 +td) - D_\theta\ell(\bar q, \theta_0)\Vert dt
   \end{align}

We denote $\varepsilon(d)= \int_0^1 \Vert D_\theta\ell(\bar q, \theta_0 +td) - D_\theta\ell(\bar q, \theta_0)\Vert dt$.

With the output kernel as the RBF kernel, notice that $q, \theta \mapsto D_\theta\ell(q, \theta)$ is continuous. Indeed, $D_\theta \ell(q, \theta) = -q^\top D v(\theta) + D_\theta C(\theta)$ and $D v(\theta)$ and $DC(\theta)$ on $\theta$ through the kernel matrices $K(\theta, \theta)$ and $K(f, \theta)$. The differential of the kernel matrix for the RBF kernel is still $C^\infty$ since the gradient of $\theta \mapsto \Vert f_i - \theta_i\Vert_2^2$ is $2(f_i - \theta_i)$.

In the end, $\theta \mapsto D_\theta\ell(\bar q, \theta)$ is continuous at $\theta_0$ and so is $ \delta \mapsto \Vert D_\theta\ell(\bar q, \theta_0 + \delta) -D_\theta\ell(\bar q, \theta_0)\Vert$ is continuous. In particular, it is equal to $0$ at $\delta=0$.

Finally,
   \begin{equation}
       V(\theta_0+d) - V(\theta_0) - D_\theta\ell(\bar q, u)d \leq \Vert d\Vert \varepsilon(d)
   \end{equation}
where $\varepsilon$ is continuous and $\varepsilon(0)=0$.


\subparagraph{Lower bound}
Assume corresponding lower bound is false, i.e. for any $\varepsilon>0$ and any $\eta >0$, there exists $d$ such that $\theta_0 + d\in U$ and
   \begin{equation}
       V(\theta_0+d) - V(\theta_0) - D_\theta\ell(\bar q, u)d < -\varepsilon\Vert d\Vert
   \end{equation}
    Let $d_n$ a sequence such that $d_n \to 0$ and
   \begin{equation}\label{eq:proof_hyp}
       V(\theta_0+d_n) - V(\theta_0) - D_\theta\ell(\bar q, \theta_0)d_n < -\varepsilon\Vert d_n\Vert
   \end{equation}

   We have a sequence $q_n\in \mathcal C$ such that $V(\theta_0+ d_n) = \ell(\bar q_n, \theta_0 + d_n)$. Because $P$ is positive definite and $q > 0$ is admissible, there is subsequence of $(q_n)$ that converges towards $\bar q$ solution to $\mathcal P(\theta_0)$ \citep[Lemma 14.4]{lee}. By abuse of notation, we denote this subsequence $(q_n)$ as well.

   Then, by the Mean Value Theorem,
   \begin{align}
       \vert \ell(q_n, \theta_0 + d_n) - \ell(q_n, \theta_0) - D_\theta \ell(q_n, \theta_0)d_n\vert \leq \Vert d_n\Vert \int_0^1 \Vert D_\theta \ell(q_n, \theta_0 + td_n) - D_\theta \ell(q_n, \theta_0)\Vert dt
   \end{align}

   For $n$ large enough, $\theta_0 + d_n$ is close to $\theta_0$ and $q_n$ is close to $\bar q$. By continuity of $q, u \mapsto D_\theta\ell(q, u)$ (again, because kernel matrices are smooth for the Gaussian kernel), we have the three following bounds:

   \begin{align}
       \vert \ell(q_n, \theta_0 + d_n) - \ell(q_n, \theta_0) - D_\theta \ell(q_n, \theta_0)d_n\vert &\leq \frac{1}{10} \varepsilon \Vert d_n \Vert\\
        \vert D_\theta \ell(\bar q, \theta_0)d_n - D_\theta \ell(q_n, \theta_0)d_n\vert &\leq \frac{1}{4}\varepsilon \Vert d_n \Vert\\
        \vert \ell(q_n, \theta_0) - \ell(\bar q, \theta_0) \vert \leq \frac{1}{10}\varepsilon \Vert d_n \Vert
    \end{align}

    So, by the triangle inequality,

    \begin{equation}
       \vert \ell(q_n, \theta_0 + d_n) - \ell(q_n, \theta_0) - D_\theta \ell(\bar q, \theta_0)d_n\vert \leq \frac{1}{2} \varepsilon \Vert d_n \Vert
    \end{equation}
    Finally, since $V(\theta + d_n) - V(\theta) \geq \ell(q_n, \theta_n) - \ell(q_n, \theta)$,
    \begin{align}
        V(\theta_0 + d_n) - V(\theta_0) - D_\theta \ell(q_n, \theta_0)&\geq-\frac{1}{2}\varepsilon\Vert d_n\Vert
    \end{align}
    This contradicts \cref{eq:proof_hyp}.
\end{proof}






\subsection{Relation to implicit differentiation}\label{sec:lagrangian}
In this section, we derive the gradient ($\theta$ is seen as a vector) of $V$ using implicit differentiation, as in \citet{optnet}. Our exposition here in informal and the goal is to verify that we recover the expression in \cref{thm:diffytw-grad}.
Two relevant references to this approach are \cite{rockafellar} for regularity of implicit mappings and \cite{convex-optimization} for convex optimization.

\paragraph{}
Differentiating without justification with a chain rule yields:
\begin{equation}\label{eq:lagrangian-first}
    \nabla V(\theta) = \nabla_1 \ell(\theta, q^*(\theta)) + \nabla_2 \ell(\theta, q^*(\theta))J q^*(\theta)
\end{equation}
where $q^*(\theta)$ is the optimal solution to the problem for parameter $\theta$.

The first term corresponds to the differential we derived formally. In this section, we verify that the second term is equal to $0$ in our setting, i.e. when the constraints do not depend on $\theta$. To do this, we must first compute the Jacobian term of $q^*(\theta)$, which is the map from parameter $\theta$ to the solution of the QP.

We proceed as follows: (1) express the Lagrangian for the QP and compute the KKT optimality conditions for the primal and dual solutions; (2) rewrite the KKT conditions as an optimality function thanks to strong duality; (3) apply the Implicit Function Theorem (we introduce it below) to the optimality function and compute the Jacobian of $q^*(\theta)$; and finally, (4) show the product $\nabla_2 \ell(\theta, q^*(\theta))J q^*(\theta)$ is equal to $0$.

\paragraph{Step 1: Derivation of the Lagrangian \& KKT optimality conditions}
The Lagrangian of \cref{prob:fullqp} is:
\begin{equation}
    L(q, \lambda, \nu, \theta) = \underbrace{\frac{1}{2}q^\top Pq - v(\theta)^\top q + C(\theta)}_{\ell(q, \theta)}+ \lambda^\top (h^\top q - 1) - \nu^\top q
\end{equation}
where
where $L: \mathbb R^M \times \mathbb R \times \mathbb R^M\times\Theta\to\mathbb R$, as there are $M$ variables, $1$ equality constraint and $M$ inequality constraints, where we added the dependence on $\theta$.

For optimal primal and dual solutions $(q^*(\theta), \lambda^*(\theta), \nu^*(\theta))$, the KKT conditions are verified:
\begin{align}
    0 &= \partial_q \ell(q^*, \theta) + \lambda^*h - \nu^* &\text{(primal stationarity)}\\
    1 & =h^\top q^*(\theta)  & \text{(primal feasibility)} \\
    0 & \geq q^*(\theta) & \text{(primal feasibility)}\\
    0 & = \mathrm{Diag}(\nu^*(\theta))q(\theta) & \text{(complimentary slackness)}
\end{align}


\paragraph{Step 2: Definition of the optimality function}

We define the function
\begin{equation}
g(q, \lambda, \nu, \theta) = \begin{bmatrix}
    Pq - v(\theta) + uh - v\\
    h^\top q- 1\\
    \mathrm{Diag}(\nu)q
\end{bmatrix}.
\end{equation}
By strong duality, $(q^*, \lambda^*, \nu^*)$ are solutions if and only if $g(q^*, \lambda^*, \nu^*, \theta) = 0$.

\paragraph{Step 3: applying the implicit function theorem}
A simple statement of the implicit function theorem from \cite{rockafellar} and orginally proven by Dini is:
\begin{theorem}
Let the function $g: \mathbb R^n \times \mathbb R^n \to \mathbb R^n$. Let $(\bar p, \bar x)\in\mathbb R^n \times \mathbb R^n$ be such that (a) $g(\bar x, \bar p)=0$, (b) $g$ is continuously differentiable at $(\bar x, \bar p)$, (c) the partial Jacobian of $g$ with respect to $x$ at $(\bar x, \bar p)$ is nonsingular, denoted $\nabla_x g(\bar x, \bar p)$.

Then, there is a continuously differentiable map $s$ defined on a neighborhood $N_p$ of $\bar p$ such that for every $p\in N_p$, $g(s(p), p)=0$ and
\begin{equation}
\nabla s(p) = - \nabla_x g(s(p), p)^{-1}\nabla_p g(s(p), p).
\end{equation}
\end{theorem}

The key hypothesis is non-singularity of the KKT matrix at an interior point in the domain. This is not straightforward, and to the best of our knowledge, we did not find a rigourous treatment. This explains why we turned to the approach in \cite{shapiro}.

Denoting $z = (q, u, v)$, $g$ is a function of two variables $(z, \theta)$. We apply the implicit function theorem by inverting $\partial_z g(z^*, \theta)$. The partial Jacobian can be written:
\begin{align}
    J_z g(z^*, \theta) = \begin{bmatrix}
    P & h^\top  & - I\\
    h & 0 & 0\\
    \mathrm{Diag}(\nu)& 0 & \mathrm{Diag}(q)
    \end{bmatrix}
\end{align}
Note that the structure of the partial Jacobian is particularly simple in our case since the constraints do not depend on $\theta$. Finally, we obtain:
\begin{align}
    J_\theta z^*(\theta) = - J_zg(z^*, \theta)^{-1}J_\theta g(z^*, \theta)
\end{align}

Returning to the context of \cref{eq:lagrangian-first}, we select the parts of the Jacobians that we are interested in:
\begin{align}
J_\theta V(\theta) &=  J_q \ell(q^*, \theta)J_\theta q^*(\theta) + J_\theta \ell(q^*, \theta),\\
&= J_q \ell(q^*, \theta)  \left[J_\theta z^*(\theta)\right]_{q, \theta} + J_\theta \ell(q^*, \theta),\\
&= - J_q\ell(q^*, \theta) \left[J_z g(z^*, \theta)^{-1} J_\theta g(z^*, \theta)\right]_{q,\theta}+ J_\theta \ell(q^*, \theta)
\end{align}
where $[J]_{q, \theta}$ is the block of the Jacobian that corresponds to differentiating $q$ with respect to $\theta$.

\paragraph{Step 4: Simplify the expression} To retrieve the result in \cref{thm:diffytw-grad}, we prove that
\begin{equation}\label{appendix:goal}
Jq\ell(q^, \theta)\left[J_z g(z^*, \theta)^{-1} J_\theta g(z^*, \theta)\right]_{q,\theta}=0.
\end{equation}

For simplicty, we assume that $\theta$ is such that $q^*(\theta) > 0 $ so $\nu^*(\theta)=0$. We then have:
\begin{align}
    J_z g(z^*, \theta) = \begin{bmatrix}
    P & - I & h\\
    0 & \mathrm{Diag}(q)& 0\\
    h^\top  & 0 & 0
    \end{bmatrix} & &
    J_\theta g(z^*, \theta) = \begin{bmatrix}
    -J_\theta v(\theta)\\
    0 \\
    0
    \end{bmatrix}
\end{align}

\paragraph{}
Showing \cref{appendix:goal} is equivalent to showing that $Jq\ell(q^, \theta)A = 0$ where $A$ verifies the following system:
\begin{equation}
\begin{bmatrix}
    P & - I & h\\
    0 & \mathrm{Diag}(q)& 0\\
    h^\top  & 0 & 0
    \end{bmatrix}
\begin{bmatrix}
    A \\
    B \\
    C
    \end{bmatrix} =
\begin{bmatrix}
    - J_\theta v(\theta) \\
    0 \\
    0
    \end{bmatrix}
\end{equation}
which is equivalent to the three following equations:
\begin{align}
   PA - B + hC =& -J_\theta v(\theta)\\
   \mathrm{Diag}(q)B =& 0\\
   h^\top A =& 0
\end{align}

\paragraph{}
We solve the system for $A$. The second equation implies that $B=0$, and the first implies
\begin{equation}
    A = - P^{-1}hC - P^{-1}J_\theta v(\theta)
\end{equation}
Finally,
\begin{align}
    J_q\ell(q^*,\theta)A &= (q^\top P - v(\theta)^\top )A = h^\top A = 0.
\end{align}
\paragraph{}
And we do indeed recover the result in \cref{thm:diffytw-grad}, using a different method. This did not seem obvious from the outset.
\qed
