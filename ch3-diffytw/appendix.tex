\section{Useful computations}

\subsection{Quadrature of non-negative linear models}\label{app:diffytw-quadrature}

Given $q\in\mathcal F_{0,1}^M$, we introduce . $q\in{\mathbb R_{\geq 0}}^M \mapsto I(q, x, y)$ is linear in the coefficients.

Consider a fixed set of points $X=(x_i)_{1\leq i\leq n-1}$. We introduce
\begin{equation}
H: q\in\mathbb R_{\geq 0}^M \mapsto (I(q, x_i, x_{i+1}))_{1\leq i\leq n-1}\in \mathbb R^{N-1}
\end{equation}

$H$ is a linear map and we can represent it by a matrix in $\mathbb R^{N-1 \times M}$ such that if $q\in \mathcal F_{0,1}^M$, then $[Hq]_{i} = \int_{x_i}^{x_{i+1}}q(u)du$. Similarly, the map $q \mapsto \int_0^1 q(u)du$ is a linear form, linear in the coefficients $q$ and can be represented by an adjoint vector $h\in\mathbb R^M$.

$H$ and $h$ are given as a function of the basis function and the anchor points. For $1 \leq i \leq n-1$ and $1\leq k\leq M$,
\begin{align}\label{eq:H}
    H_{ik}= \int_{x_i}^{x_{i+1}}k(u, \tilde x_k)du&& h_k = \int_0^1 k(u, \tilde x_k)du
\end{align}

We derive the exact expressions in three cases in the following examples.

\begin{example}[RBF kernel]\label{ex:H-rbf} If $k$ is the RBF kernel with parameter $\eta > 0$, i.e. $k(x, y) = \exp\left( - \eta (x - y)^2\right)$, then
\begin{align}
    H_{i,k} &= \frac{1}{2}\sqrt{\frac{\pi}{\eta}}\left[\mathrm{erf}(\sqrt{\eta}(x_{i+1}-\tilde x_k) - \mathrm{erf}(\sqrt{\eta}(x_i- \tilde x_k) \right]\\
    h_k &= \frac{1}{2}\sqrt{\frac{\pi}{\eta}}\left[\mathrm{erf}(\sqrt{\eta}(1-\tilde x_k) - \mathrm{erf}(\sqrt{\eta}(- \tilde x_k) \right]
\end{align}
\end{example}
\begin{example}[Laplace kernel]\label{ex:H-laplace} If $k$ is the Laplace kernel with parameter $\gamma > 0$,i.e. $k(x, y) = \exp\left( - \gamma\vert x - y\vert\right)$, then
\begin{align}
    H_{i,k} &= \varphi(\gamma(x_{i+1} - \tilde x_k)) - \varphi(\gamma(x_i - \tilde x_k))\\
    h_{k} &= \varphi(\gamma(1- \tilde x_k)) - \varphi(\gamma(- \tilde x_k))
\end{align}
where $\varphi(u) = (2 - e^u)/\gamma$ if $u \geq 0$ and $\varphi(u) = e^{-u} / \gamma$ else.
\end{example}

\section{On the differentiability of DiffyTW}
\subsection{Proof of \cref{thm:diffytw-grad}}\label{sec:proof-diffytw-grad}
\begin{proof}
\todo[inline]{Make clean}
    $X$ is $\mathbb R^M$ which is a Haussdorf topological space. The set of admissible solutions $\mathcal C$ is non empty and is closed. Indeed, $\mathcal C$ is the intersection of a hyperplane ($h^\top q=1$) and of the non-negative quadrant ($q \geq 0$) ; both are closed so $\mathcal C$ is too.

    Denote $\ell(q, \theta) = \frac{1}{2}q^\top P(\theta) q - v(\theta)^Tq + C(\theta)$ where $\theta \mapsto P(\theta), \theta \mapsto v(\theta), \theta \mapsto C(\theta)$ are $\mathcal C^\infty$.

   Our goal is to show that for any $u_0\in U$, if $\bar x$ is the (unique) solution to quadratic program $\mathcal P(u_0)$, then
   \begin{equation}\label{eq:goal}
       v(u_0 + d) - v(u_0) = D_u \ell(\bar x, u)d + o\left(\Vert d\Vert\right)
   \end{equation}
   Let $u_0 \in U$, and let $\bar x$ the solution to $\mathcal P(u_0)$. Let $d$ such that $u_0 + d \in U$. Since $v(u_0) = \ell(\bar x, u_0)$, we have $v(u_0+d) \leq \ell(\bar x, u_0 + d)$. Thus,
   \begin{equation}
       v(u_0 + d) - v(u_0) - D_u\ell(\bar x, u_0)d \leq \ell(\bar x, u_0 + d) - \ell(\bar x, u_0) - D_u \ell(\bar x, u_0)d
   \end{equation}

    By the Mean Value Theorem, $\ell(\bar x, u_0 + d) - \ell(\bar x, u_0) = \int_0^1 D_u\ell(\bar x, u_0 + td)ddt$ and we can upper bound the previous equation by:
   \begin{align}
       v(u_0 + d) - v(u_0) - D_u\ell(\bar x, u_0)d&\leq \ell(\bar x, u_0 + d) - \ell(\bar x, u_0) - D_u \ell(\bar x, u_0)d\\
        &\leq \Vert d\Vert \int_0^1 \Vert D_u\ell(\bar x, u_0 +td) - D_u\ell(\bar x, u_0)\Vert dt
   \end{align}

   We denote $\varepsilon(d)= \int_0^1 \Vert D_u\ell(\bar x, u_0 +td) - D_u\ell(\bar x, u_0)\Vert dt$. By regularity of $x, u \mapsto D_u\ell(x, u)$, $u \mapsto D_u\ell(\bar x, u)$ is continuous at $u_0$ and so $ \delta \mapsto \Vert D_u\ell(\bar x, u_0 + \delta) -D_u\ell(\bar x, u_0)\Vert$ is continuous and equal to $0$ at $\delta=0$. By consequence, $\varepsilon$ is continuous and $\varepsilon(0)=0$.

   So,
   \begin{equation}
       v(u_0+d) - v(u_0) - D_u\ell(\bar x, u)d \leq \Vert d\Vert \varepsilon(d)
   \end{equation}

    Now, we assume corresponding lower bound is false, i.e. for any $\varepsilon>0$ and any $\eta >0$, there exists $d$ such that $u_0 + d\in U$ and
   \begin{equation}
       v(u_0+d) - v(u_0) - D_u\ell(\bar x, u)d < -\varepsilon\Vert d\Vert
   \end{equation}
    Let $d_n$ a sequence such that $d_n \to 0$ and
   \begin{equation}\label{eq:proof_hyp}
       v(u_0+d_n) - v(u_0) - D_u\ell(\bar x, u_0)d_n < -\varepsilon\Vert d_n\Vert
   \end{equation}

   We have a sequence $x_n\in \mathcal C$ such that $v(u_0+ d_n) = \ell(\bar x_n, u_0 + d_n)$. By Lemma 14.4 (Lee et al), there is subsequence of $(x_n)$ that converges towards $\bar x$ solution to $\mathcal P(u_0)$. By abuse of notation, we denote it $(x_n)$ as well.

   Then, by the Mean Value Theorem,
   \begin{align}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(x_n, u_0)d_n\Vert \leq \Vert d_n\Vert \int \Vert D_u \ell(x_n, u_0 + td_n) - D_u \ell(x_n, u_0)\Vert dt
   \end{align}

   For $n$ large enough, $u_0 + d_n$ is close to $u_0$ and $x_n$ is close to $\bar x$. By continuity of $x, u \mapsto D_u\ell(x, u)$,

   \begin{equation}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(x_n, u_0)d_n\Vert \leq \frac{1}{10} \varepsilon \Vert d_n \Vert
   \end{equation}

    and
    \begin{equation}
        \Vert D_u \ell(\bar x, u_0)d_n - D_u \ell(x_n, u_0)d_n\Vert \leq \frac{1}{4}\varepsilon \Vert d_n \Vert
    \end{equation}

    So, by the triangle inequality,

    \begin{equation}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(\bar x, u_0)d_n\Vert \leq \frac{1}{2} \varepsilon \Vert d_n \Vert
    \end{equation}
    Finally,
    \begin{align}
        v(u_0 + d_n) - v(u_0) - D_u \ell(x_n, u_0)&\geq\frac{1}{2}\varepsilon\Vert d_n\Vert
    \end{align}
    This contradicts \cref{eq:proof_hyp}, CQFD.
\end{proof}

\subsection{Relation to implicit differentiation}\label{sec:lagrangian}
Our approach is to use implicit differentiation. Roughly, we proceed as follows: link optimality of $q^*$ using strong duality (Slater's Condition) and the KKT conditions ; rewrite the KKT conditions as an implicit function ; use the implicit function theorem.

\noindent We first state our result:

\begin{theorem}[Lagrangian approach]
$v(\theta) = l(\theta, q^*(\theta))$ is differentiable and is gradient is defined as:
\begin{equation}
    \nabla v(\theta) = \nabla_1 l(\theta, q^*(\theta)) + \nabla_2 l(\theta, q^*(\theta))J q^*(\theta)
\end{equation}
\end{theorem}

\begin{proof}
The Lagrangian of \cref{prob:fullqp} is:
\begin{equation}
    L(q, u, v, \theta) = \underbrace{\frac{1}{2}q^TP(\theta)q - v(\theta)^Tq + C(\theta)}_{l(q, \theta)}+ u^T(h^Tq - 1) - v^Tq
\end{equation}

where $L: \mathbb R^M \times \mathbb R \times \mathbb R^M\times\Theta\to\mathbb R$, as there are $M$ variables, $1$ equality constraint and $M$ inequality constraints, where we've added dependence on $\theta$.

For an optimal primal and dual solutsion $(q^*(\theta), u^*(\theta), v^*(\theta))$, the KKT conditions are verified:
\begin{align}
    0 &= \partial_q l(q^*, \theta) + u^*h - v^* &\text{(primal stationarity)}\\
    1 & =h^Tq^*(\theta)  & \text{(primal feasibility)} \\
    0 & \geq q^*(\theta) & \text{(primal feasibility)}\\
    0 & = \mathrm{Diag}(v^*(\theta))q(\theta) & \text{(complimentary slackness)}
\end{align}






\textbf{Step 2: translate the KKT conditions as an implicit function}

Let $g(q, u, v, \theta) = \begin{bmatrix}
    P(\theta)q - v(\theta) + uh - v\\
    h^Tq- 1\\
    \mathrm{Diag}(v)q
\end{bmatrix}$. $(q^*, u^*, v^*)$ are solutions if and only if $g(q^*, u^*, v^*, \theta) = 0$.

\textbf{Step 3: applying the implicit function theorem}
Denoting $z = (q, u, v)$, $g$ is a function of two variables $(z, \theta)$. To apply the implicit function theorem, we prove that $\partial_z g(z^*, \theta)$ is invertible for any solution $z^*$.

The partial Jacobian can be written:
\begin{align}
    J_z g(z^*, \theta) = \begin{bmatrix}
    P(\theta) & h^T & - I\\
    h & 0 & 0\\
    \mathrm{Diag}(v)& 0 & \mathrm{Diag}(q)
    \end{bmatrix}
\end{align}

If there exists $1 \leq i\leq m$ such that $v_i = q_i = 0$, $J_z g$ is not invertible. This corresponds to the "zero-zero" case for complementary slackness : the constraint is active and the Lagrange multiplier is $0$. In this case, at optimality, \textcolor{red}{todo todo}.


In the end,
\begin{align}
    J_\theta z^*(\theta) = - J_zg(z^*, \theta)^{-1}J_\theta g(z^*, \theta)
\end{align}
We can then combine these results:
\begin{align}
J_\theta v(\theta) &=  J_q l(q^*, \theta)J_\theta q^*(\theta) + J_\theta l(q^*, \theta),\\
&= J_q l(q^*, \theta)  \left[J_\theta z^*(\theta)\right]_{q, \theta} + J_\theta l(q^*, \theta),\\
&= - J_ql(q^*, \theta) \left[J_z g(z^*, \theta)^{-1} J_\theta g(z^*, \theta)\right]_{q,\theta}+ J_\theta l(q^*, \theta)
\end{align}
\end{proof}

Here we assume that $\theta$ is such that $q^*(\theta) > 0 $, so $v^*(\theta)=0$. Thus:

\begin{align}
    J_z g(z^*, \theta) = \begin{bmatrix}
    P(\theta) & - I & h\\
    0 & \mathrm{Diag}(q)& 0\\
    h^T & 0 & 0
    \end{bmatrix}
\end{align}
 and recall
\begin{align}
    J_\theta g(z^*, \theta) = \begin{bmatrix}
    -J_\theta v(\theta)\\
    0 \\
    0
    \end{bmatrix}
\end{align}

We solve the following system:
\begin{equation}
\begin{bmatrix}
    P(\theta) & - I & h\\
    0 & \mathrm{Diag}(q)& 0\\
    h^T & 0 & 0
    \end{bmatrix}
\begin{bmatrix}
    A \\
    B \\
    C
    \end{bmatrix} =
\begin{bmatrix}
    - J_\theta v(\theta) \\
    0 \\
    0
    \end{bmatrix}
\end{equation}

which is equivalent to:

\begin{align}
   P(\theta)A - B + hC =& -J_\theta v(\theta)\\
   \mathrm{Diag}(q)B =& 0\\
   h^TA =& 0
\end{align}

The second equation implies that $B=0$, and the first implies
\begin{equation}
    A = - P(\theta)^{-1}hC - P(\theta)^{-1}J_\theta v(\theta)
\end{equation}

And
\begin{align}
    J_ql(q^*,\theta)A &= (q^TP(\theta) - v(\theta)^T)A = h^TA = 0
\end{align}
\vskip 0.2in
