\section{Hiking and time-warping}\label{sec:appendix-hiking}
\todo[inline]{write}
Consider two hikers (Felix and Gabrielle) who each go on an hour-long hike round-trip from Nice. They each record their altitude on their machine-learning enabled smart watches. From both recordings, can we determine if they took the same route (in altitude at least)? Suppose they have different speeds through time. For instance, Felix may have constant speed, while Gabrielle oscillates between going slower then catching up (and vice versa). This situation is illustrated in \cref{fig:hiking} and we go into more detail in \cref{sec:appendix-hiking}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{ch3-diffytw/figures/hiking-1.pdf}
    \includegraphics[width=0.4\textwidth]{ch3-diffytw/figures/hiking-3.pdf}
    \caption[Hiking example for DiffyTW.]{Motivating example for DiffyTW: time-warping from hiking.}\label{fig:hiking}
\end{figure}

From their elevation recordings, can we determine whether they did indeed follow the same path (in elevation at least)?

We can model their profiles by two functions $f:[0,1] \to [0, \infty[$ and $g:[0, 1] \to [0, \infty[$. We can model the time-warp caused by their differences in speed by a \emph{reparametrization} of time, i.e. the existence of an increasing diffeomorphism on $[0,1]$, such that $g$ is equal to $f$ composed with the reparametrization. More formally, we would have $g = f \circ Q$ where $Q$ is the reparametrization. Our goal in this chapter is to develop a divergence $d$ that compares $f$ and $g$ with a divergence $d$ such that $d(f, g)=0$ if there exists $Q$ such that $g = f \circ Q$.\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{ch3-diffytw/figures/hiking-1.pdf}
    \caption[Sample image for Diffy experiments.]{Peppers image from Matlab software.}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{ch3-diffytw/figures/hiking-2.pdf}
    \caption[Sample image for Diffy experiments.]{Peppers image from Matlab software.}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{ch3-diffytw/figures/hiking-3.pdf}
    \caption[Sample image for Diffy experiments.]{Peppers image from Matlab software.}
\end{figure}

\section{Quadrature of non-negative linear models}\label{app:diffytw-quadrature}

Given $q\in\mathcal F_{0,1}^M$, we introduce . $q\in{\mathbb R_{\geq 0}}^M \mapsto I(q, x, y)$ is linear in the coefficients.

Consider a fixed set of points $X=(x_i)_{1\leq i\leq n-1}$. We introduce
\begin{equation}
H: q\in\mathbb R_{\geq 0}^M \mapsto (I(q, x_i, x_{i+1}))_{1\leq i\leq n-1}\in \mathbb R^{N-1}
\end{equation}

$H$ is a linear map and we can represent it by a matrix in $\mathbb R^{N-1 \times M}$ such that if $q\in \mathcal F_{0,1}^M$, then $[Hq]_{i} = \int_{x_i}^{x_{i+1}}q(u)du$. Similarly, the map $q \mapsto \int_0^1 q(u)du$ is a linear form, linear in the coefficients $q$ and can be represented by an adjoint vector $h\in\mathbb R^M$.

$H$ and $h$ are given as a function of the basis function and the anchor points. For $1 \leq i \leq n-1$ and $1\leq k\leq M$,
\begin{align}\label{eq:H}
    H_{ik}= \int_{x_i}^{x_{i+1}}k(u, \tilde x_k)du&& h_k = \int_0^1 k(u, \tilde x_k)du
\end{align}

We derive the exact expressions in three cases in the following examples.

\begin{example}[RBF kernel]\label{ex:H-rbf} If $k$ is the RBF kernel with parameter $\eta > 0$, i.e. $k(x, y) = \exp\left( - \eta (x - y)^2\right)$, then
\begin{align}
    H_{i,k} &= \frac{1}{2}\sqrt{\frac{\pi}{\eta}}\left[\mathrm{erf}(\sqrt{\eta}(x_{i+1}-\tilde x_k) - \mathrm{erf}(\sqrt{\eta}(x_i- \tilde x_k) \right]\\
    h_k &= \frac{1}{2}\sqrt{\frac{\pi}{\eta}}\left[\mathrm{erf}(\sqrt{\eta}(1-\tilde x_k) - \mathrm{erf}(\sqrt{\eta}(- \tilde x_k) \right]
\end{align}
\end{example}
\begin{example}[Laplace kernel]\label{ex:H-laplace} If $k$ is the Laplace kernel with parameter $\gamma > 0$,i.e. $k(x, y) = \exp\left( - \gamma\vert x - y\vert\right)$, then
\begin{align}
    H_{i,k} &= \varphi(\gamma(x_{i+1} - \tilde x_k)) - \varphi(\gamma(x_i - \tilde x_k))\\
    h_{k} &= \varphi(\gamma(1- \tilde x_k)) - \varphi(\gamma(- \tilde x_k))
\end{align}
where $\varphi(u) = (2 - e^u)/\gamma$ if $u \geq 0$ and $\varphi(u) = e^{-u} / \gamma$ else.
\end{example}
\section{Proof of \cref{thm:prob-qp}}\label{proof:prob-qp}
\begin{proof}
Our goal is to prove that our definition of $\hat d_M(\hat f, \hat g)$ coincides with that of DiffyTW when applied to piece-wise constant functions that interpolate $\hat f$ and $\hat g$.

\subparagraph{Encoding the function space} $\mathcal F_{0, 1}^M$ is the set of functions parametrized by $q\in\mathbb R^M$ such that $q(x) \geq 0, \forall x\in [0,1]$ and $\int_0^1 q(x)dx= 1$. Let $h\in\mathbb R^M$ be such that $\int_0^1q(x)dx = h^\top q$. Then, the constraint $q\in\mathcal F_{0,1}^M$ can be replaced by $q\in\mathbb R^M$, $q\geq 0$ (coordinate-wise) and $h^\top q = 1$.


\subparagraph{Computing $d_M(f, g)$} Using the fact that $K(z_1, z_2) = \langle \phi(z_1), \phi(z_2)\rangle$ and $I(q, x_i, x_{i+1}) = [Hq]_i$, for any $q\in\mathcal F_{0,1}^M$,
\begin{align}
\left \Vert \int_0^1 \phi(f(x))q(x)dx \right\Vert^2_\mathcal H&=\left\Vert\sum_{i=1}^{n-1} \underbrace{\int_{x_i}^{x_{i+1}}q(u)du}_{I(q, x_i, x_{i+1})}\phi(f(x_i))\right\Vert_\mathcal H^2\\
&=\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\underbrace{I(q, x_i, x_{i+1})}_{[Hq]_i}I(q, x_j, x_{j+1})\underbrace{\langle \phi(f(x_i)), f(x_j))\rangle}_{K(f(x_i), f(x_j))}\\
&= q^\top H^\top K_{ff} H q
\end{align}

\begin{align}
\left\Vert\int_0^1\phi(g(x))dx\right\Vert_\mathcal H^2 &= \left\Vert\sum_{j=1}^{m-1} \underbrace{(y_{j+1} - y_j)}_{\delta_j}\phi(g(y_j))\right\Vert_\mathcal H^2\\
&= \sum_{j=1}^{m-1} \sum_{i=1}^{m-1} \delta_i \delta_j K(g(y_j), g(y_i))\\
& = \delta^\top K_{gg} \delta
\end{align}

\begin{align}
\left\langle \int\phi(f(x))q(x)dx, \int\phi(g(x))\right\rangle&=\sum_{i=1}^{n-1}\sum_{j=1}^{m-1} I(q, x_i, x_{i+1}) \delta_j \langle \phi(f(x_i)), \phi(g(x_j))\rangle\\
&= \delta^\top K_{gf} Hq
\end{align}

Thus by developing the squared-norm in the definition of $d_M(f, g)$,
\begin{equation}
d_M(f, g) = \min_{\substack{q\in\mathbb R^M \\ h^\top q = 1 \\ q \geq 0}} \frac{1}{2} q^\top P q - v^\top q + C
\end{equation}
where $P = 2 H^\top K_{ff}H$, $C = \delta^\top K_{gg}\delta$ and $v = 2\delta^\top K_{gf}H$,

and we recover the definition of $\hat d_M(\hat f, \hat g)$.
\end{proof}
\section{Proof of \cref{corollary:discrete-invariance}}\label{proof:discrete-invariance}
\begin{proof}
$h \circ Q$ is adapted to $\hat f$ and $h$ is adapted to $\hat g$, so from \cref{thm:prob-qp}:
\begin{equation}
    \hat d_M(\hat f, \hat g) = d_M(h\circ Q, h) = 0
\end{equation}
\end{proof}
\section{Proof of \cref{thm:rectangle-approx}}\label{sec:proof-rectangle-approx}
\begin{lemma}\label{lemma:rectangle-approx}
Consider $f, g:[0, 1] \to \mathbb R^d$ and $s, r:[0,1] \to[0,1]$ two piece-wise constant functions such that $s(0)=r(0)=0$ and $s(1)=r(1)=1$. Then,
\begin{equation}
    \vert d(f, g) - d(f \circ s, g\circ r) \vert \leq \max_{q\in\mathcal F_{0,1}}\max_{x\in[0,1]}\vert q(x) \vert \left\Vert \int \phi(f) - \phi(f\circ s)\right\Vert_{\mathcal H} + \left\Vert \int \phi(g)- \phi(g\circ s)\right\Vert_\mathcal H
\end{equation}

\end{lemma}
\begin{proof}[Proof of \cref{lemma:rectangle-approx}]
Indeed, let $q\in\mathcal F_{0,1}$. By the triangle inequality,
\begin{align}
\left\Vert \int \phi(f(x))q(x)dx - \int \phi(g(x))dx \right\Vert
\leq
\left\Vert \int \phi(f(x))q(x)dx - \int \phi(f\circ s(x))q(x)dx \right\Vert\\
+ \left\Vert \int \phi(f\circ s(x))q(x)dx - \int \phi(g\circ r(x))dx \right\Vert
+ \left\Vert \int \phi(g\circ r(x))dx - \int \phi(g(x))dx \right\Vert
\end{align}
\end{proof}

\begin{lemma}\label{lemma:rbf-lip}
Let $\phi:\mathbb R^d \to \mathcal H$ such that $\langle
\phi(x), \phi(y)\rangle_\mathcal H = K(x, y)$ where $K$ is the RBF kernel with parameter $\gamma > 0$. Then,

\begin{equation}
\left\Vert \phi(z) - \phi(u)\right\Vert_\mathcal H \leq C_\gamma\sqrt{\Vert z - u \Vert_2}
\end{equation}
where $C_\gamma = \sqrt{\frac{2\sqrt{2\gamma}}{\sqrt{e}}}$.
\end{lemma}

\begin{proof}[Proof of \cref{lemma:rbf-lip}]

\begin{align}
\Vert \phi(z) - \phi(u)\Vert_\mathcal H^2 &= \Vert \phi(z) \Vert^2+ \Vert \phi(u)\Vert^2 - 2 \langle \phi(z), \phi(u)\rangle\\
&= 2 \left(1 - K(u, z)\right)
\end{align}

Denote $\varphi(x) = 1- e^{-\gamma x^2}$. $\varphi^\prime$ attains its global maxima at $x = \pm \sqrt{\frac{1}{2\gamma}}$, with value $\Vert \phi^\prime\Vert_{\infty} = \sqrt{\frac{2\gamma}{e}}$. And so:

\begin{align}
\Vert \phi(z) - \phi(u)\Vert_\mathcal H^2 & \leq \frac{2\sqrt{2\gamma}}{\sqrt{e}}\Vert z - u \Vert_2
\end{align}
\end{proof}

\begin{proof}[Proof of \cref{thm:rectangle-approx}]
Let $s:[0,1] \to [0,1]$ be a step function such that $f\circ s$ is adapted to $\hat f$. Similarly, let $r:[0,1]\to[0,1]$ be such that $g\circ r$ is adapted to $\hat g$. Then, by \cref{lemma:rectangle-approx},
\begin{equation}
    \vert d(f, g) - d(f \circ s, g\circ r) \vert \leq \max_{q\in\mathcal F_{0,1}}\Vert q \Vert_\mathcal H \left\Vert \int \phi(f) - \phi(f\circ s)\right\Vert_{\mathcal H} + \left\Vert \int \phi(g)- \phi(g\circ s)\right\Vert_\mathcal H
\end{equation}
By \cref{lemma:rbf-lip},
\begin{equation}
\left\Vert \int \phi(f) - \phi(f\circ s)\right\Vert_{\mathcal H} \leq \sum_{i=1}^{n-1} \delta_i^{3/2}C_\gamma\sqrt{\gamma L}
\end{equation}





Let $f$ be $L$-Lipschitz on $[0,1]$ and $\phi$ be the RBF kernel with parameter $\gamma > 0$.
Then, for any $\vert x - u \vert \leq \Delta$,
\begin{equation}
    \Vert \phi(f(x)) - \phi(f(u)) \Vert^2 \leq 2\left(1 - k(f(x), f(u))\right) \leq 2 \gamma L \Delta
\end{equation}
\end{proof}

\section{On the differentiability of DiffyTW}
\subsection{Proof of \cref{thm:diffytw-grad}}\label{sec:proof-diffytw-grad}
\begin{proof}
\todo[inline]{Make clean}
    $X$ is $\mathbb R^M$ which is a Haussdorf topological space. The set of admissible solutions $\mathcal C$ is non empty and is closed. Indeed, $\mathcal C$ is the intersection of a hyperplane ($h^\top q=1$) and of the non-negative quadrant ($q \geq 0$) ; both are closed so $\mathcal C$ is too.

    Denote $\ell(q, \theta) = \frac{1}{2}q^\top P(\theta) q - v(\theta)^\top q + C(\theta)$ where $\theta \mapsto P(\theta), \theta \mapsto v(\theta), \theta \mapsto C(\theta)$ are $\mathcal C^\infty$.

   Our goal is to show that for any $u_0\in U$, if $\bar x$ is the (unique) solution to quadratic program $\mathcal P(u_0)$, then
   \begin{equation}\label{eq:goal}
       v(u_0 + d) - v(u_0) = D_u \ell(\bar x, u)d + o\left(\Vert d\Vert\right)
   \end{equation}
   Let $u_0 \in U$, and let $\bar x$ the solution to $\mathcal P(u_0)$. Let $d$ such that $u_0 + d \in U$. Since $v(u_0) = \ell(\bar x, u_0)$, we have $v(u_0+d) \leq \ell(\bar x, u_0 + d)$. Thus,
   \begin{equation}
       v(u_0 + d) - v(u_0) - D_u\ell(\bar x, u_0)d \leq \ell(\bar x, u_0 + d) - \ell(\bar x, u_0) - D_u \ell(\bar x, u_0)d
   \end{equation}

    By the Mean Value Theorem, $\ell(\bar x, u_0 + d) - \ell(\bar x, u_0) = \int_0^1 D_u\ell(\bar x, u_0 + td)ddt$ and we can upper bound the previous equation by:
   \begin{align}
       v(u_0 + d) - v(u_0) - D_u\ell(\bar x, u_0)d&\leq \ell(\bar x, u_0 + d) - \ell(\bar x, u_0) - D_u \ell(\bar x, u_0)d\\
        &\leq \Vert d\Vert \int_0^1 \Vert D_u\ell(\bar x, u_0 +td) - D_u\ell(\bar x, u_0)\Vert dt
   \end{align}

   We denote $\varepsilon(d)= \int_0^1 \Vert D_u\ell(\bar x, u_0 +td) - D_u\ell(\bar x, u_0)\Vert dt$. By regularity of $x, u \mapsto D_u\ell(x, u)$, $u \mapsto D_u\ell(\bar x, u)$ is continuous at $u_0$ and so $ \delta \mapsto \Vert D_u\ell(\bar x, u_0 + \delta) -D_u\ell(\bar x, u_0)\Vert$ is continuous and equal to $0$ at $\delta=0$. By consequence, $\varepsilon$ is continuous and $\varepsilon(0)=0$.

   So,
   \begin{equation}
       v(u_0+d) - v(u_0) - D_u\ell(\bar x, u)d \leq \Vert d\Vert \varepsilon(d)
   \end{equation}

    Now, we assume corresponding lower bound is false, i.e. for any $\varepsilon>0$ and any $\eta >0$, there exists $d$ such that $u_0 + d\in U$ and
   \begin{equation}
       v(u_0+d) - v(u_0) - D_u\ell(\bar x, u)d < -\varepsilon\Vert d\Vert
   \end{equation}
    Let $d_n$ a sequence such that $d_n \to 0$ and
   \begin{equation}\label{eq:proof_hyp}
       v(u_0+d_n) - v(u_0) - D_u\ell(\bar x, u_0)d_n < -\varepsilon\Vert d_n\Vert
   \end{equation}

   We have a sequence $x_n\in \mathcal C$ such that $v(u_0+ d_n) = \ell(\bar x_n, u_0 + d_n)$. By Lemma 14.4 (Lee et al), there is subsequence of $(x_n)$ that converges towards $\bar x$ solution to $\mathcal P(u_0)$. By abuse of notation, we denote it $(x_n)$ as well.

   Then, by the Mean Value Theorem,
   \begin{align}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(x_n, u_0)d_n\Vert \leq \Vert d_n\Vert \int \Vert D_u \ell(x_n, u_0 + td_n) - D_u \ell(x_n, u_0)\Vert dt
   \end{align}

   For $n$ large enough, $u_0 + d_n$ is close to $u_0$ and $x_n$ is close to $\bar x$. By continuity of $x, u \mapsto D_u\ell(x, u)$,

   \begin{equation}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(x_n, u_0)d_n\Vert \leq \frac{1}{10} \varepsilon \Vert d_n \Vert
   \end{equation}

    and
    \begin{equation}
        \Vert D_u \ell(\bar x, u_0)d_n - D_u \ell(x_n, u_0)d_n\Vert \leq \frac{1}{4}\varepsilon \Vert d_n \Vert
    \end{equation}

    So, by the triangle inequality,

    \begin{equation}
       \Vert \ell(x_n, u_0 + d_n) - \ell(x_n, u_0) - D_u \ell(\bar x, u_0)d_n\Vert \leq \frac{1}{2} \varepsilon \Vert d_n \Vert
    \end{equation}
    Finally,
    \begin{align}
        v(u_0 + d_n) - v(u_0) - D_u \ell(x_n, u_0)&\geq\frac{1}{2}\varepsilon\Vert d_n\Vert
    \end{align}
    This contradicts \cref{eq:proof_hyp}.
\end{proof}

\subsection{Relation to implicit differentiation}\label{sec:lagrangian}
Our approach is to use implicit differentiation. Roughly, we proceed as follows: link optimality of $q^*$ using strong duality (Slater's Condition) and the KKT conditions ; rewrite the KKT conditions as an implicit function ; use the implicit function theorem.

\noindent We first state our result:

\begin{theorem}[Lagrangian approach]
$v(\theta) = l(\theta, q^*(\theta))$ is differentiable and is gradient is defined as:
\begin{equation}
    \nabla v(\theta) = \nabla_1 l(\theta, q^*(\theta)) + \nabla_2 l(\theta, q^*(\theta))J q^*(\theta)
\end{equation}
\end{theorem}

\begin{proof}
The Lagrangian of \cref{prob:fullqp} is:
\begin{equation}
    L(q, u, v, \theta) = \underbrace{\frac{1}{2}q^\top P(\theta)q - v(\theta)^\top q + C(\theta)}_{l(q, \theta)}+ u^\top (h^\top q - 1) - v^\top q
\end{equation}

where $L: \mathbb R^M \times \mathbb R \times \mathbb R^M\times\Theta\to\mathbb R$, as there are $M$ variables, $1$ equality constraint and $M$ inequality constraints, where we added the dependence on $\theta$.

For an optimal primal and dual solutsion $(q^*(\theta), u^*(\theta), v^*(\theta))$, the KKT conditions are verified:
\begin{align}
    0 &= \partial_q l(q^*, \theta) + u^*h - v^* &\text{(primal stationarity)}\\
    1 & =h^\top q^*(\theta)  & \text{(primal feasibility)} \\
    0 & \geq q^*(\theta) & \text{(primal feasibility)}\\
    0 & = \mathrm{Diag}(v^*(\theta))q(\theta) & \text{(complimentary slackness)}
\end{align}






\textbf{Step 2: translate the KKT conditions as an implicit function}

Let $g(q, u, v, \theta) = \begin{bmatrix}
    P(\theta)q - v(\theta) + uh - v\\
    h^\top q- 1\\
    \mathrm{Diag}(v)q
\end{bmatrix}$. $(q^*, u^*, v^*)$ are solutions if and only if $g(q^*, u^*, v^*, \theta) = 0$.

\textbf{Step 3: applying the implicit function theorem}
Denoting $z = (q, u, v)$, $g$ is a function of two variables $(z, \theta)$. To apply the implicit function theorem, we prove that $\partial_z g(z^*, \theta)$ is invertible for any solution $z^*$.

The partial Jacobian can be written:
\begin{align}
    J_z g(z^*, \theta) = \begin{bmatrix}
    P(\theta) & h^\top  & - I\\
    h & 0 & 0\\
    \mathrm{Diag}(v)& 0 & \mathrm{Diag}(q)
    \end{bmatrix}
\end{align}

If there exists $1 \leq i\leq m$ such that $v_i = q_i = 0$, $J_z g$ is not invertible. This corresponds to the ``zero-zero'' case for complementary slackness : the constraint is active and the Lagrange multiplier is $0$. In this case, at optimality, \textcolor{red}{todo todo}.


In the end,
\begin{align}
    J_\theta z^*(\theta) = - J_zg(z^*, \theta)^{-1}J_\theta g(z^*, \theta)
\end{align}
We can then combine these results:
\begin{align}
J_\theta v(\theta) &=  J_q l(q^*, \theta)J_\theta q^*(\theta) + J_\theta l(q^*, \theta),\\
&= J_q l(q^*, \theta)  \left[J_\theta z^*(\theta)\right]_{q, \theta} + J_\theta l(q^*, \theta),\\
&= - J_ql(q^*, \theta) \left[J_z g(z^*, \theta)^{-1} J_\theta g(z^*, \theta)\right]_{q,\theta}+ J_\theta l(q^*, \theta)
\end{align}
\end{proof}

Here we assume that $\theta$ is such that $q^*(\theta) > 0 $, so $v^*(\theta)=0$. Thus:

\begin{align}
    J_z g(z^*, \theta) = \begin{bmatrix}
    P(\theta) & - I & h\\
    0 & \mathrm{Diag}(q)& 0\\
    h^\top  & 0 & 0
    \end{bmatrix}
\end{align}
 and recall
\begin{align}
    J_\theta g(z^*, \theta) = \begin{bmatrix}
    -J_\theta v(\theta)\\
    0 \\
    0
    \end{bmatrix}
\end{align}

We solve the following system:
\begin{equation}
\begin{bmatrix}
    P(\theta) & - I & h\\
    0 & \mathrm{Diag}(q)& 0\\
    h^\top  & 0 & 0
    \end{bmatrix}
\begin{bmatrix}
    A \\
    B \\
    C
    \end{bmatrix} =
\begin{bmatrix}
    - J_\theta v(\theta) \\
    0 \\
    0
    \end{bmatrix}
\end{equation}

which is equivalent to:

\begin{align}
   P(\theta)A - B + hC =& -J_\theta v(\theta)\\
   \mathrm{Diag}(q)B =& 0\\
   h^\top A =& 0
\end{align}

The second equation implies that $B=0$, and the first implies
\begin{equation}
    A = - P(\theta)^{-1}hC - P(\theta)^{-1}J_\theta v(\theta)
\end{equation}

And
\begin{align}
    J_ql(q^*,\theta)A &= (q^\top P(\theta) - v(\theta)^\top )A = h^\top A = 0
\end{align}
\vskip 0.2in
