@book{shapiro,
	title = {Perturbation analysis of optimization problems},
	author = {Bonnans, J Fr{\'e}d{\'e}ric and Shapiro, Alexander},
	year = 2013,
	publisher = {Springer Science \& Business Media}
}
@book{lee,
	title = {Quadratic Programming and Affine Variational Inequalities},
	author = {Gue Myung Lee and Nguyen Nang Tam and Nguyen Dong Yen},
	year = 2005,
	editor = {Springer}
}
@misc{tavenard-dtw-diff,
	title = {Differentiability of DTW and the case of soft-DTW},
	author = {Romain Tavenard},
	year = 2021,
	howpublished = {\url{https://rtavenar.github.io/blog/softdtw.html}}
}
@inproceedings{sdtw,
	title = {Soft-{DTW}: a Differentiable Loss Function for Time-Series},
	author = {Marco Cuturi and Mathieu Blondel},
	year = 2017,
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	volume = 70,
	pages = {894--903},
	url = {https://proceedings.mlr.press/v70/cuturi17a.html},
	editor = {Precup, Doina and Teh, Yee Whye},
	pdf = {http://proceedings.mlr.press/v70/cuturi17a/cuturi17a.pdf},
	abstract = {We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a <em>differentiable</em> loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at https://github.com/mblondel/soft-dtw}
}
@article{dtw-sakoe,
	title = {Dynamic programming algorithm optimization for spoken word recognition},
	author = {Sakoe, H. and Chiba, S.},
	year = 1978,
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	volume = 26,
	number = 1,
	pages = {43--49},
	doi = {10.1109/TASSP.1978.1163055},
	keywords = {Dynamic programming;Heuristic algorithms;Fluctuations;Timing;Signal processing algorithms;Speech processing;Pattern matching;Constraint optimization;Feature extraction;Acoustics}
}
@inproceedings{dtw-baseline-1,
	title = {Dynamic Time Warping: Itakura vs Sakoe-Chiba},
	author = {Geler, Zoltan and Kurbalija, Vladimir and Ivanović, Mirjana and Radovanović, Miloš and Dai, Weihui},
	year = 2019,
	booktitle = {2019 IEEE International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)},
	volume = {},
	number = {},
	pages = {1--6},
	doi = {10.1109/INISTA.2019.8778300},
	keywords = {Time series analysis;Time measurement;Microsoft Windows;Task analysis;Classification algorithms;Heuristic algorithms;Prediction algorithms;time series;distances;DTW;constraints}
}
@article{dtw-baseline-2,
	title = {Deep learning for time series classification: a review},
	author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	year = 2019,
	journal = {Data Min. Knowl. Discov.},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = 33,
	number = 4,
	pages = {917–963},
	doi = {10.1007/s10618-019-00619-1},
	issn = {1384-5810},
	url = {https://doi.org/10.1007/s10618-019-00619-1},
	issue_date = {Jul 2019},
	abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
	numpages = 47,
	keywords = {Review, Classification, Time series, Deep learning}
}
@article{dba-petitjean,
	title = {A global averaging method for dynamic time warping, with applications to clustering},
	author = {François Petitjean and Alain Ketterlin and Pierre Gançarski},
	year = 2011,
	journal = {Pattern Recognition},
	volume = 44,
	number = 3,
	pages = {678--693},
	doi = {https://doi.org/10.1016/j.patcog.2010.09.013},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S003132031000453X},
	keywords = {Sequence analysis, Time series clustering, Dynamic time warping, Distance-based clustering, Time series averaging, DTW barycenter averaging, Global averaging, Satellite image time series},
	abstract = {Mining sequential data is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. Most works in this field are centred on the definition and use of a distance (or, at least, a similarity measure) between sequences of elements. A measure called dynamic time warping (DTW) seems to be currently the most relevant for a large panel of applications. This article is about the use of DTW in data mining algorithms, and focuses on the computation of an average of a set of sequences. Averaging is an essential tool for the analysis of data. For example, the K-means clustering algorithm repeatedly computes such an average, and needs to provide a description of the clusters it forms. Averaging is here a crucial step, which must be sound in order to make algorithms work accurately. When dealing with sequences, especially when sequences are compared with DTW, averaging is not a trivial task. Starting with existing techniques developed around DTW, the article suggests an analysis framework to classify averaging techniques. It then proceeds to study the two major questions lifted by the framework. First, we develop a global technique for averaging a set of sequences. This technique is original in that it avoids using iterative pairwise averaging. It is thus insensitive to ordering effects. Second, we describe a new strategy to reduce the length of the resulting average sequence. This has a favourable impact on performance, but also on the relevance of the results. Both aspects are evaluated on standard datasets, and the evaluation shows that they compare favourably with existing methods. The article ends by describing the use of averaging in clustering. The last section also introduces a new application domain, namely the analysis of satellite image time series, where data mining techniques provide an original approach.}
}
@misc{ucr,
	title = {The UCR Time Series Classification Archive},
	author = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo, and Hexagon-ML},
	year = 2018,
	}
@inbook{dtw-music,
	title = {Dynamic Time Warping},
	year = 2007,
	booktitle = {Information Retrieval for Music and Motion},
	publisher = {Springer Berlin Heidelberg},
	address = {Berlin, Heidelberg},
	pages = {69--84},
	doi = {10.1007/978-3-540-74048-3_4},
	isbn = {978-3-540-74048-3},
	url = {https://doi.org/10.1007/978-3-540-74048-3_4},
	abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.}
}
@inproceedings{cdtw,
	title = {{Computing Continuous Dynamic Time Warping of Time Series in Polynomial Time}},
	author = {Buchin, Kevin and Nusser, Andr\'{e} and Wong, Sampson},
	year = 2022,
	booktitle = {38th International Symposium on Computational Geometry (SoCG 2022)},
	publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	address = {Dagstuhl, Germany},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	volume = 224,
	pages = {22:1--22:16},
	doi = {10.4230/LIPIcs.SoCG.2022.22},
	isbn = {978-3-95977-227-3},
	issn = {1868-8969},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.SoCG.2022.22},
	editor = {Goaoc, Xavier and Kerber, Michael},
	urn = {urn:nbn:de:0030-drops-160307},
	annote = {Keywords: Computational Geometry, Curve Similarity, Fr\'{e}chet distance, Dynamic Time Warping, Continuous Dynamic Time Warping}
}
@inproceedings{frechet-clustering,
	title = {Clustering time series under the fr\'{e}achet distance},
	author = {Driemel, Anne and Krivo\v{s}ija, Amer and Sohler, Christian},
	year = 2016,
	booktitle = {Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms},
	location = {Arlington, Virginia},
	publisher = {Society for Industrial and Applied Mathematics},
	address = {USA},
	series = {SODA '16},
	pages = {766–785},
	isbn = 9781611974331,
	abstract = {The Fr\'{e}chet distance is a popular distance measure for curves. We study the problem of clustering time series under the Fr\'{e}chet distance. In particular, we give (1 + ε)-approximation algorithms for variations of the following problem with parameters k and ℓ. Given n univariate time series P, each of complexity at most m, we find k time series, not necessarily from P, which we call cluster centers and which each have complexity at most ℓ, such that (a) the maximum distance of an element of P to its nearest cluster center or (b) the sum of these distances is minimized. Our algorithms have running time near-linear in the input size for constant ε, k and ℓ. To the best of our knowledge, our algorithms are the first clustering algorithms for the Fr\'{e}chet distance which achieve an approximation factor of (1 + ε) or better.},
	numpages = 20,
	keywords = {time series, longitudinal data, functional data, dynamic time warping, clustering, approximation algorithms, Fr\'{e}chet distance}
}
@article{dtw-incomparable,
	title = {Aligning Time Series on Incomparable Spaces},
	author = {Samuel Cohen and Giulia Luise and Alexander Terenin and Brandon Amos and Marc P. Deisenroth},
	year = 2020,
	journal = {arXiv:2006.12648},
	url = {https://arxiv.org/abs/2006.12648},
	abstract = {Dynamic time warping (DTW) is a useful method for aligning, comparing and combining time series, but it requires them to live in comparable spaces. In this work, we consider a setting in which time series live on different spaces without a sensible ground metric, causing DTW to become ill-defined. To alleviate this, we propose Gromov dynamic time warping (GDTW), a distance between time series on potentially incomparable spaces that avoids the comparability requirement by instead considering intra-relational geometry. We derive a Frank-Wolfe algorithm for computing it and demonstrate its effectiveness at aligning, combining and comparing time series living on incomparable spaces. We further propose a smoothed version of GDTW as a differentiable loss and assess its properties in a variety of settings, including barycentric averaging, generative modeling and imitation learning.},
	date = {2020-06-22},
	journaltitle = {arXiv:2006.12648}
}
@article{vayer2022time,
	title = {Time Series Alignment with Global Invariances},
	author = {Vayer, Titouan and Tavenard, Romain and Chapel, Laetitia and Flamary, R{\'e}mi and Courty, Nicolas and Soullard, Yann},
	year = 2022,
	journal = {Transactions on Machine Learning Research},
	note = {https://openreview.net/forum?id=JXCH5N4Ujy}
}
@article{curve-moments,
author = {Gareth M. James},
title = {{Curve alignment by moments}},
volume = {1},
journal = {The Annals of Applied Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {480 -- 501},
keywords = {continuous monotone registration, Curve registration, landmark registration, moments},
year = {2007},
doi = {10.1214/07-AOAS127},
URL = {https://doi.org/10.1214/07-AOAS127}
}
@article{2108.05634,
      author = {Bauer, Alexander  and Scheipl, Fabian and K\"uchenhoff, Helmut and Gabriel, Alice-Agnes},
      doi = {arxiv:2108.05634},
      journal = {under review},
      language = {en},
      title = {Registration for Incomplete Non-Gaussian Functional Data},
      url = {https://arxiv.org/abs/2108.05634},
      year = {2021},
    }
@article{atar1,
title = {Exponential stability for nonlinear filtering},
journal = {Annales de l'Institut Henri Poincare (B) Probability and Statistics},
volume = {33},
number = {6},
pages = {697-725},
year = {1997},
issn = {0246-0203},
doi = {https://doi.org/10.1016/S0246-0203(97)80110-0},
url = {https://www.sciencedirect.com/science/article/pii/S0246020397801100},
author = {Rami Atar and Ofer Zeitouni},
keywords = {Nonlinear filtering, nonlinear smoothing, exponential stability, Birkhoff contraction coefficient},
abstract = {We study the a.s. exponential stability of the optimal filter w.r.t. its initial conditions. A bound is provided on the exponential rate (equivalently, on the memory length of the filter) for a general setting both in discrete and in continuous time, in terms of Birkhoff's contraction coefficient. Criteria for exponential stability and explicit bounds on the rate are given in the specific cases of a diffusion process on a compact manifold, and discrete time Markov chains on both continuous and discrete-countable state spaces. A similar question regarding the optimal smoother is investigated and a stability criterion is provided.
Résumé
Nous étudions la stabilité du filtre optimal par rapport à ses conditions initiales. Le taux de décroissance exponentielle est calculé dans un cadre général, pour temps discret et temps continu, en terme du coefficient de contraction de Birkhoff. Des critères de stabilité exponentielle et des bornes explicites sur le taux sont calculés pour les cas particuliers d'une diffusion sur une variété compacte, ainsi que pour des chaînes de Markov sur un espace discret ou continu.}
}
﻿@Inbook{chigansky,
author={Chigansky, P.
and Liptser, R.
and Van Handel, R.},
title={Intrinsic methods in filter stability},
bookTitle={The Oxford handbook of nonlinear filtering},
series={The Oxford handbook of nonlinear filtering},
year={2011},
publisher={Oxford Univ. Press, Oxford}
}



@ARTICLE{gordon-pf,
   author = {N.J. Gordon},
   author = {D.J. Salmond},
   author = {A.F.M. Smith},
   keywords = {state transition model;algorithm;bootstrap filter;recursive Bayesian filters;extended Kalman filter;simulation;measurement model;Gaussian noise;nonGaussian Bayesian state estimation;bearings only tracking problem;nonlinear Bayesian state estimation;state vector density;random samples;},
   ISSN = {0956-375X},
   language = {English},
   abstract = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
   title = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
   journal = {IEE Proceedings F (Radar and Signal Processing)},
   issue = {2},
   volume = {140},
   year = {1993},
   pages = {107-113(6)},
   publisher ={},
   copyright = {© The Institution of Electrical Engineers},
   url = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015}
}
@book{shawe-taylor, place={Cambridge}, title={Kernel Methods for Pattern Analysis}, publisher={Cambridge University Press}, author={Shawe-Taylor, John and Cristianini, Nello}, year={2004}}
@book{scholkopf-kernels,
    author = {Schölkopf, Bernhard and Smola, Alexander J.},
    title = "{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}",
    publisher = {The MIT Press},
    year = {2001},
    abstract = "{A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs—-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}",
    isbn = {9780262256933},
    doi = {10.7551/mitpress/4175.001.0001},
    url = {https://doi.org/10.7551/mitpress/4175.001.0001},
}
@unpublished{QP-Layer,
  TITLE = {{QPLayer: efficient differentiation of convex quadratic optimization}},
  AUTHOR = {Bambade, Antoine and Schramm, Fabian and Taylor, Adrien and Carpentier, Justin},
  URL = {https://inria.hal.science/hal-04133055},
  NOTE = {working paper or preprint},
  YEAR = {2023},
  KEYWORDS = {Machine Learning ; Optimization ; Differentiable Optimization ; Optimization layers},
  PDF = {https://inria.hal.science/hal-04133055/file/QPLayer_Preprint.pdf},
  HAL_ID = {hal-04133055},
  HAL_VERSION = {v1},
}
@inproceedings{optnet,
author = {Amos, Brandon and Kolter, J. Zico},
title = {OptNet: differentiable optimization as a layer in neural networks},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end train-able deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {136–145},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@misc{qpax,
      title={On the Differentiability of the Primal-Dual Interior-Point Method},
      author={Kevin Tracy and Zachary Manchester},
      year={2024},
      eprint={2406.11749},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2406.11749},
}
@INPROCEEDINGS{kunita,
  author={Blankenship, G. L. and Hopkins, W. E. and Marcus, S. I.},
  booktitle={1981 20th IEEE Conference on Decision and Control including the Symposium on Adaptive Processes},
  title={Asymptotic behavior of nonlinear filters},
  year={1981},
  volume={},
  number={},
  pages={89-89},
  keywords={Nonlinear filters;Educational institutions;Filtering;Markov processes;Steady-state;State estimation;Nonlinear equations},
  doi={10.1109/CDC.1981.269451}}

﻿@Article{blanchard,
author={Blanchard, Gilles
and M{\"u}cke, Nicole},
title={Optimal Rates for Regularization of Statistical Inverse Learning Problems},
journal={Foundations of Computational Mathematics},
year={2018},
day={01},
volume={18},
number={4},
pages={971-1013},
abstract={We consider a statistical inverse learning (also called inverse regression) problem, where we observe the image of a function f through a linear operator A at i.i.d. random design points {\$}{\$}X{\_}i{\$}{\$}, superposed with an additive noise. The distribution of the design points is unknown and can be very general. We analyze simultaneously the direct (estimation of Af) and the inverse (estimation of f) learning problems. In this general framework, we obtain strong and weak minimax optimal rates of convergence (as the number of observations n grows large) for a large class of spectral regularization methods over regularity classes defined through appropriate source conditions. This improves on or completes previous results obtained in related settings. The optimality of the obtained rates is shown not only in the exponent in n but also in the explicit dependency of the constant factor in the variance of the noise and the radius of the source condition set.},
issn={1615-3383},
doi={10.1007/s10208-017-9359-7},
url={https://doi.org/10.1007/s10208-017-9359-7}
}
