\section{Contributions}

In this thesis, we design and study methods for leveraging structure in learning problems to create better algorithms.
Each of our three contribuions is motivated, described and studied in a chapter. Each chapter is meant to be self-contained, and all necessary notations and definitions are recalled.

In this section, we outline the structure of the manuscript before introducing each contribution below.

\begin{itemize}
\item In \cref{ch:diffy}, we design a dissimilarity on general data spaces that is invariant to smooth diffeomorphisms. This is motivated by the identification of stability to diffeomorphisms as a factor of performance of deep neural networks and of invariance and stability as way of reducing the difficulty of learning problems.

\item In \cref{ch:diffytw}, we turn to time-series and introduce a divergence between time-series that is invariant to smooth reparametrizations. Reparametrizations can eliminate common variations in time-series such as misalignment, which is a widely studied problem.

\medskip

Taken together, \cref{ch:diffy,ch:diffytw} identify smooth diffeomorphisms as a rich class of transformations which induce useful invariance on data and propose solutions to leverage this prior knowledge in algorithms.

\item  In \cref{ch:hmm}, we leverage the Markovian dependence structure between observations and the underlying signals in Hidden Markov models for filtering. We identify an additional form of structure: non-negativity of conditional densities. While it may seem trivial at first glance, by combining these structures we propose a non-linear filtering algorithm with strong guarantees and competitive computational complexity.
\end{itemize}

\noindent In the rest of the introduction, we present each contribution in turn.

\subsection{Enforcing diffeormorphism invariance in distance-based methods}\label{sec:contributions-ch2}
\begin{mdframed}
The contribution described in this section is presented in \cref{ch:diffy}.

\noindent It was published in the proceedings of the \emph{International Conference on Machine Learning} in 2022 in the following publication:
\begin{mdframed}
\Myprod{\textbf{Théophile Cantelobre}, Benjamin Guedj, Carlo Ciliberto, Alessandro Rudi}
{Measuring dissimilarity with diffeormorphism invariance}{International Conference on Machine Learning (ICML), Baltimore, Maryland (USA), 2022}
\end{mdframed}
The library developed accompanying \cref{ch:diffy} is available at \url{github.com/theophilec/diffy}.
\end{mdframed}

\paragraph{Motivation}
Distance-based methods -- such as k-nearest neighbors and k-means -- are widely used in practical machine learning settings. They have been studied as proxies for understanding the performance of more complex methods such as neural networks. \cite{thiry} introduces a dictionary-based model achieving comparable performance to shallow Convolutional Neural Networks (CNN). The model replaces the learnable filter in Convolution Neural Networks (CNN) with a fixed dictionary of patches taken from the dataset. The embedded features (before the linear classification layer) are computed via the Euclidean distance between each patch of the image being embedded and the patches of the dictionary, which are then discretized and pooled before a linear classification layer is applied. The main takeaway from \cite{thiry} is that a model that embeds an image according to the distance of its patches to a fixed dictionary sampled from the dataset can perform competitively on image classification tasks.

The model in \cite{thiry} enforces translation invariance, like CNNs do, but does not incorporate any of the other invariances that can be useful in computer vision. In practice, there are two main ways of incorporating invariances into a machine learning algorithm: group averaging and data augmentation.

Group averaging implies aggregating features over the set of invariances (which can have a group structure)
\begin{equation}
    \Phi(x) = \sum_{\tau \in \mathcal T} \phi(\tau \cdot x)
\end{equation}
where $\mathcal T$ is a group of transformations. An example for $\mathcal T$ is the set of rotations by $90$ degrees, which is a finite group. If $\tau_0\in\mathcal T$, then $\Phi(\tau_0 \cdot x) = \Phi(x)$. Computing $\Phi$ requires computing $\phi(\tau \cdot x)$ for every $\tau\in\mathcal T$. Handling a rich set of invariances makes $\mathcal T$ either very large or infinite. This makes $\Phi$ difficult to express and computationally expensive \cite{bietti}.

Data augmentation relies on enforcing the invariance at training time by adding transformed versions of training points $\tau \cdot x$ for $\tau \in \mathcal T$ to the training set. In essence, this augments the training set to account for invariances, which are not necessarily present (sufficiently). This makes training computationally more expensive, is limited in the richness of tranformations (augmented examples need to be generated) and does not guarantee invariance at inference.

The divergence we design and study in \cref{ch:diffy} enforces invariance and is meant as a drop-in replacement for other distances in distance-based machine learning methods.

\paragraph{Intuition}

The problem we tackle is designing a dissimilarity $D$ that compares datapoints such that $D(x, y) = 0$ if $y = \tau \cdot x$ if $\tau$ is a smooth transformation.

\subparagraph{Seeing datapoints as functions on continuous domains} A key insight to this problem is to consider data points as functions and transformations as diffeomorphisms that act on the domain of the functions. For instance, an image is seen as a function from $\mathbb R^2$ (position on the image) to $\mathbb R^3$ (RGB value at that point). Transformations on images are then diffeomorphisms on $\mathbb R^2$, which include rigid-body transformations (translation, rotations) but also dilatations and warpings.
In order to capture a rich set of transformations, representative of naturally occuring variations in data, we consider continuous domains instead of discrete domains. This is in contrast to \cite{bietti} which considers discrete domains for the considered transformations.
In the end, our goal becomes to design $D$ such that if $Q$ is a diffeomorphism and $f$ a datapoint (function), $D(f\circ Q, f) = 0$ (or is very small).

\subparagraph{Change of variable theorem}
In addition to being a rich class of transformations, smooth diffeomorphisms have the added benefit of working well with integration.
$f$ and $f \circ Q$ have the same integral up to reweighting by $\vert \nabla Q(x)\vert$. We can make this stronger (especially for low-dimensional data like images, since $f(x)$ only has 3 dimensions) by embedding image features in an RKHS. Then, weighting the integral of $\phi(f \circ Q)$ such as it matches that of $\phi(f)$ is akin to making all the statistics of the images $f\circ Q$ and $f$ match, while reweighting certain parts of $f\circ Q$. The reweighting $q$ that minimizes the distance between $\int \phi(f\circ Q(x))q(x)dx$ and $\int \phi(g(x))dx$ is $q(x) = \vert \nabla Q(x)\vert$. We then add two ingredients to this intuition to make it work. First, because we cannot ensure that a given reweighting is indeed the jacobian determinant term of a (global) diffeomorphism, we enforce that the reweightings match over the image. The intuition behind this is seeing reweightings as selecting a region of an image and enforcing the statistics to match. Second, we intuitively wish to consider only ``simple'' regions, i.e. smooth reweightings, and for this we regularize over the choice of reweightings. This interpretation has the added value of giving the dissimilarity a explainibility aspect: we can visualize which regions of an image are matching.

\paragraph{Main contributions}
Diffy is a divergence between functions that is approximately invariant to smooth diffeomorphisms.
It is defined as an optimization problem (see \cref{sec:informal} for an intuitive derivation) and can be computed in closed-form:
\begin{mdframed}
\begin{informaltheorem}[based on \cref{def:D} and \cref{theorem:closed-form}]
Given $f, g$ to functions and $\mathcal H$ and $\mathcal F$ two reproducing kernel Hilbert spaces, $D_\lambda(f, g)$ is defined and can be computed in closed-form as follows:
\eqals{\label{eq:def-D}
    D_\lambda(f, g) := & \max_{\norh{h}\leq 1}\min_{q \in \mathcal H}\max_{\|v\|_{{\cal F}} \leq 1} \Big|  \int_X v(g(x))q(x)\dd x-  \int_X v(f(x))\mu(x)h(x)\dd x\Big|^2 + \lambda \norh{q}^2\\
   =& \lambda\norop{(GG^* + \lambda I)^{-1/2}F_\mu}^2.
}
where $G$, $F_\mu$ linear operators defined with $\phi$, $f$, $\psi$ and $\mu$.
\end{informaltheorem}
\end{mdframed}

Diffy is approximately invariant to smooth transformations to data. The residual term is related to the regularization term, which ensures that we only consider smooth solutions $q$ and, in spirit, smooth diffeomorphisms.
\begin{mdframed}
\begin{informaltheorem}[based on \cref{theorem:main-theorem}]
With smooth Sobolev kernels and a smooth diffeomorphism $Q$, we have:
\eqals{
 D_\la(f, f \circ Q) ~~\leq~~ \lambda ~ C^2_{\mu} C^2_{Q} \qquad \forall f\, \textrm{measurable},
}
where $C_\mu$ and $C_Q$ are independent of $f$.
\end{informaltheorem}
\end{mdframed}
\noindent In the limit of vanishing regularization, Diffy is invariant to smooth differomorphisms. Indeed, as $\la \to 0$, the set of admissible solutions become larger and contains all diffeomorphisms determinant jacobian functions and $D_\la(f, f\circ Q)\to 0$.

In practice, Diffy can be computed on discrete objects using domain samples and values. For images, this means the natural pixel locations and values. Plug-in estimators incorporating Nyström techniques achieve convergence of the approximate $\hat D_\lambda(f, g)$ to $D_\lambda(f, g)$ as the number of samples (\emph{e.g.\ } of pixels) grows. This is proven in the informal result below:
\begin{mdframed}
\begin{informaltheorem}[based on \cref{thm:appr-error-widehatD}]
With rich enough Nyström approximations, with high probability over the draw of $N$ sample locations for each of $f$ and $g$,
\eqals{
 \vert D_\la(f, g) - \widehat D_\la(f, g) \vert = O(N^{-1/4}).
}
This induces computational complexity $O(N^{3/2})$.
\end{informaltheorem}
\end{mdframed}
\noindent \cref{thm:appr-error-widehatD} and \cref{theorem:main-theorem} imply that $\hat D_\lambda(f\circ Q, f)\to 0$ as $N\to \infty$.

We apply Diffy to comparing images with natural and synthetic transformations and study its behavior.

\paragraph{Key takeways}

\begin{itemize}
    \item Diffy is a divergence defined over functions between general spaces, defined as an optimization problem matching the integrals of embeddings of functions in Reproducing kernel Hilbert spaces.
    \item Diffy is efficient to compute and is useful for comparing a wide range of data types, including images, videos, and point clouds. Specifically, we demonstrate that Diffy is computable in $O(N^{3/2})$ time with a guaranteed bound on the error of order $O(N^{-1/4})$ (see \cref{sec:approximation}) where $N$ is the number of sample points (\emph{e.g.\ } pixels for images) in each image, leveraging Nyström approximation methods.
    \item Diffy is approximately invariant to smooth diffeomorphisms, with level of invariance precisely controlled by a regularization parameter and the parameters of the kernels (see \cref{theorem:main-theorem}).
    \item We conduct comprehensive experiments to rigorously analyze the behavior of Diffy in practical applications, making it a strong candidate for integration in machine learning pipelines. All code is made publicly available in the \texttt{diffy} library\footnote{Available at \url{github.com/theophilec/diffy}.}.
\end{itemize}

\subsection{Handling time-warping and misalignment when comparing time-series}
\begin{mdframed}
The contribution described is presented in \cref{ch:diffytw}.
\noindent It has previously been shared online and is under review in the following pre-publication:
\begin{mdframed}
\Myprod{\noindent\textbf{Théophile Cantelobre}, Benjamin Guedj, Carlo Ciliberto, Alessandro Rudi}
{Comparing time-series with smooth, time-warping invariance}{Under review (2024)}
\end{mdframed}
The library developed accompanying \cref{ch:diffytw} is available at \url{github.com/theophilec/diffytw}.
\end{mdframed}
\paragraph{Motivation}
Time-series exhibit naturally occurring variations make analysis and inference challenging.

One common issue is misalignment. Comparing two time-series of the same signal but with different sample points is not straightforward. Indeed, consider $(x_i, f(x_i))_{1\leq i\leq n}$ and $(y_j, f(y_j))_{1 \leq j \leq m}$ two time-series representing the same underlying signal but with different sampling patterns. Treating the time series as vectors in Euclidean space is not satisfying as the points do not align in time. In fact, if $n\neq m$, it is not even obvious how to do this.

Time warping is a related problem. There is an interest in having a way of comparing trajectories that is invariant to smooth transformations such as reparametrization of time caused by speed differences, while also eliminating the warping and aligning the time-series.

We posit that to be robust to these discrepancies, divergences between time-series should aim to be invariant to time-warping. Despite their practical utility, DTW \citep{dtw} and its variants such as Soft-DTW \citep{soft-dtw} do not have such guarantees. Other methods focus exclusively on aligning the time-series, as outlined in \cref{background:diffytw}.

\paragraph{Intuition}
The key insight to our approach is that a smooth reparametrization over $[0,1]$ is an increasing, smooth bijective function over $[0,1]$. Since these are 1D functions, they can be very efficiently estimated using function approximation. We combine this with the idea from \cref{sec:contributions-ch2} to compare reweighted integrals. In this case, the derivative of a repametrization $Q$ makes the statistics of a time-series and its warped counterpart match.

DiffyTW is designed to produce a proper smooth reparametrization that aligns the two time-series, in contrast to \cref{ch:diffy}. Indeed, in higher-dimension, one cannot infer $Q$ from $\vert \nabla Q\vert$ and more generally, given a smooth, non-negative real function $q$, there is no guarantee that there exists a diffeomorphism $Q$ such that $q(x) = \vert \nabla Q(x) \vert$.

As we explain below and prove in \cref{ch:diffytw}, we can define a discrete counterpart for DiffyTW for time-series that is consistent with DiffyTW. Intuitively, this consistency -- which indirectly guarantees that certain invariance properties are ``transferred'' to Discrete DiffyTW -- relies on considering functions to interpolate the time-series and defining Discrete DiffTW to coincide with their DiffyTW value.

\paragraph{Main contributions}Following the intuition outlined above, we define DiffyTW as follows:
\begin{equation*}
d(f, g) = \min_{q \in \mathcal F_{0,1}}\left\Vert \int_0^1 \phi(f(x))q(x)dx - \int_0^1\phi(g(x))dx\right\Vert^2_\mathcal H,
\end{equation*}
where $\mathcal F_{0,1}$ is a set of smooth, non-negative functions that sum to $1$ and $\phi$ is an embedding map (for instance a neural network feature embedding or a positive definite kernel embedding).

DiffyTW is invariant to smooth reparametrizations. Indeed, thanks to the change of variable theorem:
\begin{mdframed}
\begin{informaltheorem}[based on \cref{thm:diffytw-invariance}]
For any $f:[0,1] \to\mathbb R^d$ and $Q:[0,1]\to[0,1]$ such that $Q^\prime \in\mathcal F_{0,1}$,
\begin{equation*}
d(f\circ Q, f) = 0
\end{equation*}
\end{informaltheorem}
\end{mdframed}

In defining Discrete DiffyTW $\hat d_M$, we tackle two issues: first, handling discrete time-series $\hat f, \hat g$ instead of functions $f, g$; second, choosing $\mathcal F_{0,1}$ to be set of functions we can optimize over. For the first, we interpolate $\hat f$ and $\hat g$ with piece-wise constant functions $f$ and $g$ and use the obtained expression of $d(f, g)$ as the definition of $\hat d_M$. For the second, we consider non-negaitve linear models with $M$ fixed basis functions, in the spirit of Gaussian Mixture Models. This leads to the following definition for Discrete DiffyTW as the optimal value of a Quadratic program with $M$ variables and linear inequality and equality constraints:
\begin{equation*}
    \hat d_M(\hat f, \hat g) :=\min_{\substack{q\in\mathbb R^{M}\\h^\top q=1\\0 \leq q}}\frac{1}{2}q^\top Pq - v^\top q + C
\end{equation*}
where $P$, $v$ and $C$ depend on $\hat f$, $\hat g$ and $h$ depends only on basis functions. Using interior point methods, solving the QP has $O(M^3)$ time complexity.

We show this definition is consistent with that of DiffyTW for piece-wise constant functions. This allows us to transfer invariance properties from DiffyTW to Discrete DiffyTW in the limit of a large number of samples (see \cref{thm:prob-qp}). Note that additionally, despite being applied to discrete time-series, Discrete DiffyTW still estimates a proper smooth reparametrization.

Discrete DiffyTW is differentiable with respect to its second argument, making it amenable to being used in averaging tasks such as clustering. To highlight where the difficulty is, let us recast this as differentiating $V(\theta) = \min_{q\in\mathcal C}\ell(q, \theta)$ where $\ell$ is the QP objective for parameter $\theta$ at $q$ and $\mathcal C$ is the set of constraints. In our setting, we prove we can differentiate ``through'' the $\min$ operator, that is:
\begin{mdframed}
\begin{informaltheorem}[based on \cref{thm:diffytw-grad}]
If the problem is smooth with respect to $\theta$ (e.g. if an RBF kernel is used) and if it has a unique solution (e.g. if $P$ is positive definite), then $V$ is differentiable at $\theta$ and its gradient is given by:
    \begin{equation}
        \nabla V(\theta) = J_\theta \ell(q^*(\theta), \theta) + \nabla C(\theta),
    \end{equation} where $q^*(\theta)$ is the unique solution to $\min_{q\in\mathcal C}\ell(q, \theta)$.
\end{informaltheorem}
\end{mdframed}
For intuition, this result can be read as ``$\hat d_M(\hat f, \cdot)$ is differentiable at $\hat g$ with gradient...''.

We apply DiffyTW time-series with synthetic reparametrizations as well as naturally occuring variations, and study its sensitivity to kernel parameters.

\paragraph{Key takeaways}
\begin{itemize}
    \item DiffyTW is a divergence specifically designed for comparing functions on $[0,1]$, tailored to represent time-series data.
    \item DiffyTW is provably invariant to a class of smooth reparametrizations, which can be configured depending on the problem. As a by product of the computations, it produces the reparametrization so the functions can be aligned.
    \item DiffyTW enables robust and efficient time-series comparison through its discrete counterpart, while approximately preserving the key invariance properties of DiffyTW.
    \item DiffyTW is differentiable with respect to its inputs, allowing seamless integration into broader machine learning workflows.
    \item We empirically demonstrate and analyze the behavior of DiffyTW on various time-series datasets. All code is made publicly available in the \texttt{diffytw} library\footnote{Available at \url{github.com/theophilec/diffytw}.}.
\end{itemize}

\subsection{Leveraging PSD models for stable and robust non-linear filtering}
\begin{mdframed}
The contribution described is presented in \cref{ch:hmm}.
\noindent It has previously been shared online and is under review in the following pre-publication:
\begin{mdframed}
\Myprod{\textbf{Théophile Cantelobre}, Benjamin Guedj, Carlo Ciliberto, Alessandro Rudi}
{Closed-form filtering for non-linear systems}{Under review (2024)}
\end{mdframed}
%The library developed accompanying \cref{ch:hmm} is available at \url{github.com/theophilec/psdfilter}.
\end{mdframed}

\paragraph{Motivation}
We consider the filtering problem for discrete-time Hidden Markov Models on general state-spaces. The goal of filtering is to compute (or approximate) the filtering distribution $\pi_t(dx) = \mathbb P(X_t\vert Y_1, \ldots, Y_{t})$. Markov assumptions on the model reduce computing the optimal filter to iterating the following equation:
\begin{equation}\label{eq:intro-hmm-iteration}
\pi_k(dx) = \frac{\int \pi_{k-1}(du)Q(u, dx)G(x, y_k)}{\int \int \pi_{k-1}(du)Q(u, dx)G(x, y_k)},
\end{equation}
where $G$ and $Q$ describe the Hidden Markov Model.

In general state-spaces, computing \cref{eq:intro-hmm-iteration} implies either making strong assumptions on the model (\emph{e.g.\ } Kalman filter), approximating the iteration while assuming a functional form for $\pi_T$ (\emph{e.g.\ } variants of the Kalman filter) or approximation \cref{eq:intro-hmm-iteration} using Monte Carlo sampling (\emph{e.g.\ } the particle filter). The particle filter can be made stable and robust, but does not produce a closed-form distribution.

Beyond the Markovian structure, we aim to leverage a second source of structure in filtering, that (conditional) probability distributions are non-negative functions. This can seem trivial at first glance but the idea has recently attracted interest with the development of Positive Semi-Definite Models (PSD models) by \citet{ulysse-non-negative}. PSD models are a class of non-negative functions based on the kernel sum-of-squares framework which optimally approximate smooth, non-negative functions such as (conditional) densities. These approximations are solutions to convex problems. When the RBF kernel is used, this family is useful for approximating probability densities. Indeed, products and marginals of PSD models are PSD models, which can be computed efficiently. For a primer see \citet{rudi2021psd,sampling-ulysse}.



\paragraph{Intuition}

The approach we propose works in two steps. First, using function evaluations of $Q$ and $G$ (we assume the kernels have densities), we learn $\hat Q$, $\hat G$ as Gaussian PSD models. This step is done ``offline'' \emph{via} a convex problem. Second, when we receive a sequence $y_1, \ldots, y_N$, we compute the sequence of approximate filtering distributions $\hat\pi_k$ by applying \cref{eq:intro-hmm-iteration} in closed-form with $\hat Q$, $\hat G$ and $\hat \pi_k$ \emph{in lieu} of $Q$, $G$ and $\pi_k$.

We then derive bounds for the learning steps and generalize the results in \cite{oudjane} to quantify the stability and robustness of the approximate filter.

In this development, Gaussian PSD Models are central for two reasons. First, they can go through the filtering iterations in closed-form since they are closed with respect to operations on probability densities. Second, their approximation properties are adaptive to the target function, which allows us to have a tight control on the model error and \emph{in fine} on the robustness of the filter.

\paragraph{Main contributions}
\textsc{PSDFilter} is a non-linear filtering algorithm that approximates the filtering distribution using positive semi-definite models. \textsc{PSDFilter} recovers previously proposed estimators such as the Kalman filter and can be applied to any filtering problem where the transition kernels admit a smooth density.

In \cref{theorem:main-theorem} we show that \textsc{PSDFilter} is stable and robust, and that its performance is adaptive to the regularity properties of the Hidden Markov Model. This is summarized informally below:
\begin{mdframed}
\begin{informaltheorem}[based on \cref{theorem:bound-diagonal}]
For any $\varepsilon > 0$, if the optimal kernel is $\sigma$-mixing and bounded, and we use enough points to learn $\hat G$ and $\hat Q$ from function evaluations, for any observation sequence $y_1, \ldots, y_N$ (not necessarily sampled from the true distribution), with high probability (over the draw of the training set for $\hat G$ and $\hat Q$), if $\pi_k$ is the true filter initialized at $\pi_0$ and $\hat \pi_k$ is the result of \textsc{PSDFilter} initialized at $\hat \pi_0$,
\begin{align*}
    \Vert \pi_k - \hat\pi_k\Vert_{TV} ~~\leq~~ \frac{C}{\mixing^2}\left(\frac{1-\mixing^2}{1 + \mixing^2}\right)^{k-1}\|\pi_0 - \hat{\pi}_0\|_{TV} ~~+~~ \frac{\varepsilon}{\sigma},
\end{align*}
\end{informaltheorem}
\end{mdframed}
The first right-hand term shows that the initialization error between the true filter $\pi_k$ and \textsc{PSDFilter} $\hat \pi_k$ is forgotten exponentially, which reflects the $\sigma$-mixing hypothesis. The second term shows that the error is bounded, uniformly on the observation sequence (which does not have to sampled from the HMM). In particular, we can make this bound as small as we want by reducing $\varepsilon$, at the cost of requiring a richer model and more data.

Indeed, we prove that we can learn approximations of the transition kernels with optimal learning rates by solving a convex problem. The approach is to approximate the transition kernel $Q$ by $\hat Q = \hat q^2$ where $\hat q$ is a Kernel Ridge Regression estimator of $\sqrt{Q}$ based on evaluations of $\sqrt{Q}$ (and similarly for the observation kernel $G$). This approach is justified by \cref{theorem:learning} summarized by this informal result:
\begin{mdframed}
\begin{informaltheorem}[based on \cref{theorem:learning}]
If $f$ is a $\beta$-smooth function in dimension $d$ and $\hat g$ is a Kernel Ridge Regression estimator of $g = \sqrt{f}$ with evaluations of $g$, then $\hat f = \hat g^2$ approximates $f$ with optimal rates. With the usual notations:
\begin{align}
    \Vert \hat f - f \Vert_{L^\infty(\Omega)}\leq C \Vert \sqrt{f}\Vert_{W^\beta_2(\Omega)}^2\epsilon^{1-\frac{d}{2\beta}}
\end{align}
where $C, C^\prime$ are constants depending only on $\beta, d$ and independent of $f$ and $\epsilon$.
\end{informaltheorem}
\end{mdframed}
\noindent \cref{theorem:learning} is interesting beyond the context of filtering since it gives $L_\infty$ learning rates for rank-one PSD models learned from function evaluations.

We show \textsc{PSDFilter} beats the particle filter in computational complexity for a given error level for smooth densities. Indeed, the particle filter relies solely on Monte Carlo sampling and is not adaptive to the smoothness of the densities.
\paragraph{Key takeaways of \cref{ch:hmm}}
%\newline
\begin{itemize}
    \item \textsc{PSDFilter} is a filtering algorithm that leverages positive semi-definite models to approximate the filtering distribution for a non-linear Hidden Markov Model, recovering classic estimators like the Kalman filter.
    \item \textsc{PSDFilter} is applicable to any filtering problem where the transition kernels have a smooth density, offering broad versatility.
    \item We demonstrate that \textsc{PSDFilter} is both stable and robust, with performance that adapts to the regularity properties of the Hidden Markov Model. The error bound for \textsc{PSDFilter} is uniformly controlled across observation sequences, with the ability to minimize this bound by refining the model and using more data.
    \item Learning the components of \textsc{PSDFilter} is a convex problem (KRR) and achieves optimal learning rates.
    \item \textsc{PSDFilter} surpasses the particle filter in computational efficiency for smooth densities, as it adapts to the smoothness of the densities, unlike the particle filter, which relies on Monte Carlo sampling.
\end{itemize}
